[
{"Title": "Harness analytical and predictive power with Azure Synapse Analytics", "Date": "Posted on December 7, 2020", "Contributor": "Rohan Kumar", "Content": "\nSince its preview announcement, we’ve witnessed incredible excitement and adoption of Azure Synapse from our customers and partners. We want to sincerely thank everyone that provided feedback and are now helping us bring the power of limitless analytics to all.\nAs announced last week, Azure Synapse is now generally available.\nUnified experience\nAzure Synapse brings together data integration, enterprise data warehousing, and big data analytics—at cloud scale. The unification of these workloads enables organizations to massively reduce their end-to-end development time and accelerate time to insight. It now also provides both no-code and code-first experiences for critical tasks such as data ingestion, preparation, and transformation.\n\n\n \n\nWith this release, the management and monitoring of your analytics system becomes significantly easier. With one click, teams can secure their entire analytics system and prevent data exfiltration by simply selecting the managed virtual network feature when creating a Synapse workspace. This gives valuable time back to teams hired to discover insights—rather than investing considerable time securing connections between services, building firewalls, or managing subnets.\nUnified analytics\nOver the past years, we’ve set out to rearchitect and create the next generation of query processing engine and data management to meet the needs of the modern, high-scale data workloads. The result is the new, cloud-native, distributed SQL engine that powers Azure Synapse. It can scale from queries on a handful of cores to thousands of nodes—all depending on your workload needs.\nAzure Synapse enables a level of productivity and collaboration among data professionals that previously wasn’t possible by deeply integrating Apache Spark and its new SQL engine. And it supports popular languages that developers prefer including T-SQL, Python, Scala, and Java.\nThe new flexible service model for query processing allows data teams to use both serverless and dedicated options. Organizations can now choose the most cost-effective option for each use case—enjoying the advantages of a data lake for quick data exploration with pay-per-query pricing and/or a dedicated data warehouse for more predictable and mission-critical workloads.\n\n\"Azure Synapse was a game changer as it allowed us to deliver a big data solution at the scale required and the integration with Power BI empowered our analysts to do their work efficiently.\" \n—Adhi Utama, Data Solution Architect, Grab\n\n\n \n \nUnified data teams\nAzure Synapse is also deeply integrated with Microsoft Power BI and Azure Machine Learning.\nWith Power BI directly integrated in the Synapse Studio, BI professionals can work in the same service that houses data pipelines, data lakes, and data warehouses—reducing the time it takes to access clean and secure data for dashboards. And for lightning fast query performance, the new Power BI performance accelerator for Azure Synapse automates the creation and optimization of materialized views with just a few clicks.\nFor predictive analytics, teams can deploy machine learning models from the Azure Machine Learning model registry directly to Azure Synapse using a simple, guided user experience—no data movement required. The in-engine ML scoring can generate millions of predictions in seconds all while maintaining full data security as data doesn’t leave the platform. And with AutoML, all data teams—even organizations without highly trained data scientists—can automatically apply machine learning models to their data and generate predictive insights.\nThe code-free and programmatic integration—including CI/CD support with Git integration—enables seamless version control, collaboration, and code management between data engineers, data scientists, and BI professionals, allowing them to be highly productive across a variety of use cases.\n\nLearn more about Azure Synapse Analytics latest innovations and features.\nGet started today\n\nTry the generally available features in Azure Synapse with an Azure free trial account\nIf you already have an Azure account, quickly get started with samples in Azure Synapse\nRegister for a virtual, hands-on lab for free\nCheck this blog daily for updates on how to use the latest features. We will be posting daily blogs on the Azure Synapse TechCommunity site for the next two weeks and including a link to the latest TechComunity blog here for quick reference.\n\n\n\nDecember 8th blog: Unleash the power of predictive analytics in Azure Synapse with machine learning and AI\nDecember 9th blog: Quicky get started with Azure Synapse Studio \nDecember 10th blog: Access and analyze all data from the Data Hub in Azure Synapse Analytics\nDecember 11th blog: Using the Develop Hub with Knowledge Center to accelerate development with Azure Synapse Analytics\nDecember 14th blog: Ingest and transform data with Azure Synapse Analytics with ease\nDecember 15th blog: Explore the Monitor Hub in Synapse Studio to keep track of all activities in your Synapse workspace\nDecember 16th blog: Explore the Manage Hub in Synapse Studio to provision and secure resources\nDecember 17th blog: Analyze and explore data with T-SQL in Azure Synapse Analytics\nDecember 18th blog: Kickstart your Apache Spark learning in Azure Synapse with immediately available samples\nDecember 21st blog: Integrate Power BI with Azure Synapse Analytics\n\n", "link": "https://azure.microsoft.com/en-us/blog/harness-analytical-and-predictive-power-with-azure-synapse-analytics/", "Role": "Corporate Vice President, Azure Data"},
{"Title": "Six things to consider when using Video Indexer at scale", "Date": "Posted on January 27, 2020", "Contributor": "Itay Arbel", "Content": "\nYour large archive of videos to index is ever-expanding, thus you have been evaluating Microsoft Video Indexer and decided that you want to take your relationship with it to the next level by scaling up.\nIn general, scaling shouldn’t be difficult, but when you first face such process you might not be sure what is the best way to do it. Questions like “are there any technological constraints I need to take into account?”, “Is there a smart and efficient way of doing it?”, and “can I prevent spending excess money in the process?” can cross your mind. So, here are six best practices of how to use Video Indexer at scale.\n \n1. When uploading videos, prefer URL over sending the file as a byte array\nVideo Indexer does give you the choice to upload videos from URL or directly by sending the file as a byte array, but remember that the latter comes with some constraints.\nFirst, it has file size limitations. The size of the byte array file is limited to 2 GB compared to the 30 GB upload size limitation while using URL.\nSecond and more importantly for your scaling, sending files using multi-part means high dependency on your network, service reliability, connectivity, upload speed, and lost packets somewhere in the world wide web, are just some of the issues that can affect your performance and hence your ability to scale.\n\nWhen you upload videos using URL you just need to give us a path to the location of a media file and we will take care of the rest (see below the field from the upload-video API).\nTo upload videos using URL via API you can check this short-code sample or you can use AzCopy for a fast and reliable way to get your content to a storage account from which you can submit it to Video Indexer using SAS URL.\n\n2. Increase media reserved units if needed\nUsually in the proof of concept stage when you just start using Video Indexer, you don’t need a lot of computing power. Now, when you want to scale up your usage of Video Indexer you have a larger archive of videos you want to index and you want the process to be at a pace that fits your use case. Therefore, you should think about increasing the number of compute resources you use if the current amount of computing power is just not enough.\nIn Azure Media Services, when talking about computing power and parallelization we talk about media reserved units (RUs), those are the compute units that determine the parameters for your media processing tasks. The number of RUs affects the number of media tasks that can be processed concurrently in each account and their type determines the speed of processing and one video might require more than one RU if its indexing is complex. When your RUs are busy, new tasks will be held in a queue until another resource is available.\nWe know you want to operate efficiently and you don’t want to have resources that eventually will stay idle part of the time. For that reason, we offer an auto-scale system that spins RUs down when less processing is needed and spin RUs up when you are in your rush hours (up to fully use all of your RUs). You can easily enable this functionality by turning on the autoscale in the account settings or using Update-Paid-Account-Azure-Media-Services API.\nTo minimize indexing duration and low throughput we recommend you start with 10 RUs of type S3. Later if you scale up to support more content or higher concurrency, and you need more resources to do so, you can contact us using the support system (on paid accounts only) to ask for more RUs allocation.\n3. Respect throttling\nVideo Indexer is built to deal with indexing at scale, and when you want to get the most out of it you should also be aware of the system’s capabilities and design your integration accordingly. You don’t want to send an upload request for a batch of videos just to discover that some of the movies didn’t upload and you are receiving an HTTP 429 response code (too many requests). It can happen due to the fact that you sent more requests than the limit of movies per minute we support. Don’t worry, in the HTTP response, we add a retry-after header. The header we will specify when you should attempt your next retry. Make sure you respect it before trying your next request.\n\n4. Use callback URL\nHave you ever called customer service and their response was “I’m now processing your request, it will take a few minutes. You can leave your phone number and we’ll get back to you when it is done”? The cases when you do leave your number and they call you back the second your request was processed are exactly the same concept as using callback URL.\nThus we recommend that instead of polling the status of your request constantly from the second you sent the upload request, you can just add a callback URL, and wait for us to update you. As soon as there is any status change in your upload request, we will send a POST notification to the URL you sent.\nYou can add a callback URL as one of the parameters of the upload-video API (see below the description from the API). If you are not sure how to do it, you can check the code samples from our GitHub repo. By the way, for callback URL you can also use Azure Functions, a serverless event-driven platform that can be triggered by HTTP and implement a following flow.\n\n5. Use the right indexing parameters for you\nProbably the first thing you need to do when using Video Indexer, and specifically when trying to scale, is to think about how to get the most out of it with the right parameters for your needs. Think about your use case, by defining different parameters you can save yourself money and make the indexing process for your videos faster.\nWe are giving you the option to customize your usage in Video Indexer by choosing those indexing parameters. Don’t set the preset to streaming it if you don’t plan to watch it, don’t index video insights if you only need audio insights, it is that easy.\nBefore uploading and indexing your video read this short documentation, check the indexingPreset and streamingPreset parts to get a better idea of what your options are.\n6. Index in optimal resolution, not highest resolution\nNot too long ago, we were in times when HD videos didn't exist. Now, we have videos of varied qualities from HD to 8K. The question is, what video quality do you need for indexing your videos? The higher the quality of the movie you upload means the higher the file size, and this leads to higher computing power and time needed to upload the video.\nOur experiences show that, in many cases, indexing performance has almost no difference between HD (720P) videos and 4K videos. Eventually, you’ll get almost the same insights with the same confidence.\nFor example, for the face detection feature, a higher resolution can help with the scenario where there are many small but contextually important faces. However, this will come with a quadratic increase in runtime and an increased risk of false positives.\nTherefore, we recommend you to verify that you get the right results for your use case and to first test it locally. Upload the same video in 720P and in 4K and compare the insights you get. Remember, No need to use a cannon to kill a fly.\nHave questions or feedback? We would love to hear from you. Use our UserVoice page to help us prioritize features, leave a comment below or email VISupport@Microsoft.com for any questions.\nWe want to hear about your use case, and we can help you scale.\n", "link": "https://azure.microsoft.com/en-us/blog/six-things-to-consider-when-using-video-indexer-at-scale/", "Role": "Program Manager, Azure Media Services, Video Indexer"},
{"Title": "New features for Form Recognizer now available", "Date": "Posted on March 17, 2020", "Contributor": "Neta Haiby", "Content": "\nExtracting text and structure information from documents is a core enabling technology for robotic process automation and workflow automation. Since its preview release in May 2019, Azure Form Recognizer has attracted thousands of customers to extract text, key and value pairs, and tables from documents to accelerate their business processes.\nToday, we're sharing the new Form Recognizner features that are available.\nUpdates for Azure Form Recognizer\nThe Form Recognizer March release is a major update that includes many new features our customers have asked for:\n\nCustomization: The service now supports training with and without labels, which makes it easier for customers to reliably extract valuable information from their forms. The APIs have also been redesigned as long-running operations to improve support for larger customer data sets. Automatic detection of key value pairs and table extraction have been enhanced and improved. A new sample labeling tool UX container will help customers label data more efficiently and extract the values of interest.\r\n\t  \n\n\nForm Recognizer Custom: Train with Labels, Form Recognizer Sample Labeling Tool.\nIn addition, Form Recognizer Sample Labeling Tool is now available as an open source project. You can integrate it within your solutions and make customer-specific changes to meet your needs.\n\nLayout: We released a new Layout API that is capable of extracting text and tables from documents with high accuracy optical character recognition (OCR) results on small texts. It also extracts tables from arbitrary documents, enabling a very popular application scenario for document extraction.\n\n\nLayout text and table extraction: Table extracted with 5 columns and 30 rows.\n\nPre-Built Receipt: The new version features major accuracy improvements. Error rates for certain fields like merchant name, phone number, transaction time, and subtotal have been reduced by more than 30 percent. We also added support for recognizing tips, receipt type, and line items, as well as providing confidence values.\r\n\t   \n\n\nPre-built Receipt: Key fields extracted from itemized sales receipt.\nLearn more about what’s new in Form Recognizer.\nOur customers\nAcumatica and Zelros are customers using Azure Form Recognizer and have shared their experiences with Microsoft.\n\n“By automating expense reporting with Form Recognizer, we can eliminate almost all human errors—which really helps accounting teams streamline approvals and reimbursement.“ Ajoy Krishnamoorthy, Vice President of Platform and Technology Acumatica.\n\nLearn more in our case study with Acumatica.\n\n“Zelros Documents2Insights leverages Form Recognizer to speed up the insurers' and bancassurers’ underwriting process. Identity card, proof of residence, vehicle registration document, driving license, and more. Speeding up and simplifying this business process is key to improve the experience of policyholders. Zelros Documents2Insights automates the underwriting processes, based on the Cognitive Services Computer Vision API and built on top of the Form Recognizer feature, the solution automatically reads and analyzes documents. It also cross-references information in order to correct and lower the error rate, while complying with regulatory requirements. With this, we are to process documents and subscriptions faster.”  Fabien Vauchelles, CTO of Zelros\n\nGetting started\nTo get started, please login to the Azure portal to create a Form Recognizer resource. Once your resource is created you can extract data from your forms by following one of our Quickstart templetes:\nCustom: Train a custom model for your forms to extract text, key value pairs, and tables.\n\nTrain without labels:\r\n\t\nQuickstart: Train a Form Recognizer model and extract form data by using the REST API with cURL.\nQuickstart: Train a Form Recognizer model and extract form data by using the REST API with Python.\n\n\nTrain with labels:\r\n\t\nQuickstart: Train a Form Recognizer model with labels using the sample labeling tool.\nQuickstart: Train a Form Recognizer model with labels using REST API and Python.\n\n\n\nPrebuilt receipts: Extract data from USA sales receipts.\n\nQuickstart: Extract receipt data using the REST API with cURL.\nQuickstart: Extract receipt data using the REST API with Python.\n\nLayout: Extract text and table structure (row and column numbers) from your documents.\n\n Quickstart: Extract layout data using the REST API with Python.\n\n", "link": "https://azure.microsoft.com/en-us/blog/new-features-for-form-recognizer-now-available/", "Role": "Principal Program Manager, Azure AI"},
{"Title": "Microsoft powers transformation at NVIDIA’s GTC Digital Conference", "Date": "Posted on March 24, 2020", "Contributor": "Kevin Raines", "Content": "\nThe world of supercomputing is evolving. Work once limited to high-performance computing (HPC) on-premises clusters and traditional HPC scenarios, is now being performed at the edge, on-premises, in the cloud, and everywhere in between. Whether it’s a manufacturer running advanced simulations, an energy company optimizing drilling through real-time well monitoring, an architecture firm providing professional virtual graphics workstations to employees who need to work remotely, or a financial services company using AI to navigate market risk, Microsoft’s collaboration with NVIDIA makes access to NVIDIA graphics processing units (GPU) platforms easier than ever.\nThese modern needs require advanced solutions that were traditionally limited to a few organizations because they were hard to scale and took a long time to deliver. Today, Microsoft Azure delivers HPC capabilities, a comprehensive AI platform, and the Azure Stack family of hybrid and edge offerings that directly address these challenges.\nThis year during GTC Digital, we’re spotlighting some of the most transformational applications powered by NVIDIA GPU acceleration that highlight our commitment to edge, on-prem, and cloud computing. Registration is free, so sign up to learn how Microsoft is powering transformation.\nVisualization and GPU workstations\nAzure enables a wide range of visualization workloads, which are critical for desktop virtualization as well as professional graphics such as computer-aided design, content creation, and interactive rendering. Visualization workloads on Azure are powered by NVIDIA’s world-class GPUs and Quadro technology, the world’s preeminent visual computing platform. With access to graphics workstations on Azure cloud, artists, designers, and technical professionals can work remotely, from anywhere, and from any connected device. See our NV-Series virtual machines (VMs) for Windows and Linux.\nArtificial intelligence\nWe’re sharing the release of the updated execution provider in ONNX Runtime with integration for NVIDIA TensorRT 7. With this update, ONNX Runtime can execute open Open Neural Network Exchange (ONNX) models on NVIDIA GPUs on Azure cloud and at the edge using the Azure Stack Edge, taking advantage of the new features in TensorRT 7 like dynamic shape, mixed precision optimizations, and INT8 execution.\nDynamic shape support enables users to run variable batch size, which is used by ONNX Runtime to process recurrent neural network (RNN) and Bidirectional Encoder Representations from Transformers (BERT) models. Mixed precision and INT8 execution are used to speed up execution on the GPU, which enables ONNX Runtime to better balance the performance across CPU and GPU. Originally released in March 2019, TensorRT with ONNX Runtime delivers better inferencing performance on the same hardware when compared to generic GPU acceleration.\nAdditionally, the Azure Machine Learning service now supports RAPIDS, a high-performance GPU execution accelerator for data science framework using the NVIDIA CUDA platform. Azure developers can use RAPIDS in the same way they currently use other machine learning frameworks, and in conjunction with Pandas, Scikit-learn, PyTorch, and TensorFlow. These two developments represent major milestones towards a truly open and interoperable ecosystem for AI. We’re working to ensure these platform additions will simplify and enrich those developer experiences.\nEdge\nMicrosoft provides various solutions in the Intelligent Edge portfolio to empower customers to make sure that machine learning not only happens in the cloud but also at the edge. The solutions include Azure Stack Hub, Azure Stack Edge, and IoT Edge.\nWhether you are capturing sensor data and inferencing at the Edge or performing end-to-end processing with model training in Azure and leveraging the trained models at the edge for enhanced inferencing operations Microsoft can support your needs however and wherever you need to.\nSupercomputing scale\nTime-to-decision is incredibly important with a global economy that is constantly on the move. With the accelerated pace of change, companies are looking for new ways to gather vast amounts of data, train models, and perform real-time inferencing in the cloud and at the edge. The Azure HPC portfolio consists of purpose-built computing, networking, storage, and application services to help you seamlessly connect your data and processing needs with infrastructure options optimized for various workload characteristics.\nAzure Stack Hub announced preview\nMicrosoft, in collaboration with NVIDIA, is announcing that Azure Stack Hub with Azure NC-Series Virtual Machine (VM) support is now in preview. Azure NC-Series VMs are GPU-enabled Azure Virtual Machines available on the edge. GPU support in Azure Stack Hub unlocks a variety of new solution opportunities. With our Azure Stack Hub hardware partners, customers can choose the appropriate GPU for their workloads to enable Artificial Intelligence, training, inference, and visualization scenarios.\nAzure Stack Hub brings together the full capabilities of the cloud to effectively deploy and manage workloads that otherwise are not possible to bring into a single solution. We are offering two NVIDIA enabled GPU models during the preview period. They are available in both NVIDIA V100 Tensor Core and NVIDIA T4 Tensor Core GPUs. These physical GPUs align with the following Azure N-Series VM types as follows:\n\nNCv3 (NVIDIA V100 Tensor Core GPU): These enable learning, inference and visualization scenarios. See Standard_NC6s_v3 for a similar configuration.\nTBD (NVIDIA T4 Tensor Core GPU): This new VM size (available only on Azure Stack Hub) enables light learning, inference, and visualization scenarios.\n\nHewlett Packard Enterprise is supporting the Microsoft GPU preview program as part of its HPE ProLiant for Microsoft Azure Stack Hub solution.“The HPE ProLiant for Microsoft Azure Stack Hub solution with the HPE ProLiant DL380 server nodes are GPU-enabled to support the maximum CPU, RAM, and all-flash storage configurations for GPU workloads,” said Mark Evans, WW product manager, HPE ProLiant for Microsoft Azure Stack Hub, at HPE. “We look forward to this collaboration that will help customers explore new workload options enabled by GPU capabilities.” \nAs the leading cloud infrastructure provider1, Dell Technologies helps organizations remove cloud complexity and extend a consistent operating model across clouds. Working closely with Microsoft, the Dell EMC Integrated System for Azure Stack Hub will support additional GPU configurations, which include NVIDIA V100 Tensor Core GPUs, in a 2U form factor. This will provide customers increased performance density and workload flexibility for the growing predictive analytics and AI/ML markets. These new configurations also come with automated lifecycle management capabilities and exceptional support.\nTo participate in the Azure Stack Hub GPU preview, please send us an email today. \nAzure Stack Edge preview\n\nWe also announced the expansion of our Microsoft Azure Stack Edge preview with the NVIDIA T4 Tensor Core GPU. Azure Stack Edge is a cloud managed appliance that provides processing for fast local analysis and insights to the data. With the addition of an NVIDIA GPU, you’re able to build in the cloud then run at the edge. For more information about this exciting release please see the detailed blog.\nGTC Digital\nMicrosoft session recordings will be available on the GTC Digital site starting March 26. You can find a list of the Microsoft digital sessions along with corresponding links in the Microsoft Tech Community blog here.\n\n1 IDC WW Quarterly Cloud IT Infrastructure Tracker, Q3 2019, January 2020, Vendor Revenue\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-powers-transformation-at-nvidia-s-gtc-digital-conference/", "Role": "Sr. Product Marketing Manager, Azure Marketing"},
{"Title": "Microsoft is expanding the Azure Stack Edge with NVIDIA GPU preview", "Date": "Posted on March 24, 2020", "Contributor": "Dean Paron", "Content": "\nWe’re expanding the Microsoft Azure Stack Edge with NVIDIA T4 Tensor Core GPU preview during the GPU Technology Conference (GTC Digital). Azure Stack Edge is a cloud-managed appliance that brings Azure’s compute, storage, and machine learning capabilities to the edge for fast local analysis and insights. With the included NVIDIA GPU, you can bring hardware acceleration to a diverse set of machine learning (ML) workloads.\nWhat’s new with Azure Stack Edge\nAt Mobile World Congress in November 2019, we announced a preview of the NVIDIA GPU version of Azure Stack Edge and we’ve seen incredible interest in the months that followed. Customers in industries including retail, manufacturing, and public safety are using Azure Stack Edge to bring Azure capabilities into the physical world and unlock scenarios such as the real-time processing of video powered by Azure Machine Learning.\nThese past few months, we’ve taken our customers' feedback to make key improvements and are excited to make our preview available to even more customers today.\nIf you’re not already familiar with Azure Stack Edge, here are a few of the benefits:\n\nAzure Machine Learning: Build and train your model in the cloud, then deploy it to the edge for FPGA or GPU-accelerated inferencing.\nEdge Compute: Run IoT, AI, and business applications in containers at your location. Use these to interact with your local systems, or to pre-process your data before it transfers to Azure.\nCloud Storage Gateway: Automatically transfer data between the local appliance and your Azure Storage account.  Azure Stack Edge caches the hottest data locally and speaks file and object protocols to your on-prem applications.\nAzure-managed appliance: Easily order and manage Azure Stack Edge from the Azure Portal.  No initial capex fees; pay as you go, just like any other Azure service.\n\nEnabling our partners to bring you world-class business applications\nEqually important to bringing you a great device is enabling our partners to bring you innovative applications to meet your business needs.  We’d love to share some of the continued investment we’re making with partners to bring their exciting developments to you.\nAs self-checkouts grow in prevalence, Malong Technologies is innovating in AI applications for loss prevention.\n\n“For our customers in the retail industry, artificial intelligence innovation is happening at the edge,” said Matt Scott, co-founder and chief executive officer, Malong Technologies. “Along with our state-of-the-art solutions, our customers need hardware that is powerful, reliable, and custom-tailored for the cloud. Microsoft’s Azure Stack Edge fits the bill perfectly. We’re proud to be a Microsoft Gold Certified Partner, working with Microsoft to help our retail customers succeed.”\n\nIncreasing your manufacturing organization’s quality inspection accuracy is key to Mariner’s Spyglass Visual Inspection application.\n\n“Mariner has standardized on Microsoft’s Azure Stack Edge for our Spyglass Visual Inspection and Spyglass Connected Factory products. These solutions are mission critical to our manufacturing customers. Azure Stack Edge provides the performance, stability and availability they require.” – Phil Morris, CEO, Mariner \n\nBuilding computer vision solutions to improve performance and safety in manufacturing and other industries is a key area of innovation for XXII.\n\n“XXII is thrilled to be a Microsoft partner and we are working together to provide our clients with real time video analysis software on edge with the Azure Stack Edge box. With this solution, Azure allow us to harvest the full potential of NVIDIA GPUs directly on edge and be able to provide our clients in retail, industry and smart city with smart video analysis that are easily deployable, scalable and easily manageable with Azure stack Edge.” – Souheil Hanoune, Chief Scientific Officer, XXII\n\nMore to come with Azure Stack Edge\nThere are even more exciting developments with Azure Stack Edge coming. We’re putting the final touches on much-awaited new compute and AI capabilities including virtual machines, Kubernetes clusters, and multi-node support. Along with these new features announced at Ignite 2019, Data Box Edge was renamed Azure Stack Edge to align with the Azure Stack portfolio.\nOur Rugged series for sites with harsh or remote environments is also coming this year, including the battery-powered form-factor that can be carried in a backpack. The versatility of these Azure Stack Edge form-factors and cloud-managed capabilities brings cloud intelligence and compute to retail stores, factory floors, hospitals, field operations, disaster zones, and rescue operations.\n\nGet started with the Azure Stack Edge with NVIDIA GPU preview\nThank you for continuing to partner with us as we bring new capabilities to Azure Stack Edge. We’re looking forward to hearing from you.\n\nTo get started with the preview, please email us and we’ll follow up to learn more about your scenarios.\nLearn more about Azure Stack Edge.\n\nLearn more about Azure’s Hybrid Strategy\nRead about more updates from Azure during NVIDIA’s GTC.\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-is-expanding-the-azure-stack-edge-with-nvidia-t4-gpu-preview/", "Role": "General Manager, Azure Stack Edge and Data Box"},
{"Title": "Extending the power of Azure AI to Microsoft 365 users", "Date": "Posted on March 30, 2020", "Contributor": "John 'JG' Chirapurath", "Content": "\nToday, Yusuf Mehdi, Corporate Vice President of Modern Life and Devices, announced the availability of new Microsoft 365 Personal and Family subscriptions. In his blog, he shared a few examples of how Microsoft 365 is innovating to deliver experiences powered by artificial intelligence (AI) to billions of users every day. Whether through familiar products like Outlook and PowerPoint, or through new offerings such as Presenter Coach and Microsoft Editor across Word, Outlook, and the web, Microsoft 365 relies on Azure AI to offer new capabilities that make their users even more productive.\nWhat is Azure AI?\nAzure AI is a set of AI services built on Microsoft’s breakthrough innovation from decades of world-class research in vision, speech, language processing, and custom machine learning. What is particularly exciting is that Azure AI provides our customers with access to the same proven AI capabilities that power Microsoft 365, Xbox, HoloLens, and Bing. In fact, there are more than 20,000 active paying customers—and more than 85 percent of the Fortune 100 companies have used Azure AI in the last 12 months.\nAzure AI helps organizations:\n\nDevelop machine learning models that can help with scenarios such as demand forecasting, recommendations, or fraud detection using Azure Machine Learning.\nIncorporate vision, speech, and language understanding capabilities into AI applications and bots, with Azure Cognitive Services and Azure Bot Service.\nBuild knowledge-mining solutions to make better use of untapped information in their content and documents using Azure Search.\n\nMicrosoft 365 provides innovative product experiences with Azure AI\nThe announcement of Microsoft Editor is one example of innovation. Editor, your personal intelligent writing assistant is available across Word, Outlook.com, and browser extensions for Edge and Chrome. Editor is an AI-powered service available in more than 20 languages that has traditionally helped writers with spell check and grammar recommendations. Powered by AI models built with Azure Machine Learning, Editor can now recommend clear and concise phrasing, suggest more formal language, and provide citation recommendations.\n\nAdditionally, Microsoft PowerPoint utilizes Azure AI in multiple ways. PowerPoint Designer uses Azure Machine Learning to recommend design layouts to users based on the content on the slide. In the example image below, Designer made the design recommendation based on the context in the slide. It can also can intelligently crop objects and people in images and place them in optimal layout on a slide. Since its launch, PowerPoint Designer users have kept nearly two billion Designer slides in their presentation.\nYou can take a closer look at how the PowerPoint team built this feature with Azure Machine Learning in this blog.\n\nPowerPoint also uses Azure Cognitive Services such as the Speech service to power live captions and subtitles for presentations in real-time, making it easier for all audience members to follow along. Additionally, PowerPoint also uses Translator Text to provide live translations into over 60 languages to reach an even wider audience. These AI-powered capabilities in PowerPoint are providing new experiences for users, allowing them to connect with diverse audiences they were unable to reach before.\nThese same innovations can also be found in Microsoft Teams. As we look to stay connected with co-workers, Teams has some helpful capabilities intended to make it easier to collaborate and communicate while working remotely. For example, Teams offers the ability of live captioning meetings, which leverages the Speech API for speech transcription. But it doesn’t stop there. As you saw with PowerPoint, Teams also uses Azure AI for live translations when you set up Live Events. This functionality is particularly useful for company town hall meetings or even for any virtual event with up to ten thousand attendees, allowing presenters to reach audiences worldwide\n\nThese are just a few of the ways Microsoft 365 applications utilize Azure AI to deliver industry-leading experiences to billions of users. When you consider the fact that other Microsoft products such as Microsoft 365, Xbox, HoloLens 2, Dynamics 365, and Power Platform all rely on Azure AI, you begin to see the massive scale and the breadth of scenarios that only Azure can offer. Best of all, these same capabilities are available to anyone in Azure AI. \nLearn more about Azure AI\nBelow are some technical blogs for a closer look at how these capabilities were built.\n\nHow Azure Machine Learning powers suggested replies in Outlook.\nHow Azure machine learning enables PowerPoint Designer.\n\n", "link": "https://azure.microsoft.com/en-us/blog/extending-the-power-of-azure-ai-to-microsoft-365-users/", "Role": "Vice President, Azure"},
{"Title": "Meeting the challenges of today and tomorrow with Azure AI", "Date": "Posted on May 21, 2020", "Contributor": "Eric  Boyd", "Content": "\nIt’s inspiring to see how customers continue to reimagine how they work with the help of AI, which is more important today than ever. Our customers are finding innovative ways to deliver crisis management solutions, drive cost-savings, redefine customer engagement, and accelerate decision-making.\nHere are some notable examples we’ve recently seen:\nScaling crisis management\nOn the frontlines, first responders rely on Azure AI to scale their triage process to address the overwhelming number of people needing care and to ease volume in the system. For example, healthcare providers have created more than 1,400 bots using our Healthcare Bot service, helping more than 27 million people access critical healthcare information. The U.S. Centers for Disease Control and Prevention released a COVID-19 assessment bot that is powered by Azure Bot Service. Motorola Solutions uses Azure Bot Service, as well as speech and language services, in its own voice assistant for public safety, ViQi, to help 911 dispatchers and first responders focus on what matters most.\nRealizing cost-savings\nAzure AI is also helping customers optimize their operations to reduce costs. KPMG built a risk and fraud analytics solution using our speech and language services to streamline call center transcription and translation—cutting time, cost, and effort by as much as 80 percent. Norwegian Hull Club, a mutual marine insurance company, built a knowledge mining solution using Cognitive Search for employees to access archived documents, reducing the process from hours to minutes and helping them generate significant savings. Duck Creek is leveraging Cognitive Search and Azure Synapse Analytics to build a claims management solution for insurance companies to save on operational costs by reducing claims errors and minimizing fraud.\nAccelerating decision making \nGiven the pace of change, organizations need to move forward with agility, insights, and confidence. Carhartt used Azure Machine Learning to identify new store locations. These sites exceeded their revenue goals by 285 percent.\n\nTibco helps field analysts accelerate their decision-making for maintaining power plant operations by leveraging Anomaly Detector and Text Analytics. Wilson Allen is helping law firms around the world to automate paper-based processes by using Form Recognizer and Text Analytics to find new business development opportunities. PwC has built a solution with Cognitive Search to help financial institutions meet regulatory requirements by quickly pinpointing relevant information in dense legal documents. TalentCloud’s solution uses Azure Machine Learning to help farmers increase their yield, reduce pollution, and step up food safety by providing crop management recommendations and controlling irrigation equipment. Scandinavian Airlines (SAS) has used the new responsible ML capabilities in Azure Machine Learning to spot fraudulent transactions with 99 percent accuracy.\n\nReimagining customer engagement\nOffering people more efficient, personalized experiences continues to evolve everywhere—from call centers to classrooms. Vodafone created TOBi, a customer service chatbot with our speech and language services, so customers can quickly complete tasks, freeing up support agents to focus more on complex customer cases. Similarly, Universal Electronics has developed a virtual assistant to enable voice-based control across thousands of consumer devices such as universal remotes and smart thermostats. In banking, Insight Enterprises has created a self-service kiosk solution using our vision, speech, and language services for a more intuitive customer experience. In education, our partners are developing new Azure AI-powered solutions to facilitate remote learning. For example, TAL Education Group is using the pronunciation assessment capability in our Speech service to help students with remote presentation and foreign language practice. Wakelet and Nearpod are helping students enhance their reading comprehension skills using Immersive Reader.\nOnward together\nOur customers inspire us to continue our focus on supporting their journey with AI. We look forward to innovating with you to help your organization succeed in meeting the challenges of today and tomorrow.\n\nSee the latest product announcements. \nDiscover more about Azure AI.\nRead about the new responsible ML capabilities.\n\n", "link": "https://azure.microsoft.com/en-us/blog/meeting-the-challenges-of-today-and-tomorrow-with-azure-ai/", "Role": "Corporate Vice President, Azure AI "},
{"Title": "A year of bringing AI to the edge", "Date": "Posted on November 26, 2019", "Contributor": "Tina Coll", "Content": "\nThis post is co-authored by Anny Dow, Product Marketing Manager, Azure Cognitive Services.\nIn an age where low-latency and data security can be the lifeblood of an organization, containers make it possible for enterprises to meet these needs when harnessing artificial intelligence (AI).\nSince introducing Azure Cognitive Services in containers this time last year, businesses across industries have unlocked new productivity gains and insights. The combination of both the most comprehensive set of domain-specific AI services in the market and containers enables enterprises to apply AI to more scenarios with Azure than with any other major cloud provider. Organizations ranging from healthcare to financial services have transformed their processes and customer experiences as a result.\n \n\nThese are some of the highlights from the past year:\nEmploying anomaly detection for predictive maintenance\nAirbus Defense and Space, one of the world’s largest aerospace and defense companies, has tested Azure Cognitive Services in containers for developing a proof of concept in predictive maintenance. The company runs Anomaly Detector for immediately spotting unusual behavior in voltage levels to mitigate unexpected downtime. By employing advanced anomaly detection in containers without further burdening the data scientist team, Airbus can scale this critical capability across the business globally.\n“Innovation has always been a driving force at Airbus. Using Anomaly Detector, an Azure Cognitive Service, we can solve some aircraft predictive maintenance use cases more easily.”  —Peter Weckesser, Digital Transformation Officer, Airbus\nAutomating data extraction for highly-regulated businesses\nAs enterprises grow, they begin to acquire thousands of hours of repetitive but critically important work every week. High-value domain specialists spend too much of their time on this. Today, innovative organizations use robotic process automation (RPA) to help manage, scale, and accelerate processes, and in doing so free people to create more value.\nAutomation Anywhere, a leader in robotic process automation, partners with these companies eager to streamline operations by applying AI. IQ Bot, their unique RPA software, automates data extraction from documents of various types. By deploying Cognitive Services in containers, Automation Anywhere can now handle documents on-premises and at the edge for highly regulated industries:\n“Azure Cognitive Services in containers gives us the headroom to scale, both on-premises and in the cloud, especially for verticals such as insurance, finance, and health care where there are millions of documents to process.” —Prince Kohli, Chief Technology Officer for Products and Engineering, Automation Anywhere\nFor more about Automation Anywhere's partnership with Microsoft to democratize AI for organizations, check out this blog post.\nDelighting customers and employees with an intelligent virtual agent\nLowell, one of the largest credit management services in Europe, wants credit to work better for everybody. So, it works hard to make every consumer interaction as painless as possible with the AI. Partnering with Crayon, a global leader in cloud services and solutions, Lowell set out to solve the outdated processes that kept the company’s highly trained credit counselors too busy with routine inquiries and created friction in the customer experience. Lowell turned to Cognitive Services to create an AI-enabled virtual agent that now handles 40 percent of all inquiries—making it easier for service agents to deliver greater value to consumers and better outcomes for Lowell clients.\nWith GDPR requirements, chatbots weren’t an option for many businesses before containers became available. Now companies like Lowell can ensure the data handling meets stringent compliance standards while running Cognitive Services in containers. As Carl Udvang, Product Manager at Lowell explains:\n\"By taking advantage of container support in Cognitive Services, we built a bot that safeguards consumer information, analyzes it, and compares it to case studies about defaulted payments to find the solutions that work for each individual.\"\nOne-to-one customer care at scale in data-sensitive environments has become easier to achieve.\nEmpowering disaster relief organizations on the ground\nA few years ago, there was a major Ebola outbreak in Liberia. A team from USAID was sent to help mitigate the crisis. Their first task on the ground was to find and categorize the information such as the state of healthcare facilities, wifi networks, and population density centers.  They tracked this information manually and had to extract insights based on a complex corpus of data to determine the best course of action.\nWith the rugged versions of Azure Stack Edge, teams responding to such crises can carry a device running Cognitive Services in their backpack. They can upload unstructured data like maps, images, pictures of documents and then extract content, translate, draw relationships among entities, and apply a search layer. With these cloud AI capabilities available offline, at their fingertips, response teams can find the information they need in a matter of moments. In Satya’s Ignite 2019 keynote, Dean Paron, Partner Director of Azure Storage and Edge, walks us through how Cognitive Services in Azure Stack Edge can be applied in such disaster relief scenarios (starting at 27:07): \n\nTransforming customer support with call center analytics\nCall centers are a critical customer touchpoint for many businesses, and being able to derive insights from customer calls is key to improving customer support. With Cognitive Services, businesses can transcribe calls with Speech to Text, analyze sentiment in real-time with Text Analytics, and develop a virtual agent to respond to questions with Text to Speech. However, in highly regulated industries, businesses are typically prohibited from running AI services in the cloud due to policies against uploading, processing, and storing any data in public cloud environments. This is especially true for financial institutions.\nA leading bank in Europe addressed regulatory requirements and brought the latest transcription technology to their own on-premises environment by deploying Cognitive Services in containers. Through transcribing calls, customer service agents could not only get real-time feedback on customer sentiment and call effectiveness, but also batch process data to identify broad themes and unlock deeper insights on millions of hours of audio. Using containers also gave them flexibility to integrate with their own custom workflows and scale throughput at low latency.\nWhat's next?\nThese stories touch on just a handful of the organizations leading innovation by bringing AI to where data lives. As running AI anywhere becomes more mainstream, the opportunities for empowering people and organizations will only be limited by the imagination.\nVisit the container support page to get started with containers today.\nFor a deeper dive into these stories, visit the following\n\nAutomation Anywhere case study\nAutomation Anywhere’s partnership with Microsoft\nLowell case study\nAzure Stack Edge update from Microsoft Ignite 2019\nCognitive Services in Azure Stack Edge demo (at 27:07)\n\n", "link": "https://azure.microsoft.com/en-us/blog/a-year-of-bringing-ai-to-the-edge/", "Role": "Senior Product Marketing Manager"},
{"Title": "Combine the Power of Video Indexer and Computer Vision", "Date": "Posted on December 11, 2019", "Contributor": "Anika  Zaman", "Content": "\nWe are pleased to introduce the ability to export high-resolution keyframes from Azure Media Service’s Video Indexer. Whereas keyframes were previously exported in reduced resolution compared to the source video, high resolution keyframes extraction gives you original quality images and allows you to make use of the image-based artificial intelligence models provided by the Microsoft Computer Vision and Custom Vision services to gain even more insights from your video. This unlocks a wealth of pre-trained and custom model capabilities. You can use the keyframes extracted from Video Indexer, for example, to identify logos for monetization and brand safety needs, to add scene description for accessibility needs or to accurately identify very specific objects relevant for your organization, like identifying a type of car or a place.\nLet’s look at some of the use cases we can enable with this new introduction.\nUsing keyframes to get image description automatically\nYou can automate the process of “captioning” different visual shots of your video through the image description model within Computer Vision, in order to make the content more accessible to people with visual impairments. This model provides multiple description suggestions along with confidence values for an image. You can take the descriptions of each high-resolution keyframe and stitch them together to create an audio description track for your video.\n\nUsing Keyframes to get logo detection\nWhile Video Indexer detects brands in speech and visual text, it does not support brands detection from logos yet. Instead, you can run your keyframes through Computer Vision’s logo-based brands detection model to detect instances of logos in your content.\nThis can also help you with brand safety as you now know and can control the brands showing up in your content. For example, you might not want to showcase the logo of a company directly competing with yours. Also, you can now monetize on the brands showing up in your content through sponsorship agreements or contextual ads.\nFurthermore, you can cross-reference the results of this model for you keyframe with the timestamp of your keyframe to determine when exactly a logo is shown in your video and for how long. For example, if you have a sponsorship agreement with a content creator to show your logo for a certain period of time in their video, this can help determine if the terms of the agreement have been upheld.\nComputer Vision’s logo detection model can detect and recognize thousands of different brands out of the box. However, if you are working with logos that are specific to your use case or otherwise might not be a part of the out of the box logos database, you can also use Custom Vision to build a custom object detector and essentially train your own database of logos by uploading and correctly labeling instances of the logos relevant to you.\n\nUsing keyframes with other Computer Vision and Custom Vision offerings\nThe Computer Vision APIs provide different insights in addition to image description and logo detection, such as object detection, image categorization, and more. The possibilities are endless when you use high-resolution keyframes in conjunction with these offerings.\nFor example, the object detection model in Computer Vision gives bounding boxes for common out of the box objects that are already detected as part of Video Indexer today. You can use these bounding boxes to blur out certain objects that don’t meet your standards.\n\nHigh-resolution keyframes in conjunction with Custom Vision can be leveraged to achieve many different custom use cases. For example, you can train a model to determine what type of car (or even what breed of cat) is showing in a shot. Maybe you want to identify the location or the set where a scene was filmed for editing purposes. If you have objects of interest that may be unique to your use case, use Custom Vision to build a custom classifier to tag visuals or a custom object detector to tag and provide bounding boxes for visual objects.\nTry it for yourself\nThese are just a few of the new opportunities enabled by the availability of high-resolution keyframes in Video Indexer. Now, it is up to you to get additional insights from your video by taking the keyframes from Video Indexer and running additional image processing using any of the Vision models we have just discussed. You can start doing this by first uploading your video to Video Indexer and taking the high-resolution keyframes after the indexing job is complete and second creating an account and getting started with the Computer Vision API and Custom Vision.\nHave questions or feedback? We would love to hear from you. Use our UserVoice page to help us prioritize features, leave a comment below or email VISupport@Microsoft.com for any questions.\n\n", "link": "https://azure.microsoft.com/en-us/blog/combine-the-power-of-video-indexer-and-computer-vision/", "Role": "Program Manager, Azure Media Services, Video Indexer"},
{"Title": "Creating a more accessible world with Azure AI", "Date": "Posted on January 16, 2020", "Contributor": "John 'JG' Chirapurath", "Content": "\nAt Microsoft, we are inspired by how artificial intelligence is transforming organizations of all sizes, empowering them to reimagine what’s possible. AI has immense potential to unlock solutions to some of society’s most pressing challenges.\nOne challenge is that according to the World Health Association, globally, only 1 in 10 people with a disability have access to assistive technologies and products. We believe that AI solutions can have a profound impact on this community. To meet this need, we aim to democratize AI to make it easier for every developer to build accessibility into their apps and services, across language, speech, and vision.\nIn view of the upcoming Bett Show in London, we’re shining a light on how Immersive Reader enhances reading comprehension for people regardless of their age or ability, and we’re excited to share how Azure AI is broadly enabling developers to build accessible applications that empower everyone.\nEmpowering readers of all abilities\nImmersive Reader is an Azure Cognitive Service that helps users of any age and reading ability with features like reading aloud, translating languages, and focusing attention through highlighting and other design elements. Millions of educators and students already use Immersive Reader to overcome reading and language barriers.\nThe Young Women’s Leadership School of Astoria, New York, brings together an incredible diversity of students with different backgrounds and learning styles. The teachers at The Young Women’s Leadership School support many types of learners, including students who struggle with text comprehension due to learning differences, or language learners who may not understand the primary language of the classroom. The school wanted to empower all students, regardless of their background or learning styles, to grow their confidence and love for reading and writing.\n \nTeachers at The Young Women’s Leadership School turned to Immersive Reader and an Azure AI partner, Buncee, as they looked for ways to create a more inclusive and engaging classroom. Buncee enables students and teachers to create and share interactive multimedia projects. With the integration of Immersive Reader, students who are dyslexic can benefit from features that help focus attention in their Buncee presentations, while those who are just learning the English language can have content translated to them in their native language.\nLike Buncee, companies including Canvas, Wakelet, ThingLink, and Nearpod are also making content more accessible with Immersive Reader integration. To see the entire list of partners, visit our Immersive Reader Partners page. Discover how you can start embedding Immersive Reader into your apps today. To learn more about how Immersive Reader and other accessibility tools are fostering inclusive classrooms, visit our EDU blog.\nBreaking communication barriers\nAzure AI is also making conversations, lectures, and meetings more accessible to people who are deaf or hard of hearing. By enabling conversations to be transcribed and translated in real-time, individuals can follow and fully engage with presentations.\nThe Balavidyalaya School in Chennai, Tamil Nadu, India teaches speech and language skills to young children who are deaf or hard of hearing. The school recently held an international conference with hundreds of alumni, students, faculty, and parents. With live captioning and translation powered by Azure AI, attendees were able to follow conversations in their native languages, while the presentations were given in English.\nLearn how you can easily integrate multi-language support into your own apps with Speech Translation, and see the technology in action with Translator, with support for more than 60 languages, today.\nEngaging learners in new ways\nWe recently announced the Custom Neural Voice capability of Text to Speech, which enables customers to build a unique voice, starting from just a few minutes of training audio.\nThe Beijing Hongdandan Visually Impaired Service Center leads the way in applying this technology to empower users in incredible ways. Hongdandan produces educational audiobooks featuring the voice of Lina, China’s first blind broadcaster, using Custom Neural Voice. While creating audiobooks can be a time-consuming process, Custom Neural Voice allows Lina to produce high-quality audiobooks at scale, enabling Hongdandan to support over 105 schools for the blind in China like never before.\n“We were amazed by how quickly Azure AI could reproduce Lina's voice in such a natural-sounding way with her speech data, enabling us to create educational audiobooks much more quickly. We were also highly impressed by Microsoft's commitment to protecting Lina's voice and identity.\"—Xin Zeng, Executive Director at Hongdandan\nLearn how you can give your apps a new voice with Text to Speech.\nMaking the world visible for everyone\nAccording to the International Agency for the Prevention of Blindness, more than 250 million people are blind or have low vision across the globe. Last month, in celebration of the United Nations International Day of Persons with Disabilities, Seeing AI, a free iOS app that describes nearby people, text, and objects, expanded support to five new languages. The additional language support for Spanish, Japanese, German, French, and Dutch makes it possible for millions of blind or low vision individuals to read documents, engage with people around them, hear descriptions of their surroundings in their native language, and much more. All of this is made possible with Azure AI.\nTry Seeing AI today or extend vision capabilities to your own apps using Computer Vision and Custom Vision.\nGet involved\nWe are humbled and inspired by what individuals and organizations are accomplishing today with Azure AI technologies. We can’t wait to see how you will continue to build on these technologies to unlock new possibilities and design more accessible experiences. Get started today with a free trial.\nCheck out our AI for Accessibility program to learn more about how companies are harnessing the power of AI to amplify capabilities for the millions of people around the world with a disability.\n", "link": "https://azure.microsoft.com/en-us/blog/creating-a-more-accessible-world-with-azure-ai/", "Role": "Vice President, Azure"},
{"Title": "From Microsoft Azure to everyone attending NAB Show 2018 — Welcome!", "Date": "Posted on April 9, 2018", "Contributor": "Tad Brockway", "Content": "\nNAB Show is one of my favorites. The creativity, technology, content and storytelling are epic, as is the digital transformation well underway.\nThis transformation — driven by new businesses models, shifting consumption patterns and technology advancements — will change this great industry. What won’t change is the focus on creators and connecting their content with consumers around the world so they, too, can be a part of the story.\nMicrosoft’s mission is to “empower every person and every organization on the planet to achieve more.” We are committed to helping everyone in the industry — customers like Endemol Shine, UFA, Jellyfish and Reuters — do just that. With innovations across cloud storage, compute, CDN, Media Services and new investments in Avere Systems and Cycle Cloud, Microsoft Azure is ready to help modernize your media workflows and your business.\nHow? Well, queue scene…\nYour productions are increasingly global and demanding. Azure can help.\nCreators, media artists and innovators want to work together in a flexible, secure and collaborative way from wherever they are. More datacenters, in more regions of the world, than all competitors combined means the show can go on, everywhere. That means a film crew doing a car chase scene through the streets of Monaco can upload the day’s rough cuts into Azure, while dailies are viewed by editors in Hollywood. Bollywood. Or a beach house in Malibu.\nIncreasing content resolution levels from 4K to 8K means increased production and post-production data storage requirements. Virtual Reality, predicted to go as high as 16K (the resolution of the human eye!), and 360° viewing formats will push those requirements even higher. Azure can help with pay-for-what-you-use storage options, from High Performance SSD to Archive Storage, at an industry-leading price — leaving you more time and budget to get the scene ‘right’.\nSimilarly, sophisticated new VFX, VR and rendering technologies mean increased compute requirements. With Azure, you get all the CPU and GPU processing power you need, for as long as you need, only paying for what you use. Low Priority VMs and Reserved Instances provide flexible ways to maximize your compute budget. Azure Cycle Cloud can be used to orchestrate the workflows, storage and compute of jobs in Azure and provide critical compliance and governance support. Avere Systems, a new member of the Microsoft Azure team, takes this one step further with industry-leading options that accelerate the performance of your storage, regardless of location.\nContent management and intelligence are a big challenge. Enter Azure + Cognitive Services.\nYou need to store today’s productions, every production ever made and every creative idea. Who knows when you’ll need them again? And you need to discover, identify and tag content — quickly, easily and automatically.\nWith Azure’s global scale and convenient storage tiers, you can store your content at the right cost, close to where you need it. It’s a cost-effective solution for your exabytes of tape-based media archives. In fact, our partners make it easy to move your data into Azure from almost any media — tapes, optical drives, hard disks or film. Check our Offline Media Import website for more details.\nWhen you move your media to the cloud, you can analyze it with Cognitive Services and benefit from Microsoft’s AI platform. These services can automatically transcribe speech — making dialog searchable — or comb through mounds of archived video and audio to find content featuring a certain actress or actor.\nYou can reach global audiences more easily and gain new insights with Azure + Microsoft AI\nAzure Media Services sets your content free with a proven platform for large scale live events, linear TV and on-demand content. In addition, you can securely distribute content to theaters via ExpressRoute networks and offline via Disk and Data Box. For even broader distribution, Azure CDN provides a uncluttered highway to your consumers, whether your content is static, dynamic or real-time.\nAs content is delivered, Azure’s advanced analytics and Microsoft’s AI platform go to work to discover insights that help you segment audiences, create personalized experiences and influence production and editorial decisions.\nTo help bring your production to life, a broad ecosystem awaits\nYou’ll have access to partner solutions spanning dailies, editorial, VFX, color correction, OTT, playout and more. Check out Sudheer’s blog to learn which partners and product features we’ve added since last year’s IBC.\nWhile the digital transformation of this industry is well underway, it’s far from over\nClick here for a sneak peek into our not-too-distant future, brought to you by Microsoft Azure and our partners.\n\nIf you’re attending NAB Show 2018, we hope to see you! Stop by our booth SL6716 to:\n\nChat with product team representatives from Azure Media Services, Azure Storage, Azure HPC, Microsoft Stream and Microsoft Skype for Broadcast.\nVisit with partners from 360 Collaboration, Avid, GreyMeta, Make.TV, Nimble Collective, Ooyala, Prime Focus Technologies, Teradici, Teknikos and Verizon Digital Media Services.\nSee some great customer, partner and product team presentations. A detailed schedule is available here.\n\nThanks for reading and have a great show!\nTad\n", "link": "https://azure.microsoft.com/en-us/blog/from-microsoft-azure-to-everyone-attending-nab-show-2018-welcome/", "Role": "Corporate Vice President, Microsoft Azure for Operators"},
{"Title": "Speech services July 2018 update", "Date": "Posted on July 18, 2018", "Contributor": "Grace Sturman", "Content": "\nA lot has happened since we announced that Speech services is now in preview, we have released the Cognitive Services Speech SDK June 2018 update.\nToday, we are excited to announce that we have just released the 0.5.0 version of the Speech SDK. With this update, we have added support for UWP (on Windows version 1709), .NET Standard 2.0 (on Windows), and Java on Android 6.0 (Marshmallow, API level 23) or higher. We have made some feature changes and done some bug fixes. Most notably, we now support long-running audio and automatic reconnection. This will make the Speech service more resilient overall, in the event of timeout, network failures or service errors. We’ve also improved the error messages to make it easier to handle the errors. Please visit the Release Notes page for details. We will continue to add support for more platforms and programming languages, as we work toward making the Speech SDK generally available this fall.\n\nBesides the Speech SDK, Custom Voice has also released a new feature to support more training data formats. All ‘.wav’ files (RIFF) with a sampling rates equal to or higher than 16khz are now accepted. Furthermore, we have extended support to more plain text encoding types (ANSI/UTF-8/UTF-8-BOM/UTF-16-LE/UTF-16-BE). For more details, visit our docs about how to prepare data and customize voice fonts. A new document is released to help you create high quality audio samples of human speech, with a focus on issues that you are likely to encounter during your voice training data preparation. For more details, see how to record voice samples for a custom voice.\n\nIn addition, we are very happy to announce new content for our Speech (Preview) documentation.\nThe content update aims to help developers to quickly navigate to the right content, based on the type of application they are developing.\nWe have a new separate section on the end-to-end customization process, including acoustic adaptation, language adaptation, pronunciation and voice fonts. We’ve added documentation about the Batch Transcription API which is ideal for customers that have large quantities of audio files on storage.\nThe Documentation is also complementing this SDK update with the following sections.\n\nBrand new Scenario section to help you navigate the documentation according to your applications needs.\nConsolidated e2e Customization section (including data and tutorial on GitHub)\nBrand new Batch Transcription API including GitHub Sample\nMore detail and elaborate FAQ section for each of the sub-services, under the Resources.\n\nThe documentation is live now. Please use the Feedback section at the bottom of the documentation pages to tell us what you think.”\nInterested in the Microsoft Speech services? You can try it out for free. To learn more and review sample code, please reference our documentation page. Please follow us on Twitter @msspeech3 to be notified for the future updates.\n", "link": "https://azure.microsoft.com/en-us/blog/speech-services-july-2018-update/", "Role": "Senior Program Manager, Speech Services"},
{"Title": "Public preview: Named Entity Recognition in the Cognitive Services Text Analytics API", "Date": "Posted on October 23, 2018", "Contributor": "Ashish Makadia", "Content": "\nToday, we are happy to announce the public preview of Named Entity Recognition as part of the Text Analytics Cognitive Service. Named Entity Recognition (NER) is the ability to take free-form text and identify the occurrences of entities such as people, locations, organizations, and more. With just a simple API call, NER in Text Analytics uses robust machine learning models to find and categorize more than twenty types of named entities in any text documents.\nMany organizations have messy piles of unstructured text in the form of customer feedback, enterprise documents, social media feeds, and more. However, it is challenging to understand what information these ever-growing stacks of documents contain. Text Analytics has long been helping customers make sense of these troves of text with capabilities such as Key Phrase Extraction, Sentiment Analysis, and Language Detection. Today's announcement adds to this suite of powerful and easy-to-use natural language processing solutions that make it easy to tackle many problems.\nNamed Entity Recognition and Entity Linking\nBuilding upon the Entity Linking feature that was announced at Build earlier this year, the new Entities API processes the text using both NER and Entity Linking capabilities. This makes it an extremely powerful solution for squeezing the most structured information out of the unstructured text.\nEntity Linking is the ability to identify and disambiguate the well-known identity of an entity found in the text, for example, determining whether the word \"Mars\" is being used as the planet or as the Roman god of war. This process requires the presence of a knowledge base which recognizes entities are linked. Knowledge bases from Bing and Wikipedia are used for Text Analytics. When the Text Analytics Entities API recognizes an entity using entity linking, it will provide links to more information about the entity on the web.\nNamed Entity Recognition, in contrast, can identify the entities in unstructured text regardless of whether the entities are well-known or exist in a knowledge base. When Text Analytics identifies an entity using NER, it will provide the type of entity i.e. person, location, organization, and others in the API response. In some cases, it will also provide a subtype.\nIn cases where an entity is recognized using both Entity Linking and Named Entity Recognition, the API will return the entity's type as well as web links to more information about the entity.\nSupported entity types\nUsing the Text Analytics Cognitive Service, it's currently possible to recognize more than twenty types of entities in both Spanish and English. View the most current list of supported languages:\n\n\n\nType\nSubType\nExample\n\n\n\n\nPerson\nN/A*\n\"Jeff\", \"Ashish Makadia\"\n\n\nLocation\nN/A*\n\"Redmond, Washington\", \"Paris\"\n\n\nOrganization\nN/A*\n\"Microsoft\"\n\n\nQuantity\nNumber\n\"6\", \"six\"\n\n\nQuantity\nPercentage\n\"50%\", \"fifty percent\"\n\n\nQuantity\nOrdinal\n\"2nd\", \"second\"\n\n\nQuantity\nNumberRange\n\"4 to 8\"\n\n\nQuantity\nAge\n\"90 days old\", \"30 years old\"\n\n\nQuantity\nCurrency\n\"$10.99\"\n\n\nQuantity\nDimension\n\"10 miles\", \"40 cm\"\n\n\nQuantity\nTemperature\n\"32 degrees\"\n\n\nDateTime\nN/A*\n\"6:30PM February 4, 2012\"\n\n\nDateTime\nDate\n\"May 2nd, 2017\", \"05/02/2017\"\n\n\nDateTime\nTime\n\"8am\", \"8:00\"\n\n\nDateTime\nDateRange\n\"May 2nd to May 5th\"\n\n\nDateTime\nTimeRange\n\"6pm to 7pm\"\n\n\nDateTime\nDuration\n\"1 minute and 45 seconds\"\n\n\nDateTime\nSet\n\"every Tuesday\"\n\n\nDateTime\nTimeZone\n“UTC-7”, “CST”\n\n\nURL\nN/A*\n\"https://www.bing.com\"\n\n\nEmail\nN/A*\n\"support@microsoft.com\"\n\n\n\nDepending on the input and extracted entities, certain entities may omit the SubType.\nNext steps\nRead more about Text Analytics and its capabilities, then visit our documentation. Please visit our pricing page to learn about the various tiers of service to fit your needs.\n", "link": "https://azure.microsoft.com/en-us/blog/public-preview-named-entity-recognition-in-the-cognitive-services-text-analytics-api/", "Role": "Program Manager II, Applied AI"},
{"Title": "Conversational - AI updates December 2018", "Date": "Posted on December 20, 2018", "Contributor": "Yochay Kiriaty", "Content": "\nThis blog post was co-authored by Vishwac Sena Kannan, Principal Program Manager, FUSE Labs.\nWe are thrilled to present the release of Bot Framework SDK version 4.2 and we want to use this opportunity to provide additional updates on Conversational-AI releases from Microsoft.\nIn the SDK 4.2 release, the team focused on enhancing monitoring, telemetry, and analytics capabilities of the SDK by improving the integration with Azure App Insights. As with any release, we fixed a number of bugs, continued to improve Language Understanding (LUIS) and QnA integration, and enhanced our engineering practices. There were additional updates across the other areas like language, prompt and dialogs, and connectors and adapters. You can review all the changes that went into 4.2 in the detailed changelog. For more information, view the list of all closed issues.\nTelemetry updates for SDK 4.2\nWith the SDK 4.2 release, we started improving the built-in monitoring, telemetry, and analytics capabilities provided by the SDK. Our goal is to provide developers with the ability to understand their overall bot-health, provide detailed reports about the bot’s conversation quality, as well as tools to understand where conversations fall short. To do that, we decided to further enhance the built-in integration with Microsoft Azure Application Insights. For that end, we have streamlined the integration and default telemetry emitted from the SDK. This includes waterfall dialog instrumentation, docs, examples for querying data, and a PowerBI dashboard.\nBot Framework can use the App Insights telemetry to provide information about how your bot is performing and track key metrics. For example, once you enable App Insights for your bot, the SDK will automatically trace important information for each activity that gets sent to your bot. Essentially, per activity, for example, a user interacts with your bot by typing some utterance, the SDK emit traces for all different stages of the activity processing. This then can be placed on a timeline showing each component latency and performance – as you can see from the following image.\nThis can help identify slow responses and further optimize your bot performance.\nBeyond basic bot performance analysis, we have instrumented the SDK to emit traces for the dialog stack in the SDK, primarily the waterfall dialog. The following image is a visualization showing the behavior of a waterfall dialog. Specifically, this image shows three events before and after someone completes a dialog across all sessions. The center “Initial Event” is the starting point that fans left and right showing before and after, respectively. This is great to show drop-off rate, shown in red, and where most conversation ‘flows’ by the thickness of the line. This view is a default app insight report, all we had to do is connect the wires between the SDK, dialogs, and App Insights.\n\nThe SDK and integration with App Insights provide a lot more capabilities, for example:\n\nComplete activity tracing including all dependencies.\nLUIS telemetry, including non-functional such as latency, error rate, and functions such as intent distribution, intent sentiment, and more.\nQnA telemetry including non-functional such as latency, error rate, and functional such QnA score and relevance.\nWord Cloud, common and utterances showing for top most used words and phrases – this can help in case you missed some intents or QnA.\nConversation length expressed in term of time and step-count.\nCome up with your own reports using custom queries.\nCustom logging to your bot.\n\nSolutions\nThe creation of a high-quality conversational experience requires a foundational set of capabilities. To help customers and partners succeed with building great conversational experiences, we released the enterprise bot template at Microsoft Ignite 2018. This template brings together all the best practices and supporting components we've identified through the building of conversational experiences.\n\nSynchronized with the SDK 4.2 release, we have delivered updates to the enterprise template which provides additional localization for LUIS models and responses including multi-language dispatcher support for customers that wish to support multiple native languages in one bot deployment. We’ve also replaced custom telemetry work with the new native SDK support for dialog telemetry and a new Conversational Analytics Power BI dashboard providing deep analytics into usage, dialog quality, and more.\nThe enterprise template is now joined by retail customer support focused template which provides further LUIS models for this scenario and example dialogs for order management, stock availability, and store location.\nThe virtual assistant solution accelerator, which enables customers and partners to build their own virtual assistants tailored to their brand and scenarios, has continued to evolve. Ignite was our first crucial milestone for virtual assistant and skills. Work has continued with regular updates to all elements of the overall solution.\nWe now have full support for six languages including Chinese for the virtual assistant and skills. The productivity skills (i.e., calendar, email, and tasks) have updated conversation flows, entity handling, new pre-built domain language models, and work with Microsoft Graph. This release also includes the first automotive capabilities enabling control of car features along with updates to skills enabling proactive experiences, Speech DDK integration, and experimental skills (restaurant booking and news).\nLanguage Understanding December update\nDecember was a very exciting month for Language Understanding in Microsoft. On December 4, 2018, we announced Docker container support for LUIS in public preview. Hosing LUIS runtime on containers provides a great set of benefits including:\n\nControl over data: Allow customers to use the service with complete control over their data. This is essential for customers that cannot send data to the cloud but need access to the technology. Support consistency in hybrid environments – across data, management, identity, and security.\n\n\nControl over model updates: Provide customers flexibility in versioning and updating of models deployed in their solutions.\n\n\nPortable architecture: Enable the creation of a portable application architecture that can be deployed in the cloud, on-premises, and the edge.\n\n\nHigh throughput/low latency: Provide customers the ability to scale for high throughput, low latency, requirements by enabling Cognitive Services to run in Azure Kubernetes Service physically close to their application logic, and data.\n\nWe recently posted a technical reference blog, “Getting started with Cognitive Services Language Understanding container.” We also posted a demo video, “Language Understanding – Container Support,” which shows how to run containers.\nLUIS has expanded its service to seven new regions completing worldwide availability in all major Azure regions including UK, India, Canada, and Japan.\nAmong the other notables is the enhancement of the training experience. This included the improvement in the time required to train application. The team also released new pre-built entity extractors for people’s names, and geographical locations in English and Chinese, and expanded the phone number, URL, and email entities across all languages.\nQnA Maker updates\nIn December, the QnA Maker service released an improvement for its intelligent extraction capabilities. Along with accuracy improvements for existing supported sources, QnA Maker can now extract information from simple “Support” URLs. Read more about extraction and supported data sources in the documentation, “Data sources for QnA Maker content.” QnA Maker also rolled out an improved ranking and scoring algorithm for all English KBs. Details on the confidence score can be found on documentation.\nThe team also released SDKs for the service in .NET, Node.js, Go, and Ruby.\nWeb chat speech update\n\nWe now support the new Cognitive Services Speech to Text and Text to Speech services directly in Web Chat 4.2. This sample is a great place to learn about the new feature and start migrating your bot from Bing Speech to the new Speech Services.\nWe also added a few samples including backchannel injection and minimize mode. Backchannel injection demonstrates how to add sideband data to outgoing activities. You can leverage this technique to send browser language and time zone information alongside with messages sent by the user. Minimize mode sample shows how to load Web Chat on-demand and overlay it on top of your existing web page.\nYou can read more about Web Chat 4.2 in our changelog.\nGet started\nAs we continue to improve our conversational AI tools and framework, we look forward to seeing what conversational experiences you will build for your customers. Get started today!\n", "link": "https://azure.microsoft.com/en-us/blog/conversational-ai-updates-december-2018/", "Role": "Principal Program Manager, Azure Platform"},
{"Title": "New updates to Azure AI expand AI capabilities for developers", "Date": "Posted on March 26, 2019", "Contributor": "Anand Raman", "Content": "\nAs companies increasingly look to transform their businesses with AI, we continue to add improvements to Azure AI to make it easy for developers and data scientists to deploy, manage, and secure AI functions directly into their applications with a focus on the following solution areas:\n\nLeveraging machine learning to build and train predictive models that improve business productivity with Azure Machine Learning.\nApplying an AI-powered search experience and indexing technologies that quickly find information and glean insights with Azure Search.\nBuilding applications that integrate pre-built and custom AI capabilities like vision, speech, language, search, and knowledge to deliver more engaging and personalized experiences with our Azure Cognitive Services and Azure Bot Service.\n\nToday, we’re pleased to share several updates to Azure Cognitive Services that continue to make Azure the best place to build AI. We’re introducing a preview of the new Anomaly Detector Service which uses AI to identify problems so companies can minimize loss and customer impact. We are also announcing the general availability of Custom Vision to more accurately identify objects in images. \nFrom using speech recognition, translation, and text-to-speech to image and object detection, Azure Cognitive Services makes it easy for developers to add intelligent capabilities to their applications in any scenario. To this date more than a million developers have already discovered and tried Cognitive Services to accelerate breakthrough experiences in their application.\nAnomaly detection as an AI service\nAnomaly Detector is a new Cognitive Service that lets you detect unusual patterns or rare events in your data that could translate to identifying problems like credit card fraud.\nToday, over 200 teams across Azure and other core Microsoft products rely on Anomaly Detector to boost the reliability of their systems by detecting irregularities in real-time and accelerating troubleshooting. Through a single API, developers can easily embed anomaly detection capabilities into their applications to ensure high data accuracy, and automatically surface incidents as soon as they happen.\nCommon use case scenarios include identifying business incidents and text errors, monitoring IoT device traffic, detecting fraud, responding to changing markets, and more. For instance, content providers can use Anomaly Detector to automatically scan video performance data specific to a customer’s KPIs, helping to identify problems in an instant. Alternatively, video streaming platforms can apply Anomaly Detector across millions of video data sets to track metrics. A missed second in video performance can translate to significant revenue loss for content providers that monetize on their platform.\nCustom Vision: automated machine learning for images\nWith the general availability of Custom Vision, organizations can also transform their business operations quickly and accurately identifying objects in images.\nPowered by machine learning, Custom Vision makes it easy and fast for developers to build, deploy, and improve custom image classifiers to quickly recognize content in imagery. Developers can train their own classifier to recognize what matters most in their scenarios, or export these custom classifiers to run them offline and in real time on iOS (in CoreML), Android (in TensorFlow), and many other devices on the edge. The exported models are optimized for the constraints of a mobile device providing incredible throughput while still maintaining high accuracy.\nToday, Custom Vision can be used for a variety of business scenarios. Minsur, the largest tin mine in the western hemisphere, located in Peru, applies Custom Vision to create a sustainable mining practice by ensuring that water used in the mineral extraction process is properly treated for reuse on agriculture and livestock by detecting treatment foam levels. They used a combination of Cognitive Services Custom Vision and Azure video analytics to replace a highly manual process so that employees can focus on more strategic projects within the operation.\n\nScreenshot of the Custom Vision platform, where you can train the model to detect unique objects in an image, such as your brand’s logo.\nStarting today, Custom Vision delivers the following improvements:\n\nHigh quality models – Custom Vision features advanced training with a new machine learning backend for improved performance, especially on challenging datasets and fine-grained classification. With advanced training, you can specify a compute time budget and Custom Vision will experimentally identify the best training and augmentation settings.\nIterate with ease – Custom Vision makes it simple for developers to integrate computer vision capabilities into applications with 3.0 REST APIs and SDKs. The end to end pipeline is designed to support the iterative improvement of models, so you can quickly train a model, prototype in real world conditions, and use the resulting data to improve the model which gets models to production quality faster.\nTrain in the cloud, run anywhere – The exported models are optimized for the constraints of a mobile device, providing incredible throughput while still maintaining high accuracy. Now, you can also export classifiers to support Azure Resource Manager (ARM) for Raspberry Pi 3 and the Vision AI Dev Kit.\n\nFor more information, visit the Custom Vision Service Release Notes.\nGet started today\nToday’s milestones illustrate our commitment to make the Azure AI platform suitable for every business scenario, with enterprise-grade tools that simplify application development, and industry leading security and compliance for protecting customers’ data.\nTo get started building vision and search intelligent apps, please visit the Cognitive Services site.\n", "link": "https://azure.microsoft.com/en-us/blog/new-updates-to-azure-ai-expand-ai-capabilities-for-developers/", "Role": "Group Product Manager, Azure AI"},
{"Title": "Happy? Sad? Angry? This Microsoft tool recognizes emotions in pictures", "Date": "Posted on November 16, 2015", "Contributor": "Julia Nikitina", "Content": "\nHumans have traditionally been very good at recognizing emotions on people’s faces, but computers? Not so much.\nThat is, until now. Recent advances in the fields of machine learning and artificial intelligence are allowing computer scientists to create smarter apps that can identify things like sounds, words, images – and even facial expressions.\nThe Microsoft Project Oxford team today announced plans to release public beta versions of new tools that help developers take advantage of those capabilities, including one that can recognize emotion. Chris Bishop, head of Microsoft Research Cambridge in the United Kingdom, showed off the emotion tool earlier today in a keynote talk at Future Decoded, a Microsoft conference on the future of business and technology.Read more...\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-11-16/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Microsoft Cognitive Services Hack: Fake News Chrome Extension", "Date": "Posted on December 6, 2016", "Contributor": "Cornelia Carapcea", "Content": "\nAt the recent HackPrinceton hackathon, a team of four students tackled the issue of fake news, by building a Chrome extension for Facebook. The extension, called “FiB”, takes signals from Microsoft Cognitive Services API’s to classify news posts as “Verified” or “Non-Verified”.   The student’s FiB app utilizes the following Microsoft Cognitive Services APIs:\n\nComputer Vision OCR operation is used to understand text from Twitter snapshots\nComputer Vision Adult operation is used to classify potentially NSFW content in images.\nBing Search API is used to search for the topic of the article, and return the most relevant, trustworthy source for the topic.\n\nThe signals above get combined with other signals like website reputation and likelihood of malware and phishing; the resulting “Verified” or “Non-Verified” flag gets displayed to the user as part of their Facebook feed via the Chrome extension. The team also wants to use Text Analytics API in the future to help with keyword extraction from the article.\nThe team is made up of four students with varying coding experience:\n\nNabanita De is a second-year Masters in Computer Science student at the University of Massachusetts Amherst\nAnant Goel is a Freshman at Purdue University\nMark Craft is a Sophomore at the University of Illinois\nQinglin Chen is a Sophomore at the University of Illinois\n\nNabanita, Anant, Mark and Qinglin had 36 hours to complete their hack. They were able to get started on Cognitive Services very fast, by signing up for free keys and reading through the multiple code sample at https://www.microsoft.com/cognitive-services .\nThe work caught the attention of Business Insider and Washington Post, and the students published some of their code to GitHub.\nThe Microsoft Cognitive Services crew is happy to see the students’ initiative and the positive pickup it has had so far. We believe this is a good example of how one can bake AI into their app and provide real value to their users. To further support this initiative, Microsoft has created higher-transaction free keys for Nabanita, Anant, Mark and Qinglin.\nIf you have been using Cognitive Services, and you have questions or suggestions, please engage with Microsoft Cognitive Services at StackOverflow and UserVoice!\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-12-06/", "Role": "Principal Product Lead, Cognitive Services"},
{"Title": "Microsoft Cognitive Services updates - Bing Entity Search API and Project Prague", "Date": "Posted on July 12, 2017", "Contributor": null, "Content": "\nThis blog post was authored by the Microsoft Cognitive Services Team. \nMicrosoft Cognitive Services enables developers to augment the next generation of applications with the ability to see, hear, speak, understand, and interpret needs using natural methods of communication.\nToday, we are excited to announce several service updates:\n\nWe are launching Bing Entity Search API, a new service available in Free Preview which makes it easy for developers to build experiences that leverage the power of Bing knowledge graph with more engaging contextual experiences. Tap into the power of the web to search for the most relevant entities such as movies, books, famous people, and US local businesses, and easily provide primary details and information sources about them.\nMicrosoft Cognitive Services Lab’s Project Prague is now available. Project Prague lets you control and interact with devices using gestures to have a more intuitive and natural experience.\nPresentation Translator, a Microsoft Garage project, is now available for download. It provides presenters the ability to add subtitles to their presentations in real time, in the same language for accessibility scenarios or in another language for multi-language situations. With customized speech recognition, presenters have the option to customize the speech recognition engine (English or Chinese) using the vocabulary within the slides and slide notes to adapt to jargon, technical terms, product, place names, etc. Presentation Translator is powered by the Microsoft Translator live feature, built on the Translator APIs of Microsoft Cognitive Services.\n\nLet’s take a closer look at what these new APIs and services can do for you.\nBring rich knowledge of people, places, things and local businesses to your apps with Bing Entity Search API\nAs announced today, Bing Entity Search API is a new addition in our already existing set of Microsoft Cognitive Services Search APIs, including Bing Web Search, Image Search, Video Search, News Search, Bing Autosuggest, and Bing Custom Search. This API lets you search for entities in the Bing knowledge graph and retrieve the most relevant entities and primary details and information sources about them. This API also supports searching for local businesses in the US. It helps developers easily build apps that harness the power of the web and delight users with more engaging contextual experiences.\nGet started\n\nTo get started today, let’s get a free preview subscription key on the Try Cognitive Services webpage.\nAfter getting the key, I can start sending entity search queries to Bing. It’s as simple as sending the following query:\n\n\r\nGET https://api.cognitive.microsoft.com/bing/v7.0/entities?q=mount+rainier HTTP/1.1  \r\nOcp-Apim-Subscription-Key: 123456789ABCDE  \r\nX-Search-ClientIP: 999.999.999.999  \r\nX-Search-Location: lat:47.60357;long:-122.3295;re:100  \r\nHost: api.cognitive.microsoft.com  \r\n\nThe request must specify the q query parameter, which contains the user's search term, and the Ocp-Apim-Subscription-Key header. For location aware queries like restaurants near me, it’s important to also include the X-Search-Location and X-MSEdge-ClientIP headers.\nFor more information about getting started, see the documentation page Making your first entities request.\nThe response\nThe following shows the response to the Mount Rainier query.\n\r\n{\r\n    \"_type\" : \"SearchResponse\",\r\n    \"queryContext\" : {\r\n        \"originalQuery\" : \"mount rainier\"\r\n    },\r\n    \"entities\" : {\r\n        \"queryScenario\" : \"DominantEntity\",\r\n        \"value\" : [{\r\n            \"contractualRules\" : [{\r\n                \"_type\" : \"ContractualRules\\/LicenseAttribution\",\r\n                \"targetPropertyName\" : \"description\",\r\n                \"mustBeCloseToContent\" : true,\r\n                \"license\" : {\r\n                    \"name\" : \"CC-BY-SA\",\r\n                    \"url\" : \"http:\\/\\/creativecommons.org\\/licenses\\/by-sa\\/3.0\\/\"\r\n                },\r\n                \"licenseNotice\" : \"Text under CC-BY-SA license\"\r\n            },\r\n            {\r\n                \"_type\" : \"ContractualRules\\/LinkAttribution\",\r\n                \"targetPropertyName\" : \"description\",\r\n                \"mustBeCloseToContent\" : true,\r\n                \"text\" : \"en.wikipedia.org\",\r\n                \"url\" : \"http:\\/\\/en.wikipedia.org\\/wiki\\/Mount_Rainier\"\r\n            },\r\n            {\r\n                \"_type\" : \"ContractualRules\\/MediaAttribution\",\r\n                \"targetPropertyName\" : \"image\",\r\n                \"mustBeCloseToContent\" : true,\r\n                \"url\" : \"http:\\/\\/en.wikipedia.org\\/wiki\\/Mount_Rainier\"\r\n            }],\r\n            \"webSearchUrl\" : \"https:\\/\\/www.bing.com\\/search?q=Mount%20Rainier...\",\r\n            \"name\" : \"Mount Rainier\",\r\n            \"image\" : {\r\n                \"name\" : \"Mount Rainier\",\r\n                \"thumbnailUrl\" : \"https:\\/\\/www.bing.com\\/th?id=A21890c0e1f...\",\r\n                \"provider\" : [{\r\n                    \"_type\" : \"Organization\",\r\n                    \"url\" : \"http:\\/\\/en.wikipedia.org\\/wiki\\/Mount_Rainier\"\r\n                }],\r\n                \"hostPageUrl\" : \"http:\\/\\/upload.wikimedia.org\\/wikipedia...\",\r\n                \"width\" : 110,\r\n                \"height\" : 110\r\n            },\r\n            \"description\" : \"Mount Rainier, Mount Tacoma, or Mount Tahoma is the highest...\",\r\n            \"entityPresentationInfo\" : {\r\n                \"entityScenario\" : \"DominantEntity\",\r\n                \"entityTypeHints\" : [\"Attraction\"],\r\n                \"entityTypeDisplayHint\" : \"Mountain\"\r\n            },\r\n            \"bingId\" : \"9ae3e6ca-81ea-6fa1-ffa0-42e1d78906\"\r\n        }]\r\n    }\r\n}\r\n\nFor more information about consuming the response, please refer to the documentation page Searching the Web for entities and places.\nTry it now\nDon’t hesitate to try it by yourself by going to the Entities Search API Testing Console.\nCreate more natural user experiences with gestures - Project Prague\nProject Prague is a cutting edge, easy-to-use SDK that helps developers and UX designers incorporate gesture-based controls into their apps. It enables you to quickly define and implement customized hand gestures, creating a more natural user experience.\nThe SDK enables you to define your desired hand poses using simple constraints built with plain language. Once a gesture is defined and registered in your code, you will get a notification when your user does the gesture, and can select an action to assign in response.\nUsing Project Prague, you can enable your users to intuitively control videos, bookmark webpages, play music, send emojis, or summon a digital assistant.\n\n\nLet’s say that I want to create new gesture to control my app \"RotateRight”. First, I need to ensure that I have the hardware and software requirements. Please refer to the requirement section for more information. Intuitively, when performing the \"RotateRight\" gesture, a user would expect some object in the foreground application to be rotated right by 90°. We have used this gesture to trigger the rotation of an image in a PowerPoint slideshow in the video above.\nThe following code demonstrates one possible way to define the \"RotateRight\" gesture:\n\r\nvar rotateSet = new HandPose(\"RotateSet\", new FingerPose(new[] { Finger.Thumb, Finger.Index }, FingerFlexion.Open, PoseDirection.Forward),\r\n                                          new FingertipPlacementRelation(Finger.Index, RelativePlacement.Above, Finger.Thumb),\r\n                                          new FingertipDistanceRelation(Finger.Index, RelativeDistance.NotTouching, Finger.Thumb));\r\n\r\nvar rotateGo = new HandPose(\"RotateGo\", new FingerPose(new[] { Finger.Thumb, Finger.Index }, FingerFlexion.Open, PoseDirection.Forward),\r\n                                        new FingertipPlacementRelation(Finger.Index, RelativePlacement.Right, Finger.Thumb),\r\n                                        new FingertipDistanceRelation(Finger.Index, RelativeDistance.NotTouching, Finger.Thumb));\r\n\r\nvar rotateRight = new Gesture(\"RotateRight\", rotateSet, rotateGo);\r\n\nThe \"RotateRight\" gesture is a sequence of two hand poses, \"RotateSet\" and \"RotateGo\". Both poses require the thumb and index to be open, pointing forward, and not touching each other. The difference between the poses is that \"RotateSet\" specifies that the index finger should be above the thumb and \"RotateGo\" specifies it should be right of the thumb. The transition between \"RotateSet\" and \"RotateRight\", therefore, corresponds to a rotation of the hand to the right.\nNote that the middle, ring, and pinky fingers do not participate in the definition of the \"RotateRight\" gesture. This makes sense because we do not wish to constrain the state of these fingers in any way. In other words, these fingers are free to assume any pose during the execution of the \"RotateRight\" gesture.\nHaving defined the gesture, I need to hook up the event indicating gesture detection to the appropriate handler in your target application:\n\r\nrotateRight.Triggered += (sender, args) => { /* This is called when the user performs the \"RotateRight\" gesture */ };\nThe detection itself is performed in the Microsoft.Gestures.Service.exe process. This is the process associated with the \"Microsoft Gestures Service\" window discussed above. This process runs in the background and acts as a service for gesture detection. I will need to create a GesturesServiceEndpoint instance in order to communicate with this service. The following code snippet instantiates a GesturesServiceEndpoint and registers the \"RotateRight\" gesture for detection:\n\r\nvar gesturesService = GesturesServiceEndpointFactory.Create();\r\nawait gesturesService.ConnectAsync();\r\nawait gesturesService.RegisterGesture(rotateRight);\r\nWhen you wish to stop the detection of the \"RotateRight\" gesture, you can unregister it as follows:+ \r\nC#Copy\r\nawait gesturesService.UnregisterGesture(rotateRight);\r\n\nThe handler will no longer be triggered when the user executes the \"RotateRight\" gesture. When finished working with gestures, keep in mind I should dispose the GesturesServiceEndpoint object:\n\n\r\ngesturesService?.Dispose();\r\n\nPlease note that in order for the above code to compile, you will need to reference the following assemblies, located in the directory indicated by the MicrosoftGesturesInstallDir environment variable:\n\nMicrosoft.Gestures.dll\nMicrosoft.Gestures.Endpoint.dll\nMicrosoft.Gestures.Protocol.dll\n\nFor more information about the Getting Started guide, please refer to the documentation.\nThank you again and happy coding!\n\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-cognitive-services-updates-bing-entity-search-api-and-project-prague/", "Role": null},
{"Title": "Build custom video AI workflows with Video Indexer and Logic Apps", "Date": "Posted on July 19, 2017", "Contributor": "Anika  Zaman", "Content": "\nUpdate May 7, 2020: For the most up to date information on the Video Indexer Logic App and Power Automate connectors, please see our Tutorial: Use Video Indexer with Logic App and Power Automate documentation.\nWith the Video Indexer connector for Logic Apps, you can now set up custom workflows connecting your most used apps with Video Indexer to further automate the process of extracting deep insights from your videos.\nIn this blog, I will walk you through an example of using the Video Indexer connector in a Logic App to set up a workflow where whenever a new video file is created in a specific folder of your OneDrive, the video is automatically uploaded and indexed. Once completed, the insights of the newly indexed video will be stored as a JSON file in the designated folder of your OneDrive.\nLimitations to note\nCurrently, there is a 50 MB file size limit for OneDrive and other storage connectors to trigger. The Video Indexer connector allows you to upload a video via file content from a storage connector or a shared access signature URL. Although there is currently no way to access a URL to the video from storage connectors, we are in the process of adding in this feature on OneDrive, OneDrive for Business, and Azure Storage. Once implemented, we will be able to work with videos larger than 50 MB. However, we have to work with the limit for now.\nSetting up the Logic App\nTo begin, log into your Azure Portal and create a new Logic App. You can follow the tutorial to learn how to create and deploy a new Logic App.\nOnce you have created the Logic App, go to the Logic Apps Designer and select Blank Logic App.\n \nThe first thing we will need is a “Trigger” that will fire off an event when a new file has been created in your OneDrive folder for videos.\nIn the search bar for connectors and triggers, search for “OneDrive”. You will see options for OneDrive (consumer) and OneDrive for Business. You can do either based on the account that you have or want because they have similar steps. In this tutorial, I am using OneDrive.\nClick on the OneDrive connector. This will show you all of the triggers available for OneDrive.\n\nSelect “When a file is created”. This will fire the trigger in the Logic App each time a new file is dropped into the designated OneDrive folder. Once complete you will be prompted to sign into your OneDrive account.\n\nAfter you have signed in, you will see the trigger and its different fields. For the Folder field, click on the folder icon and navigate to your folder for videos. I have selected a folder for videos on my OneDrive called “Video”.  You can choose any folder that is appropriate or create a new folder specific to your own workflow. It is important to note that this folder should only have video or audio files. Any other files will result in an upload error in the Video Indexer connector.\nYou can also set how often you want the trigger to check whether a file has been created in the specified folder. Under the “How often do you want to check for items?” section, I have set Frequency to Minute and Interval to 3 to have my trigger check every 3 minutes.\n\nNext, you will need to set an action that uploads the video that has been created in your OneDrive folder to your Video Indexer portal. Click Next Step and select Add an action.\n\nSearch for “Video Indexer” and select the Video Indexer connector. You should see the different actions listed out. We currently do not have any triggers for the Video Indexer, however, triggers will come later when support for WebHooks will be added to the Video Indexer API.\n    \nYou should see two options for uploading a video onto your Video Indexer portal. One is called Upload video and index and will allow you to upload a video using file content data. The other option is called Upload video and index (using a URL) and will allow you to upload a video using an URL. Both options will automatically index the videos upon upload.\nIn this tutorial we will be uploading the video using file content data, so select Upload video and index.\nYou should be prompted to create a connection using your Video Indexer API Key. Enter in a name for the connection as well as your API Key. You can follow the tutorial to learn how to subscribe to the Video Indexer API and how to access your API key.\n\nUpon creating the connection, it should open the Upload video and index action. If you click on any of the fields, you should see response elements from the OneDrive trigger. For the File Content field, select the File content response element. For Video Name, you can select the File name response element or type any name that you want. Set your privacy as you want. Here, I have set privacy to “Private”.\n\nUpon clicking Show advanced options, you will see many more fields that you can fill out to provide more information on your video. I will be leaving them blank here because they are not required fields.\n\nThe Upload video and index action returns the id of the video upon upload, however, that does not mean that the indexing has been completed. For this, you need to add in a check that will only let the Logic App move forward if the video has been fully processed. Select New Step and then More. You can then select Add a do until.\n\nYou should now see an Until loop. Within the Until loop, select Add an action. Search for the Video Indexer connector again and select the action Get processing state. For the Video Id field, select the Video Id response element from Upload video and index.\n   \nFor the “Choose a value” field in the Until loop, select the State response element from Get processing state. For the field that says “Choose a value”, type in “Processed”. The State being “Processed” is an indication that the indexing of the video is complete.\n\nWithin the Until loop and after the Get processing state action, select Add an action. Search for and select the Delay action (it is a part of the Schedule connector). You will need to set the count and unit fields to essentially determine how often to check if the State is “Processed”. Here, I have set Count to “3” and Unit to “Minute” to check every 3 minutes.\n\nThe next step is to attain the insights of the newly processed video. Outside of the Until loop, select New Step and search for the Video Indexer Connector. Select the Get video breakdown action. For the Video Id field, select the Video Id response element from Upload video and index. This action will give you all of the insights of the video.\n\nNow that you have the insights of the video through Get video breakdown, you will now create a file with the new insights and store it in an appropriate folder of your OneDrive.\nSelect Add an action and search for the OneDrive connector. Select the action Create file. For the Folder path field, click on the folder icon and navigate to the appropriate folder for storing the insights of your video. I chose my folder called “Insights”.\nFor the File name field, type or select a name for the new text file. Here, I selected the Name response element from the Get video breakdown and typed in “Insights” after. For the File content field, select Summarized Insights or whichever specific response element from Get video breakdown that you want to store. Learn more about the response elements.\n\nSave your logic app, and you are done! You should now test the logic app.\nTesting the Logic App\nStart by selecting Run. Then, to trigger the logic app, upload a video file onto the OneDrive video folder that you specified in the trigger. As mentioned before, there is a 50 MB file size limit for the OneDrive trigger, so you will need to select your video file appropriately. \nYou should be able to look at the run details of your Logic App under the Run History section of the Overview page of your logic app.\n\nYou are now ready to test out many different combinations of workflows using the Video Indexer connector to find what works best to make your processes automated and more efficient.\nYou can create custom workflows that integrate live and on demand workflows in Media Services with Video Indexer using samples from the Media Services GitHub site and the Video Indexer connector as long as your video files are within the 50 MB limit for now. You can also create a Logic App to push the insights from Video Indexer into systems like Cosmos DB and use Azure Search to query across the metadata or to join the insights to your own custom metadata.\n", "link": "https://azure.microsoft.com/en-us/blog/build-custom-video-ai-workflows-with-video-indexer-and-logic-apps/", "Role": "Program Manager, Azure Media Services, Video Indexer"},
{"Title": "Microsoft Video Indexer expands functionality unlocking more video insights", "Date": "Posted on September 13, 2017", "Contributor": "Milan Gada", "Content": "\nIn May 2017, we announced the global public preview of the Video Indexer service at the Microsoft Build Developers Conference in Seattle. As part of Microsoft Cognitive Services, Video Indexer was introduced as a unique integrated bundling of Microsoft's cloud-based artificial intelligence and cognitive capabilities applied specifically for video content. It has since become the industry's most comprehensive Video AI service available, making it easy to extract insights from videos. For the last four months, we have received a tremendous, enthusiastic response from our global customers and partners who began using Video Indexer within their own in-house workflows and video solutions. With the International Broadcasters Conference (IBC) just around the corner, we are excited to share significant feature updates to the Video Indexer service.\nAdditionally, we are very pleased that several customers and partners have already integrated Video Indexer within their own video solutions and product offerings. A few examples are as follows:\n\nOoyala has integrated Video Indexer into their Flex Media Logistics platform to enable auto-transcription, translation, content-aware advertising insertion, and better content monetization and search.\nZone TV is using Video Indexer to automate the curation of a first-of-its-kind customizable suite of linear TV channels.\nAxle Video is taking advantage of Video Indexer to automatically tag entire video files and video segments to greatly accelerate the post-production process and efficient search across video libraries.\n\nOverall Service updates since May\n\nSpeech-to-text – Video Indexer now supports Egyptian Arabic for speech-to-text.\r\n\r\n \nWith the addition of Arabic, Video Indexer portal has been updated to support RTL (right to left) text in the transcript tab as well as the search page.\n\n\nCaptions – You can now start playing captions automatically in the Video Indexer player by adding showCaptions=true to the iframe embed player URL.\nDownload insights from portal – You can now download the JSON insights extracted by Video Indexer via the portal.\nPlayback speeds – The Video Indexer player was updated to support different playback speeds.\nInline transcript editing – The transcript panel has been updated to support inline editing.\nEditing in insights widget – The insights widget has been updated to support editing in embedded mode (based on an access token).\nKeywords editing – The insights widget has been updated to enable the addition and removal of keywords.\nAnnotations – The insights widget now shows the annotations extracted by the Video Indexer engine.\n\nAPI updates\nThe Video Indexer API reference page provides details about the APIs. Some APIs are new, while some have been enhanced with new features.\n\nUpload – The Upload API has been enhanced with the following:\r\n\r\n \ncallbackUrl – This enables you to receive a callback when the indexing operation finishes on a video.\nindexingPreset – You can specify if you want to use the default preset or only perform audio indexing.\nstreamingPreset – With this parameter, you can control whether the video gets encoded to multiple bitrates or not. You may want to disable encoding if you are only interested in extracting insights from your videos. This would help improve the turn around on processing the video.\n\n\nRe-Index Breakdown – This is a new API and will enable you to reprocess a video by re-triggering insights extraction on an existing video in your account.\nRe-Index Breakdown by External Id – Similar to the above, but this API will enable you to use the external id versus the id provided by Video Indexer to trigger the re-indexing of the video.\nUpdate Transcript – This API enables you to update the transcript for a video by providing a JSON string containing the full video VTT.\nUpdate Face Name – This is an API for providing a friendly name to the faces detected by Video Indexer.\nGet Insights Widget URL – This API has been updated with an optional parameter called “allowEdit” that would allow the user to get an access token for editing in embedded mode.\nGet Insights Widget URL by External Id – Similar to the above, but this API will enable you to use the external id versus the id provided by Video Indexer.\n\nOther updates\n\nLogic Apps – With the Video Indexer connector for Logic Apps, you can now setup custom workflows connecting your most used apps with Video Indexer to further automate the process of extracting insights from your video. You can read more about this in the Video Indexer connector for Logic Apps blog.\nMicrosoft Flow – With the integration between Video Indexer and Microsoft Flow, users can now extract insights from their business videos without writing a single line of code.\nUI updates – The Video Indexer web portal UI has been updated with several enhancements based on feedback provided by customers.\n\nBesides all the above listed updates, Video Indexer backend services have been updated to make the service more robust and efficient. Looking ahead, Video Indexer will be making progress along the following dimensions:\n\nMore video AI algorithms to extract additional metadata.\nImproving accuracy of video AI technologies.\nMore intelligent correlation among the extracted metadata to provide human friendly insights.\nUser interface enhancement to showcase the extracted metadata and insights.\nPaid offering of Video Indexer.\n\nPlease submit your feedback related to Video Indexer on uservoice. Also, you can track updates from Video Indexer team by following us on Twitter (@Video_Indexer). Members of the Video Indexer team will be present at the Microsoft Stand (Hall 15, Stand 1, 35, and 36). Come see us to learn more!\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-video-indexer-expands-functionality-unlocking-more-video-insights/", "Role": "Principal Program Manager, Azure Media Services"},
{"Title": "Custom Vision Service introduces classifier export, starting with CoreML for iOS 11", "Date": "Posted on September 14, 2017", "Contributor": "Anna Roth", "Content": "\nTo enable developers to build for the intelligent edge, Custom Vision Service from Microsoft Cognitive Services has added mobile model export.\nCustom Vision Service is a tool for easily training, deploying, and improving custom image classifiers. With just a handful of images per category, you can train your own image classifier in minutes. Today, in addition to hosting your classifiers at a REST endpoint, you can now export models to run offline, starting with export to the CoreML format for iOS 11. Export will allow you to embed your classifier directly in your application and run it locally on a device. The models you export are optimized for the constraints of a mobile device, so you can classify on device in real time.\nCustom Vision Service is designed to build quality classifiers with very small training datasets, helping you build a classifier that is robust to differences in the items you are trying to recognize and that ignores the things you are not interested in. With today's update, you can easily add real time image classification to your mobile applications. Creating, updating, and exporting a compact model takes only minutes, making it easy to build and iteratively improve your application. More export formats and supported devices are coming in the near future.\nA sample app and tutorial for adding real time image classification to an iOS app is now available.\nTo learn and starting building your own image classifier, visit www.customvision.ai.\n\nScreenshot of a fruit recognition classifier in our sample app.\n", "link": "https://azure.microsoft.com/en-us/blog/custom-vision-service-introduces-classifier-export-starting-with-coreml-for-ios-11/", "Role": "Senior Program Manager, Microsoft Cognitive Services"},
{"Title": "Diving deep into what’s new with Azure Machine Learning", "Date": "Posted on September 25, 2017", "Contributor": "Matt Winkler", "Content": "\nEarlier today, we disclosed a set of major updates to Azure Machine Learning designed for data scientists to build, deploy, manage, and monitor models at any scale. This has been in private preview for the last 6 months, with over 100 companies, and we’re incredibly excited to share these updates with you today. This post covers the learnings we’ve had with Azure Machine Learning so far, the trends we’re seeing from our customers today, the key design points we’ve considered in building these new features, and dive into the new capabilities.\nLearnings so far\nWe launched Azure Machine Learning Studio three years ago, designed to enable established data scientists and those new to the space to easily compose and deploy ML models. Before the term was in use, we enabled serverless training of experiments built by graphically composing from a rich set of modules, and then deploying these as a web service with the push of a button. The service serves billions of scoring requests on top of hundreds of thousands of models built by data scientists. It has been incredibly rewarding to see how the service has been used by our customers including:\n\nTeaching Data Science in high school at the Australian School for Girls \nPredictive maintenance at Rolls-Royce\nRecognition of snow leopards from remote camera stations with the Snow Leopard Trust\n\nOver time we’ve worked with many customers who are looking for the next level of power and control and the capabilities we announced today address those desires. As we look at the data science workflow, we see customers walking through the following stages:\n\nKey trends\nOver the last few years, we’ve interacted with customers in every industry, with varying amounts of experience with ML, solving problems across every domain. Reflecting on those engagements, we see the following trends:\nAccelerating adoption of ML and AI by developers – When we talk with developers, they are looking to create immersive, personalized, and productive experiences for their customers. More and more, we are seeing ML and AI capabilities becoming part of the way apps are being written. Within Microsoft, it’s been exciting to see how developers across the company are using AI in the applications we’re building. Two of my favorite examples come from the PowerPoint team. They have leveraged Microsoft Cognitive Services to build real-time translation and captioning capabilities and language understanding tools to transform bullet point lists into timelines. The Microsoft Cognitive Services, a set of 30 ready-to-use AI APIs, have been used by hundreds of thousands of developers looking to enhance their applications. This demand for AI by developers will only increase, further pushing organizations to provide easy to consume AI built on their data, as the way we write software evolves around these new capabilities.\nHybrid training and scoring – Every customer has a unique set of requirements on their data. Compliance, cost, regulation, physics, and existing processes and applications are all factors in where data and decisions can live. This poses a data management and movement challenge, as well as a need for tools, services, and frameworks that can operate across all data. Additionally, we need the ability to deploy the model in many places. While we see customers consolidating large amounts of data into data lakes and using tools like Spark for preparing and analyzing their data, the models they produce need to be deployed to a variety of form factors. We’ve seen two common hybrid patterns. The first involves sensitive data residing in an on-premises system being used to train a model that then gets deployed to the cloud or an application. The second involves training models on vast amounts of data ingested into the cloud (such as an IOT application), and then deploying that model on-premises or in a disconnected environment.\nScoring as close to the event as possible – Following on the above point, it is becoming more important for developers to be able to consume models everywhere, and we see a rise in edge and on-device based scoring. Recently, our Custom Vision Service enabled the ability to output a trained recognition model to CoreML for consumption directly within iOS. Customers in the IOT space are looking to put models directly onto devices, or onto gateway devices that can operate independently. Whether looking to address latency or the ability to support scoring even while disconnected, customers want the flexibility to train a model anywhere but control their deployment to place scoring as close to the event as possible.\nAccelerating diversification of hardware, frameworks, and tools – One of the most exciting aspects of working in this space is the pace of innovation that we’re seeing across every layer of the stack. There is a dizzying array of tools to choose from, each being used in amazing ways. At the hardware layer, we see exponential increases in processing capabilities across CPU, GPU, FPGAs, and more. Over time, this innovation will result in mature toolchains, all the way to the hardware level that are optimized for specific workloads, letting customers tune and tradeoff cost, latency, and control. In the short term, we see customers experimenting to discover the best set of tools for their use case.\nDesign points\nGiven the learnings we’ve had, we’ve anchored our design on the following four points to shape these new capabilities\nMachine Learning at scale\nThe services and tools we build must operate at scale, and we see customers encountering challenges with at least five different dimensions of “scale.”\nOperating across all data – Customers must be able to have access to all their data to use building models at scale. In many cases, this means being able to use tools like Spark on increasing amounts of data in a data lake, but this is only part of the challenge. Customers need to be able to find and acquire the data, and then they need to be able to understand and shape the data. None of those stages can be bottlenecked as data sizes increase, so our tools need to scale from a local CSV file to petabyte-scale data lakes in the cloud.\nOperating at increased compute needs (up and out) – Beyond the expansion of data, the techniques we’re using require scaling our processing power. Customers must be scale up onto machines that can fit huge datasets directly in memory, or machines with the latest GPUs for deep learning. Customers will scale out for problem sets on top of distributed data infrastructures like Spark, or for massively parallel processing in hyperparameter sweeps and model evaluation on top of our Azure Batch service. These two forces combine as we look to do deep learning at scale, requiring many scaled up machines.\nScaling the number of models and experiments you have – As teams increase the rate of experimentation, the number of experiments being run, and models being produced will increase. It becomes critical to be able to manage, monitor, and maintain those models. Teams need the ability to pinpoint and recreate any version of a model, and the number of artifacts that need to be tracked scales as organizations power more decisions with AI.\nScaling the consumption of models – Models produced, using any tools and frameworks, must be deployed in ways that enable easy scaling to serve any number of requests. For us, this means embracing container based deployment of models to enable customers fine-grained control, as well as being able to use services such as Azure Container Service to provide a scalable hosting layer in Azure.\nScaling the team – Data science teams are not teams of one. Our tools and services must enable collaboration and sharing, in a secure fashion, across all stages of the data science lifecycle. Much as source control systems have evolved for software development to flexibly support a variety of teams and processes, our system needs to support the AI development lifecycle as teams continue to grow.\nBuilding with the tools you’re using today\nThere is no one framework or toolchain which rules them all. Given the rapid diversification of the ecosystem, our customers can experiment and choose the tools that will work best for them. The acceleration of innovation in the ecosystem also means that the tools and frameworks and techniques today are going to be different than those in six months’ time. Any service and tool that we build needs to enable data scientists to pick and choose from the ecosystem and use those tools, and we must build it in a way that provides a consistent experience for training, deployment, and management as these evolve.\nOne side effect of this expansion in the tools space is a challenge with reproducibility. In 12 months’ time, how will you recreate an experiment given that the tools have changed? Much like software developers needing to checkpoint a verified set of dependencies, our system needs to ensure that as frameworks come and go, you can reliably reproduce the results.\nIncreasing the rate of experimentation\nWhen we look at the key areas of friction for data science teams, we consistently hear about challenges in:\n\nAcquiring, prepping, and understanding data\nReliably scaling training, from rapid local prototyping and scaling to larger data sets\nComparing and selecting models trained with different tools and techniques\nDeploying models to production\nGetting telemetry back from the deployed models to learn and improve the model\n\nWe believe that by eliminating the friction in each of these steps, and between these steps, teams will be able to increase their rate of experimentation. This lets them create better models more quickly, ultimately providing more value for their customers and increasing the efficiency of the team. The services and tools must reduce these key points of friction, while still preserving the flexibility and control required.\nModels everywhere\nFinally, the service we’re building must enable the deployment, management, and monitoring of models everywhere. It’s critical that our customers can have flexibility in their deployment form factor, including:\n\nDeploying to the cloud, for scalable web services for consumption by millions of devices or apps\nDeploying to a data lake, for processing data at scale in batch, interactive, and real-time streaming pipelines\nDeploying to a data engine, like SQL Server 2017, for scoring data inline for transaction processing and data warehousing\nDeploying to the edge, for moving the scoring as close to the event as possible and supporting disconnected scenarios\n\nDiving into our new capabilities\nGiven these design points, we’ve released the following new capabilities for Azure Machine Learning\nExperimentation\nThe Azure Machine Learning Experimentation service allows developers and data scientists to increase their rate of experimentation. With every project backed by a Git repository, and with a simple command line tool for managing experimentation and training runs, every execution can track the code, configuration, and data that’s used for the run. More importantly, the outputs of that experiment, from model files, log output, and key metrics are tracked, giving you a powerful repository with the history of how your model evolves over time.\nThe Experimentation service is built to allow you to leverage any Python tools and frameworks that you want to use. The experiments can run locally, inside of a Docker container locally or remotely, or scaling out on top of Spark. The power of Apache Spark in HDInsight enables data preparation and transformation, as well as training, across large amounts of your data. Deep learning based experimentation can occur on GPU accelerated virtual machines using any framework such the Microsoft Cognitive Toolkit, Tensorflow, Caffe, PyTorch, and more. Going forward, the Azure Batch AI service can be used to provide massive scaled out training, and then operationalized through the Model Management service.\nPython libraries from Machine Learning Server (revoscalepy and microsoftml) available with Azure Machine Learning include the Pythonic versions of Microsoft’s Parallel External Memory Algorithms (linear and logistic regression, decision tree, boosted tree and random forest) and the battle tested ML algorithms and transforms (deep neural net, one class SVM, fast tree, forest, linear and logistic regressions). In addition, these libraries contain a rich set of APIs for connecting with remote data sources and deployment targets. Using these libraries, one can train a model with the Experimentation Service and operationalize these models wherever Machine Learning Server runs – SQL Server, HD Insight, Windows Server, Linux, and all three distributions of Spark; this power of training and operationalization is available for virtually every Python toolkit. This gives you the incredible power to build your models and operationalize them in the production data platforms right from your favorite data science environment.\nWe know that data science isn’t a linear process, and the Experimentation service lets you look back in time to compare experiments that produced the right results. When you find the right version, it’s easy to set your project to the exact code, configuration, and data that were used so you can start development from any point in history. Finally, by tracking the environment configuration, the Experimentation service makes it easy to quickly set up and configure new environments with exactly the same configuration. The Experimentation service manages training on your local machine, in Docker containers running locally or in the cloud, or on scale-out engines in Azure like Spark on HDInsight. By changing a command line parameter, you can easily move my job from local execution to the cloud.\nModel Management\nThe Model Management service provides deployment, hosting, versioning, management, and monitoring for models in Azure, on-premises, and to IOT Edge devices. We’ve taken a key bet on Docker as the vehicle to provide customers with control, flexibility, and the convenience of containers as the mechanism for hosting models. With containers, you get a repeatable and consistent environment for hosting your models. Models are exposed via web services written in Python, giving you the ability to add more advanced logic, custom logging, state management, or other code into the web service execution pipeline. That logic is packaged up to construct a container which can be registered in your Azure Container Registry and then deployed using any standard Docker toolchain.  When deploying models at scale on an Azure Container Service cluster, we’ve built a hosting infrastructure optimized for model serving, that handles automatic scaling of containers, as well as efficiently routing requests to available containers.\nDeployed models and services can be monitored through Application Insights, giving you details on model execution, including specific model metrics, on a per-decision level. With deployed models, versions are tracked, enabling a link back to the specific code, configuration, and data used to create the model. Support is provided for managing deployments of model upgrades, enabling no-downtime while reliably deploying new versions, and allowing rollback to previous versions if required. Retraining scenarios, where a deployed model is monitored and then updated after being trained on new data, are possible, enabling continuous improvement of models based on new data.\n\nOne key benefit of this model is that it gives you full control over the deployment profile. Do you want to have a single cluster that is the host for all of your models? Do you want to deploy a cluster per department or customer? Do you have a single model that needs to support high call volumes during the day and scale down at night? Our goal is to give you control of the container hosting infrastructure in order to have the control and customization that have heard you ask for. You have full flexibility to choose the VM type and deployment profile, and we’ve made it easy to get started with the Data Science Virtual Machine as a single instance for development and testing before deploying to a cluster.\nModels deployed through the Model Management service also surface swagger as an easy mechanism to consume the services from your code. Our partners in Excel are using this capability to make it incredibly easy to discover and consume models directly from Excel.\nThe Experimentation and Model Management services work together to provide governance and lineage of deployed models all the way back to the training job used to create the model. With telemetry enabled on a deployed model, this gives you visibility into any decision and the ability to trace the decision back to the experiment that created the model. This gives you a debugging and diagnostics story across the end to end lifecycle of a model.\n\nWorkbench\nWhen we started working on this, we worked to ensure that our service is fully operable from the command line. But we also heard that there were challenges with environment set up, data wrangling, and visualizing run comparison. We’ve built the Azure Machine Learning Workbench as a companion application to your current development process, and we’re excited to get your feedback. The Azure Machine Learning Workbench is a client application that runs on Windows and Macs. It has an easy set-up and installation and will install a configured Python environment, complete with conda, Jupyter, and more, along with connectivity to all of the backend services in Azure. We intend it to be a control panel for your development lifecycle and a great way to get started using the services.\nWe’ve also put a rich set of samples into the application, with single-click training using a range of tools and techniques. You can see these by clicking “New Project.” The experimentation history management mentioned as part of the Experimentation service is shown below. You can get a glance of the evolution of key metrics over the history of your runs, get a detailed view of any individual run, and compare the performance side by side.\n\nAny visualization you’ve built in your code, using matplotlib, for instance, will be archived and stored as part of the details of the run. In the screenshot below, you can see the wordclouds being emitted following some document analysis. Also, note that the Git commit id is included, enabling you to easily checkout from that commit if you want to pick up and start editing from where you were with this experiment.\n\nFinally, runs can be compared and evaluated side by side:\n\nThe Azure Machine Learning Workbench also hosts Jupyter notebooks that can be configured to target local or remote kernels, enabling iterative development within the notebook on your laptop, or hooked up to a massive Spark cluster running on HDInsight.\n\nAI powered data wrangling\nWe know that one of the biggest areas where time is spent is in data preparation. We have consistent feedback on the challenges (and time spent) getting data, shaping it, and preparing it, and we are looking to make that better. We want to reduce the time and effort to acquire data for modeling, and we want to fundamentally change the pace with which data scientists can prepare and understand data, and accelerate the time to get to “doing data science.” As part of the Azure Machine Learning Workbench, we’re introducing a new set of data wrangling technology, powered by AI to make you more productive preparing data. We have combined a variety of techniques, using advanced research from Microsoft Research on program synthesis (PROSE) and data cleaning, to create a data wrangling experience that drastically reduces the time that needs to be spent getting data prepared. With the inclusion of a simple set of libraries for handling data sources, data scientists can focus on their code, not on changing file paths and dependencies when they move between environments. By building these experiences together, the data scientist can leverage the same tools in the small and in the large, as they scale out transparently across our cloud compute engines, simply by choosing target environments for execution.\n\nOne of the most powerful capabilities of this data wrangling tool is building your data transformations by example. We know that a lot of time is spent on formats and transformations of numbers and dates, and have made it simple to transform data by providing examples. Here I’m taking a datetime string and changing it into a day of week + 2 hour bucket that I can later use to join or summarize my data set. If the transformations provided don’t fit what I need, it’s also easy to inject custom python code or libraries which can be used to filter or transform the data.\n\nThe data transformations you build can easily be incorporated into your Python code by easily returning a pandas dataframe containing the results of the transformation.\n\nGenerating the dataframe creates the following code:\n\r\n# Use the Azure Machine Learning data wrangling package\r\nfrom azureml.dataprep import package\r\n# Use the Azure Machine Learning data collector to log various metrics\r\nfrom azureml.logging import get_azureml_logger\r\nlogger = get_azureml_logger()\r\n# This call will load the referenced package and return a DataFrame.\r\n# If run in a PySpark environment, this call returns a\r\n# Spark DataFrame. If not, it will return a Pandas DataFrame.\r\ndf = package.run('bikes.dprep', dataflow_idx=0)\r\n# Remove this line and add code that uses the DataFrame\r\ndf.head(10)\r\n\nVisual Studio Code Tools for AI\nFinally, we know that it’s important for you to use the development tools of your choice, and we want to make that experience work seamlessly with our new services in Azure. We’re pleased to announce our first editor integration with the release of the Visual Studio Code Tools for AI. This extension provides a rich set of capabilities for building models with deep learning frameworks including Microsoft Cognitive Toolkit (CNTK), Google TensorFlow, Theano, Keras and Caffe2, while integrating with the Experimentation service for executing jobs locally and in the cloud, and for deployment with the Model Management services.  \n\nGetting started\nIf you’re interested in learning more, head over and check out our documentation on how to deploy a new account in Azure. You can also find a great set of quick start guides, as well as detailed tutorials walking through the features of the service.\n\nSetup and installation\nQuick start sample – Iris \nIn depth, three part Iris classification tutorial \nIn depth data wrangling tutorial\n\nData scientists on our team have put together detailed scenario walkthroughs, complete with sample data, for you to get started on some interesting challenges or adapt their techniques to your next problem, including:\n\nAerial image classification\nDocument collection analysis\nPredictive maintenance\nTime series based forecasting\n\nWe’re constantly working on and refreshing the documentation, if you have a comment or suggestion, please let us know.\nWhat’s next\nToday, these services and tools are available as a public preview in Azure. In the coming months, we will bring the service to GA while we continue to work with customers and use their feedback to guide the key features, updates, and global rollout. We invite you to engage with the team through our feedback site, on our MSDN forum, or tag us on twitter with the #AzureML. Senior PM Ted Way will be presenting a webinar on October 4, 2017 if you’re interested in learning more.\nThis is the start of an exciting journey, and we can’t wait to work with you on it!\n", "link": "https://azure.microsoft.com/en-us/blog/diving-deep-into-what-s-new-with-azure-machine-learning/", "Role": "Group Program Manager, Machine Learning"},
{"Title": "At Ignite, Microsoft is updating its Cognitive Services collection of intelligent APIs", "Date": "Posted on September 25, 2017", "Contributor": null, "Content": "\nMicrosoft Cognitive Services enables developers to augment the next generation of applications with the ability to see, hear, speak, understand, and interpret needs using natural methods of communication. Think about the possibilities: being able to add vision and speech recognition, emotion and sentiment detection, language understanding, and search, to applications without having any data science expertise.\nToday, we are excited to announce several service updates:\n\nText Analytics API is now generally available. Text Analytics is a cloud-based service that provides advanced natural language processing over raw text. It includes API functions such as sentiment analysis, key phrase extraction and language detection.\nBing Custom Search API will be generally available in October. Bing Custom Search lets you create a highly-customized targeted web search experience to deliver more relevant results from your targeted web space through a commercial grade service.\nBing Search APIs v7 will be will be generally available in October. Allowing you to bring the immense knowledge of the planet to your applications, the v7 update will provide several improvements, such as results coming back fast with improved performance for queries on the Bing Web Search API. New sorting and filtering options make it easier to find relevant results in news trending topics and image searches. Better error messages make it easy to troubleshoot and diagnose problem queries, and updated, modernized documentation make it easy bring the power of the Bing Search APIs to your applications.\nWe plan to make Language Understanding Intelligent Service and Microsoft Bot Framework, which contains everything you need to build and connect intelligent bots, generally available later this year.\nWe’re also adding new capabilities to our services:\r\n\t\nQnAMaker preview API is now enabling to build, train and publish a simple question and answer bot from product manuals.\nWe’re expanding Face API, Computer Vision API and Content Moderator in 7 additional regions - South Central US, West US2, East US, Brazil, North Europe, Australia East and East Asia.\n\n\n\nCreating a highly targeted search for your users\nAs mentioned, we’re excited to announce that Bing Custom Search will be generally available in October!\r\nWith Bing Custom Search, you can create a highly-customized targeted web search experience, to deliver more relevant results from targeted web space through a commercial grade service.\nFeaturing a straightforward User Interface, Bing Custom Search enables you to create your own web search engine without a line of code. You can specify the slices of the web you want to draw from – or let cutting-edge AI technology help to identify them. Businesses of any size, hobbyists and entrepreneurs can design and deploy web search applications for any possible scenario.\nFor example, Amicus has recently released a platform that changes the way charitable aid is funded and delivered, showing donors where every dollar is spent and giving non-profits real-time tools to report and measure performance. This allows donors to fund ‘projects’ instead of blindly giving money to an organization, and non-profits to build project requests based on measurable outcomes. This transition presented a very unique challenge: how to enable donors to research and learn about the projects and activities performed by non-profits? Amicus needed to help donors Learn, Find and Fund projects that were of interest and relevant to them, something complex with traditional search engines.\nWith Bing Custom Search, part of Microsoft Cognitive Services, Amicus has been able to identify its own set of relevant web pages in advance: when users have a single concept of interest (like ‘water’, ‘education’ or ‘India’), Bing Custom Search is able to deliver highly relevant results in the context of global aid.\n“This is exactly what our audience needs in order to learn about a broader range of important work performed by relief organizations, beyond those the donors currently know about. Bing Custom Search, part of Microsoft Cognitive Services, delivers a ‘Learn and Find’ experience in ways never before possible.” – says Beth Katz, Chief Product Officer at Amicus.\nGet Started with Bing Custom Search\nTo easily get started with Bing Custom Search, you can look at the service page, refer to the documentation and start building a great experience with the quick start guide.\nBringing text to life in your application\nWe are excited to announce that Text Analytics is now generally available in the Azure Portal, and now available in four additional regions: South Central US, East US, West Europe, and Southeast Asia.\nText Analytics API is a cloud-based service that provides advanced natural language processing over raw text. Text Analytics API has three main functions: sentiment analysis, key phrase extraction, and language detection.\n\nSentiment Analysis - Find out what customers think of your brand or topic by analyzing raw text for clues about positive or negative sentiment. This API returns a sentiment score between 0 and 1 for each document, where 1 is the most positive. Our models are pretrained using an extensive body of text and natural language technologies from Microsoft. \nKey Phrase Extraction - Automatically extract key phrases to quickly identify the main points. For example, for the input text ‘The food was delicious and there were wonderful staff’, the service returns the main talking points: ‘food’ and ‘wonderful staff’. \nLanguage Detection - For up to 120 languages, detect which language the input text is written in and report a single language code for every document submitted on the request.\n\nSome customers scenarios where Text Analytics is used are customer feedback analytics, as an enricher to search scenarios, and in conjunction with LUIS and the Bot Framework (analyzing sentiment of a conversation over time).\nMore and more customers are using Text Analytics API: Brainshark is a cloud-based sales training and readiness platform that helps sales people achieve mastery in the presentation of sales materials to clients, slashing the costs and resources needed for training and maximizing the effectiveness of sales engagements.\nBrainshark is now creating a training platform that allows sales representatives to perfect their pitch through video and Cognitive Services. Utilizing Face API, Emotion API, and Text Analytics, it’s possible to analyze their pitch, and feed a Machine Learning model to provide feedback on their performance.\n“Now, companies are simply pushing sales people into the field and they’re learning through experience— a ridiculously expensive way to train. Every deal lost due to lack of confidence costs the company real money. If we can minimize that and actually get sales people ready to sell, it’ll have a huge impact on productivity,” says Jim Ninivaggi, Senior Vice President, Business Development at Brainshark.\nGet Started with Text Analytics API\nOne of the best way to get started with Text Analytics API is to look at our Quick Start guides. Here is a snippet from the C# Quickstart that show how to consume the API using the C# SDK.  We also have quickstart in Java, Node.js, Python, Ruby and PHP.\nLet’s say that I want to be able to explore the most important phrases, sentiment and language from feedback I receive from my customers with Text Analytics API.\n\r\n// EXTRACTING LANGUAGE\r\nLanguageBatchResult result = client.DetectLanguage( \r\n    new BatchInput( \r\n        new List<Input>() \r\n            { \r\n                new Input(\"1\", \"This is a document written in English.\"), \r\n                new Input(\"2\", \"Este es un document escrito en Español.\"), \r\n                new Input(\"3\", \"这是一个用中文写的文件\") \r\n            })); \r\n// Printing language results. \r\nforeach (var document in result.Documents) \r\n{ \r\n    Console.WriteLine(\"Document ID: {0}, Language:{1}\",\r\n                       document.Id, document.DetectedLanguages[0].Name); \r\n} \r\n// GETTING KEY PHRASES \r\nKeyPhraseBatchResult result2 = client.KeyPhrases( \r\n    new MultiLanguageBatchInput( \r\n        new List<MultiLanguageInput>() \r\n            { \r\n                new MultiLanguageInput(\"ja\", \"1\", \"猫は幸せ\"), \r\n                new MultiLanguageInput(\"de\", \"2\", \r\n                              \"Fahrt nach Stuttgart und dann zum Hotel zu Fu.\"), \r\n                new MultiLanguageInput(\"en\", \"3\", \"My cat is stiff as a rock.\"), \r\n                new MultiLanguageInput(\"es\", \"4\", \"A mi me encanta el fútbol!\") \r\n            })); \r\n// Printing keyphrases \r\nforeach (var document in result2.Documents) \r\n{ \r\n    Console.WriteLine(\"Document ID: {0} \", document.Id); \r\n    Console.WriteLine(\"\\t Key phrases:\"); \r\n    foreach (string keyphrase in document.KeyPhrases) \r\n        { Console.WriteLine(\"\\t\\t\" + keyphrase);  } \r\n} \r\n// SENTIMENT ANALYSIS \r\nSentimentBatchResult result3 = client.Sentiment( \r\n    new MultiLanguageBatchInput( \r\n        new List<MultiLanguageInput>() \r\n            { \r\n                new MultiLanguageInput(\"en\", \"0\", \"I had the best day of my life.\"), \r\n                new MultiLanguageInput(\"en\", \"1\", \r\n                       \"This was a waste of my time. The speaker put me to sleep.\"), \r\n                new MultiLanguageInput(\"es\", \"2\", \r\n                        \"No tengo dinero ni nada que dar...\"), \r\n                new MultiLanguageInput(\"it\", \"3\", \r\n                         \"L'hotel veneziano era meraviglioso. \r\n                          È un bellissimo pezzo di architettura.\"), \r\n            })); \r\n\r\n// Printing sentiment results \r\nforeach (var document in result3.Documents) \r\n{ \r\n    Console.WriteLine(\"Document ID: {0} , Sentiment Score: {1:0.00}\", \r\n                      document.Id, document.Score); \r\n} \r\n\nDon’t hesitate to refer to the API definitions for technical documentation for the APIs.\nWe also developed a very interesting tutorial here that integrates Text Analytics into Power BI to extract the most important phrases and sentiment from customer feedback. You’ll see how we’re using a custom Power Query function and creating a nice Word Cloud from these phrases.\nHappy coding!\r\n \r\nThe Microsoft Cognitive Services Team\n", "link": "https://azure.microsoft.com/en-us/blog/at-ignite-microsoft-is-updating-its-cognitive-services-collection-of-intelligent-apis/", "Role": null},
{"Title": "Bing Search API v7 and Bing Custom Search are now generally available", "Date": "Posted on October 16, 2017", "Contributor": null, "Content": "\nThis post was authored by The Microsoft Cognitive Services Team.\nMicrosoft Cognitive Services enables developers to augment the next generation of applications with the ability to see, hear, speak, understand, and interpret needs using natural methods of communication.  \r\n \r\nToday, we are announcing the general availability of Bing Custom Search API and Bing Search API v7. Showcased at Microsoft Ignite 2017 just last month, these APIs are now both available on the Azure Portal. \n“We’re excited to bring to market this big Bing APIs release that includes new features, faster performance and new APIs. By leveraging the power of Bing AI connected with Azure, we can empower every developer to build richer experiences harnessing the power of Bing AI.”  -Gurpreet Singh Pall, VP, Bing and Cortana Product Ecosystem\r\n \r\nLet’s dive into the features and how to get to started.\r\n \nBing Custom Search lets you create a highly-customized and targeted web search experience to deliver more relevant results from your targeted web space, through a commercial grade service. \r\n \r\nWe listened to your feedback about the preview version we announced at Build 2017 and addressed the need for customized search solutions with Bing Custom Search. Businesses of any size, hobbyists and entrepreneurs can design and deploy web search applications for any possible scenario.\r\n \r\nFor example, Amicus has recently released a platform that changes the way charitable aid is funded and delivered, showing donors where every dollar is spent and giving non-profits real-time tools to report and measure performance. This allows donors to fund ‘projects’ instead of blindly giving money to an organization, and non-profits to build project requests based on measurable outcomes. This transition presented a very unique challenge: how to enable donors to research and learn about the projects and activities performed by non-profits? Amicus needed to help donors learn, find and fund projects that were of interest and relevant to them, something too complicated for traditional search engines.\nWith Bing Custom Search, part of Microsoft Cognitive Services, Amicus has been able to identify its own set of relevant web pages in advance: when users have a single concept of interest (like ‘water’, ‘education’ or ‘India’), Bing Custom Search is able to deliver highly relevant results in the context of global aid.\n“This is exactly what our audience needs in order to learn about a broader range of important work performed by relief organizations, beyond those the donors currently know about. Bing Custom Search, part of Microsoft Cognitive Services, delivers a ‘Learn and Find’ experience in ways never before possible.” – says Beth Katz, Chief Product Officer at Amicus.\r\n \r\nFor more information about Bing Custom Search general availability pricing and included quantities for the Trial Tier, please visit the Bing Custom Search Pricing page. \r\n \r\nFor more information about preview keys you might have, please refer to the related documentation. \r\n \nWe’re also excited to announce the general availability of Bing Search APIs v7, which allows you to bring the immense knowledge of the planet to your applications. Results come back fast with improved performance for queries on the Bing Web Search API. \nWe are pleased to announce several new updates to the program including new sorting and filtering options for finding specific results in trending topics or image searches, better error messages to ease troubleshooting and improve problem query diagnosis, and updated documentation to make it easier to bring the power of Bing Search APIs to your applications.\nThe v7 of Bing APIs include the following services:\n\nBing Web Search API (webpage, pricing page, and upgrade guide) offers enhanced search details from billions of web documents. \nBing Image Search API (webpage, pricing page, and upgrade guide) includes thumbnails, full image URLs, publishing website info, image metadata, and more. \nBing Video Search API (webpage, pricing page, and upgrade guide) includes useful metadata such as the creator, encoding format, video length, view count, and more. \nBing News Search API (webpage, pricing page, and upgrade guide) includes authoritative images of the news article, related news and categories, trending topics, provider information, article URLs, and dates images were added. \nBing Autosuggest API (webpage, pricing page, and upgrade guide) helps users complete queries faster by adding intelligent type-ahead capabilities to an app or website. \nBing Spell Check API (webpage, pricing page, and upgrade guide) helps correct spelling errors, recognizing the difference among names, brand names, and slang, and understanding homophones as they’re being typed. \n\nPlease check out the documentation, migration guide and pricing links listed above which will help you better understand how to handle the existing v5 keys and migrate from API v5 to API v7. \r\n \r\nFor questions or feedback please visit Stack Overflow and Azure Support.\n-The Microsoft Cognitive Services Team\n", "link": "https://azure.microsoft.com/en-us/blog/bing-search-api-v7-and-bing-custom-search-are-now-generally-available/", "Role": null},
{"Title": "Microsoft Cognitive Services updates for Microsoft Connect();", "Date": "Posted on November 15, 2017", "Contributor": null, "Content": "\nMicrosoft Cognitive Services enables developers to augment the next generation of applications with the ability to see, hear, speak, understand, and interpret needs using natural methods of communication. Think about the possibilities: being able to add vision and speech recognition, emotion and sentiment detection, language understanding, and search, to applications without having any data science expertise.\nToday, we are excited to announce several service updates:\n\nWe’re continuing our momentum on Azure Bot Service and Language Understanding Intelligent Service, which are going to be in general availability by the end of this calendar year.\nText Analytics API regions and languages - Since the beginning of November, Text analytics API is now available in new regions: Australia East, Brazil, South Central US, East US, North Europe, East Asia, West US 2. We are also releasing new languages for key phrases: Swedish, Italian, Finnish, Portuguese, Polish and Dutch.\nTranslator API - A year ago, Microsoft Translator launched its neural network powered languages called neural machine translation (NMT). Neural networks capture the context of full sentences with high quality translation and more human-sounding output. Today, the Microsoft Translator team is announcing several developments in the Neural Machine Translation technology, making advanced AI translations more accessible (don’t hesitate to refer to the detailed announcement blog post):\r\n \n10 new languages are now available for Neural Machine Translation, for a total of 21 supported languages, in both Translator Text API, Translator Speech API and in Microsoft Translator apps and services. The 10 new languages cover Bulgarian, Czech, Danish, Dutch, Hindi, Norwegian, Portuguese, Romanian, Swedish, Turkish.\nAll of the API traffic for Chinese and Hindi is now powered by Neural Machine Translation. This means that developers do not need to call the “generalnn” category in their apps to benefit from these new systems.\nNew “hybrid” translation is available for APIs and apps users alike, bringing benefits of neural translations to languages not yet available on NMT. When only one of the two languages you are translating to or from is Neural Machine Translation-powered, you will still see an increase in translation quality as Microsoft Translator will automatically use NMT for that section of the translation - this portion of the translation will improve, making the entire translation better.\nSpeech translation is now powered end to end with Long Short Term Memory (LSTM) technology. This has a direct impact on the quality of the machine translation, since the more accurate the speech recognition is, the more accurate the resulting translation will be.\nNeural Machine Translation is now also available as an on premises service - More information about Microsoft Translator’s on premises offering.\n\n\nCustom Vision model export - We are excited to announce the availability of mobile model export for Custom Vision Service. This new feature will allow you to embed your classifier directly in your application and run it locally on your device. The models you export will be optimized for the constraints of a mobile device, so you can easily classify on your device in real time. In addition to hosting your classifiers at a REST endpoint, you can now export models to run offline, starting with export to the CoreML format for iOS 11. We’ll be able to export to Android as well in a few weeks. With this new capability, adding real time image classification to your mobile applications has never been easier. More on how to create and export your own Custom Vision Model is below.\n\n \nHow to create and export your own Custom Vision Model\nLet’s dive into the new custom vision features and how to get to started.\nCustom Vision Service is a tool for easily creating your own custom image classifier by training, deploying, and improving it. With several images per category you’re looking for, you can get started to create and train your own image classifier in a few minutes.\n\nOn top of hosting the classifier you trained at a REST endpoint, it is now possible to export models to run offline, starting with export to the CoreML format for iOS 11.\nFirst, let’s build our classifier. You’ll need to have :\n\nA valid Microsoft Account or an Azure Active Directory OrgID (\"work or school account\"), so you can sign into customvision.ai and get started (Note that OrgID login for AAD users from national clouds is not currently supported)\nA set of images to train your classifier (you should have a minimum of 30 images per tag).\nA few images to test your classifier after the classifier is trained.\n\nStarting your classifier\n\nIn the previous post presenting Custom Vision Service, we had presented how to quickly create an image classifier by coding each step.\nFor this time, let’s try it with the Custom Vision Service UI. First, let’s access to the Custom Vision Service site at: https://customvision.ai.\nClick New Project to create your first project;\nThe New Project dialog box appears and lets you enter a name, description and select a domain.\n\n\nIn order to be able to export your classifier, you’ll need to select a compact domain. Please refer to the full tutorial for detailed domains explained.\nAdding images to train your classifier\n\nLet's say you want a classifier to distinguish between dogs and ponies. Even if the system minimum would be with five images per category, you would need to upload and tag at least 30 images of dogs and 30 images of ponies.\nTry to upload a variety of images with different camera angles, lighting, background, types, styles, groups, sizes, etc. We recommend variety in your photos to ensure your classifier is not biased in any way and can generalize well.\nClick Add images\n\n\n\nBrowse to the location of your training images.\n\nNote: You can use the REST API to load training images from URLs. The web app can only upload training images from your local computer.\n\n\nSelect the images for your first tag by clicking Open to open the selected images.\nOnce selected, you’ll assign tags by typing in the tag you want to assign, then pressing the + button to assign the tag. You can add more than one tag at a time to the images.\n\n\n\nWhen you are done adding tags, click Upload [number] files. The upload could take some time if you have a large number of images or a slow Internet connection.\nAfter the files have uploaded, click Done.\n\n\nTrain your classifier\nAfter your images are uploaded, you are ready to train your classifier. All you have to do is click the Train button.\n\nIt should only take a few minutes to train your classifier.\n\nThe precision and recall indicators estimate you how good your classifier is, based on automatic testing. Note that Custom Vision Service uses the images you submitted for training to calculate these numbers, using a process called k-fold cross validation.\n\nNote: Each time you hit the Train button, you create a new iteration of your classifier. You can view all your old iterations in the Performance tab, and you can delete any that may be obsolete. When you delete an iteration, you end up deleting any images uniquely associated with it.\nThe classifier uses all the images to create a model that identifies each tag. To test the quality of the model, the classifier then tries each image on its model to see what the model finds.\nExporting your classifier\n\nExport allows you to embed your classifier directly in your application and run it locally on a device. The models you export are optimized for the constraints of a mobile device, so you can classify on device in real time.\nIn the case you previously had an existing classifier, you need to convert it to a compact domain, please refer to this tutorial.\nOnce your project has finished training in the previous section, you can export your model:\nGo to the Performance tab and select the iteration you want to export (probably your most recent iteration.)\nIf this iteration used a compact domain, an export button appears at the top bar.\nClick on export, then select your format (currently, iOS/CoreML is available.)\nClick on export and then download to download your model.\n\n\nUsing the exported model in an iOS application\nLast step, please refer to the Swift source of sample iOS application for models exported from Custom Vision Service and the demo CoreML model with Xamarin.\nHappy coding!\n- The Microsoft Cognitive Services Team\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-cognitive-services-updates-for-microsoft-connect/", "Role": null},
{"Title": "Announcing the General Availability of Azure Bot Service and Language Understanding, enabling developers to build better conversational bots", "Date": "Posted on December 13, 2017", "Contributor": "Lili Cheng", "Content": "\nConversational AI, or making human and computer interactions more natural, has been a goal since technology became ubiquitous in our society. Our mission is to bring conversational AI tools and capabilities to every developer and every organization on the planet, and help businesses augment human ingenuity in unique and differentiated ways.\nToday, I’m excited to announce Microsoft Azure Bot Service and Microsoft Cognitive Services Language Understanding (LUIS) are both generally available.\nAzure Bot Service enables developers to create conversational interfaces on multiple channels while Language Understanding (LUIS) helps developers create customized natural interactions on any platform for any type of application, including bots. Making these two services generally available on Azure simultaneously extends the capabilities of developers to build custom models that can naturally interpret the intentions of people conversing with bots.\nThis announcement delivers on our AI Platform approach, providing developers and data scientists with all the tools they need to create AI applications in the cloud and on mobile devices. In November, at Connect(); 2017, we released tools to infuse AI into new and existing applications quickly and easily with updates to Azure Machine Learning (AML) including Azure IoT Edge integration, as well as new Visual Studio Tools for AI. In September, at Microsoft Ignite 2017, we announced tools for the AI-driven Digital Transformation and described how the Microsoft AI platform enables a rich variety of application scenarios.\nNew capabilities of Azure Bot Service and Language Understanding\nWith the general availability of Azure Bot Service and Language Understanding, we're also introducing new capabilities to help developers achieve more. Azure Bot Service is now available in more regions and offers premium channels to communicate better with users and provide advanced customization capabilities.\n\nAzure Bot Service allows you to select various templates from simple form, questions and answers, in either C# or node.js \nLanguage Understanding (LUIS) now has an updated user interface and is available in more regions. It is also expanded up to 500 intents and 100 entities, so developers can create more conversational experiences for their apps. For example, a travel app with LUIS would extract from the sentence “Book me a ticket to Paris” an intent named BookFlight and entity Location as “Paris” to process the order.\n\nLanguage Understanding new portal, listing for one intent, the potential sentences created\nLanguage Understanding is part of Microsoft Cognitive Services, a collection of intelligent APIs that enables systems to see, hear, speak, understand and interpret our needs using natural methods of communication. We’ve been making several of these Cognitive Services more customizable, allowing developers to use their own data with algorithms for specific needs. For example, with Custom Speech Service, the research division of an enterprise could create a bot able to understand the specific vernacular of chemical compounds. Or, a fast food restaurant could create an application for taking orders in a noisy drive-through environment.\nFeel free to deep dive into the detailed information about the new features of Language Understanding and Azure Bot Service here.\nCustomer adoption\nToday, more than 760,000 developers from 60 countries are using Cognitive Services to add intelligent capabilities to their applications. Additionally, over 240,000 developers have signed up to use the Azure Bot Service which provides developers with everything they need to build and connect intelligent bots. And thousands of customers are already developing intelligent applications with Azure Bot Service and/or LUIS, such as Dixons Carphone, Equadex, Human Interact, Molson Coors, Sabre, UPS, and many more.\nEquadex is one customer using Language Understanding for smart applications. Some children with Autism Spectrum Disorder can experience barriers that can make it difficult to communicate and verbalize their thoughts in order to successfully navigate their world. Equadex worked to provide a tool to alleviate communication difficulty with an easy-to-use mobile app that provides a visual representation of language. With the Microsoft Cognitive Services REST APIs and Microsoft Azure tools, Equadex was easily able to incorporate powerful machine learning and artificial intelligence into its Helpicto app. Equadex hopes that Helpicto will eventually help all people with language difficulties communicate more easily.\n“We wanted to deliver to the market an innovative technology that could translate natural language into a universal form that someone who is nonverbal could use and understand,” explains Anthony Allebée, Chief Technology Officer at Equadex. “With features like LUIS and the Computer Vision API, Cognitive Services helped us quickly turn our dream of an enhanced communication tool into a reality,” says Allebée.\nWith a story that starts in 1774, Molson Coors has spent centuries defining brewing greatness. As one of the largest global brewers, Molson Coors works to deliver extraordinary brands that delight the world’s beer drinkers. In order to help its employees better access knowledge in the organization and collaborate across time zones and geographies, Molson Coors is exploring the use of knowledge bots for specific IT and Procurement topics, leveraging Microsoft Cognitive Services QnA Maker, Azure Bot Service, Microsoft Teams, and the Calendar.Help service powered by Cortana.\nTo improve customer service with intelligent applications as well as increase the efficiency of IT staff, UPS recently improved service levels via a chatbot, the UPS Bot. This sophisticated agent runs on the Microsoft bot technology on Azure. Customers can engage with the UPS Bot in text-based and voice-based conversations to get the information they need about shipments, rates, and UPS locations.\n“Within five weeks, we had developed a chatbot prototype with the Microsoft bot technology. Our Chief Information and Engineering Officer loved it and asked that we get a version into production in just two months…and that’s just what we did,” said Kumar Athreya, Senior Applications Development Manager of Shipping Systems, UPS.\nMicrosoft AI platform\nWe are making AI approachable and productive for all developers and data scientists with our flexible AI platform, combining the latest advances in technologies like machine learning and deep learning, with our comprehensive data, Azure cloud and productivity platform.\nPowered by Azure, our AI platform integrates:\n\nHigh-level services to accelerate the development of AI solutions. This includes conversational AI with Azure Bot Service, trained models such as Cognitive Services (pre-built APIs and custom AI services), allowing developers to use their own data with algorithms trained for their specific needs, and full custom AI services such as Azure Machine Learning.\nAn underlying AI infrastructure with virtually infinite scale\nTools to increase productivity for developers and data scientists, bringing AI to every developer and every scenario.\n\n\nThe Microsoft AI platform, provides a comprehensive cloud powered AI for every developer.\nI invite you to visit www.azure.com/ai to learn more about how AI can augment and empower digital transformation efforts. We’ve also launched the AI School to help developers get up to speed with all of these AI technologies.\nDive in and learn how to infuse conversational AI into your applications today.\nLili Cheng\r\n@lilich\n", "link": "https://azure.microsoft.com/en-us/blog/announcing-the-general-availability-of-azure-bot-service-and-language-understanding-enabling-developers-to-build-better-conversational-bots/", "Role": "Corporate Vice President, Conversational AI"},
{"Title": "Conversational Bots Deep Dive – What’s new with the General Availability of Azure Bot Service and Language Understanding", "Date": "Posted on December 13, 2017", "Contributor": null, "Content": "\nThis post was authored by the Azure Bot Service and Language Understanding Team.\nMicrosoft brings the latest advanced chatbot capabilities to developers' fingertips, allowing them to create apps that see, hear, speak, understand, and interpret users’ needs -- using natural communication styles and methods.\nToday, we’re excited to announce we’re making generally available Microsoft Cognitive Services Language Understanding service (LUIS) and Azure Bot Service, two top notch AI services to create digital agents that interact in natural ways and make sense of the surrounding environment.\nThink about the possibilities: all developers regardless of expertise in data science able to build conversational AI that can enrich and expand the reach of applications to audiences across a myriad of conversational channels. The app will be able to understand natural language, reason about content and take intelligent actions. Bringing intelligent agents to developers and organizations that do not have expertise in data science is disruptive to the way humans interact with computers in their daily life and the way enterprises run their businesses with their customers and employees.\nThrough our preview journey in the past two years, we have learned a lot from interacting with thousands of customers undergoing digital transformation. We highlighted some of our customer stories (such as UPS, Equadex, and more) in our general availability announcement. This post covers conversational AI in a nutshell using Azure Bot Service and LUIS, what we’ve learned so far, and dive into the new capabilities. We will also show how easy it is to get started in building a conversational bot with natural language.\nConversational AI with Azure Bot Service and LUIS\nAzure Bot Service provides a scalable, integrated bot development and hosting environment for conversational bots that can reach customers across multiple channels on any device. Bots provide the conversational interface that accepts user input in different modalities including text, speech, cards, or images. The Azure Bot Service offers a set of fourteen channels to interact with users including Cortana, Facebook Messenger, Skype, etc. Intelligence is enabled in the Azure Bot Service through the cloud AI services forming the bot brain that understands and reasons about the user input. Based on understanding the input, the bot can help the user complete some tasks, answer questions, or even chit chat through action handlers. The following diagram summarizes how conversational AI applications are enabled through the Azure Bot Service and the Cloud AI services including language understanding, speech recognition, Q&A Maker, etc.\n\nLanguage Understanding (LUIS) is the key part of the bot brain that allows the bot to understand natural language input and reason about it to take the appropriate action. As customization is critical for every business scenario, Language Understanding helps build custom models for your business vertical with little effort and without prior expertise in data science. Designed to identify valuable information in conversations, it interprets user goals (intents) and distills valuable information from sentences (entities), for a high quality, nuanced language model.\nWith the General Availability of Language Understanding and Azure Bot Service, we're also introducing new capabilities to help you achieve more and delight your users\nLanguage Understanding:\n\nWith an updated user interface, we’re providing Language Understanding service (LUIS) users more intents and entities than ever: expanding up to 500 intents (task or action identified in the sentence) and 100 entities (relevant information extracted, from the sentence, to complete the task or action associated to the intent) per application.\nLanguage Understanding is now available in 7 new regions (South Central US, East US, West US 2, East Asia, North Europe, Brazil South, Australia East) on top of the 5 existing regions (West Europe, West US, East US2, West central US, South east Asia). This will help customers to improve network latency and bandwidth.\nThe Language Understanding service is also supporting more languages for its various features, in addition to English.\r\n \nThe prebuilt entities (representing common concepts like numbers, date, time) previously available in English are now available in French and Spanish.\nPrebuilt domains (off-the-shelf collections of intents and entities grouped by domain that you can directly add and use in your application) are now also available in Chinese.\nPhrase suggestions that help the developer customize your LUIS domain vocabulary are available in 7 new languages Chinese, Spanish, Japanese, French, Portuguese, German, and Italian.\n\n\n\nAzure Bot Service:\n\nSpeed bot development by providing an integrated environment with the Microsoft Bot Framework channels, development tools and hosting solutions.\nConnect with your audience with no code modifications via our supported channels on the Bot Service; Office 365 Email, GroupMe, Facebook Messenger, Kik, Skype, Slack, Microsoft Teams, Telegram, text/SMS, Twilio, Cortana, Skype for Business – or provide a custom experience in your app or website.\nBot Service is now integrated into the Azure portal; easy access to 24x7 support, monitoring capabilities, integrated billing and more in the trusted Azure ecosystem.\nNow generally available in 9 different regions namely West US, East US, West Europe, and Southeast Asia including new deployments in North Europe, Australia Southeast, Australia East, Brazil South, and East Asia regions.\nWe are also announcing Premium Channels including webchat and directline.  Premium channels offer unique capabilities over the standard channels:\r\n \nCommunicate with your users on your website or in your application instead of sharing that data with public chat services.\nOpen Source webchat and directline clients enabling advanced customization opportunities.\n99.9% availability guarantees for premium channels\n\n\n\nDevelopers can connect to other Azure services to enrich their bots as well as add Cognitive Services to enable your bots to see, hear, interpret, and interact in more human ways. For example, on top of language, the Computer Vision and Face APIs can enable bots to understand images and faces passed to the bot.\nLearning through our customer’s experiences\nFor several years now, Microsoft has been leading the charge into the application of AI to build new intelligent conversational experiences…everything from proprietary solutions built to target a specific audience on a specific chat service to general purpose API’s that expect the developer to create the rest of the custom solution themselves.  We are still at the beginning of this evolution of the conversational application model; but already we have takeaways that are guiding how we think about the future.\nBots are changing how we do business. We are constantly having great discussions with customers who see bots as a key part of their digital transformation as a business. They see the opportunity to enhance their customer support experiences, provide easy access to information, or even expose their business to an audience that might not otherwise visit their website.\nDevelopers need to have choice in technologies. With the growth in popularity of open source technologies, developers want choice of the technology components they use to build solutions.\nGreat conversational applications are multi-modal. Our customers are building conversational experiences which accomplish multiple tasks. For example, a customer support bot may have a Q&A search function, a support ticket entry function, a guided dialog to diagnose a problem, and an appointment scheduling function that hands off to a human for final confirmation.\nAI platforms must scale to the needs of business. Often as not, business scenarios are based on sets of concepts that are being codified into the bot. Developers require the technologies they depend on to scale to the complexity of their business without arbitrary limits getting in the way.\nConversational app platforms need to be reliable and compliant. In the same way that mobile app platforms have needed to provide robust and secure platforms to enable great productivity scenarios, so too will conversational application platforms; they must be certifiably secure, reliable, compliant and privacy aware. In addition, the platform should to make it easy for developers building to it to build compliant solutions as well.\nBusinesses are global and multi-lingual. Businesses need to talk to customers world-wide 24/7 in their language of choice.\nThere is art in building a great conversational application. Much in the same way the 80’s and 90’s cemented what we now think of as common controls for native apps, and the 2000’s for web and mobile, the industry is still defining what it means to be a great conversational application.\nKey design considerations\nGiven the learnings we’ve had, we’ve anchored our design on the following six points to shape the Azure Bot Service and Language Understanding (LUIS) capabilities:\nCode-first approach: Azure Bot Service is built on top of the BotBuilder SDK V3 (in Node.js) that takes a code-first approach to enable developers to have full control over their bots’ conversational capabilities. Available for both Node.JS and C#, the open source SDK’s provides multiple dialog types and conversational orchestration tools to help the developer with various tasks like slot filling, dialog management and card representation.\nDifferent dialog management flavors: developers build bots that range from simple question answer bots to multi-turn solutions that span ten or fifteen turns to complete a task. We provide a rich set of dialog management flavors to cover the different task types a bot developer might wish to expose. You can create bots that utilize a mix of prompts, form filling, natural language, and your own dialog management system with the ability to reuse some of the components like prompts.\nOpen bot platform: Building on Azure's commitment to open source technologies, applications using our SDK and LUIS can be deployed on any connected infrastructure and consumed from any device anywhere targeting your audience on multiple chat channels. This open design allows the offering to be integrated with different deployment platforms including public cloud or on-premise infrastructure.\nGlobal and multi-lingual: We have put considerable effort into making our services highly available and as close to customers as possible as part of the Azure cloud.  Azure Bot Service and Language Understanding support a growing list of languages for understanding conversations.\nGetting started quickly: While bots can be deployed anywhere, with Azure we provide rich connected cloud services for hosting your bot and AI applications with a single click.  The Azure Bot Service and LUIS get you a running bot that can converse with users in a natural way in minutes. Azure Bot Service takes care of provisioning all of the Azure resources you need so that developers can focus on their business logic. LUIS provides customizable pre-built apps and entity dictionaries, such as Calendar, Music, and Devices, so you can build and deploy a solution more quickly. Dictionaries are mined from the collective knowledge of the web and supply billions of entries, helping your model to correctly identify valuable information from user conversations.\nCustom models with little effort: as customization is critical for every business scenario, LUIS capitalizes on the philosophy of machine teaching to help non-expert machine learning developers build effective custom language understanding models. While machine learning focuses on creating new algorithms and improving the accuracy of “learners”, the machine teaching discipline focuses on the eﬃcacy of the “teachers”. Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. It provides the developer with a set of tools to build machine learning models by transferring the developer domain knowledge to the machine learning algorithms. This contrasts with Machine Learning which is about creating useful models from this knowledge. Developer knowledge is expressed in LUIS through schema (what intents and entities are in the LUIS application) and labeled examples.  It supports a wide variety of techniques for reliably recognizing entities with normalization to allow them to be easily consumed in a program.\nAlways monitor, learn and improve: Azure Bot Service and LUIS use Azure monitoring tools to help developers monitor the performance of their bots including the quality of the language understanding models and the bot usage. Once the model starts processing input, LUIS begins active learning, allowing you to constantly update and improve the model. It helps you pick the most informative utterances from your real bot traffic to add to your model and continuously improve. This intelligent selection of examples to add to the training data of the LUIS model helps developers build cost effective models that don’t require a lot of data and yet perform with high accuracy.\nGetting started with the Bot Service and Language Understanding\nIn this section, we’ll create a bot using the Azure Bot Service that uses Language Understanding (LUIS) to understand the user. When creating a bot using natural language, the bot determines what a user wants to do by identifying their intent. This intent is determined from spoken or textual input, or utterances, which in turn can be mapped to actions that Bot developers has coded. For example, a note-taking bot recognizes a Notes. Create intent to invoke the functionality for creating a note. A bot may also need to extract entities, which are important words in utterances. In the example of a note-taking bot, the Notes. Title entity identifies the title of each note.\nCreate a Language Understanding bot with Bot Service\nTo create your bot; log in the Azure portal, select Create new resource in the menu blade and select AI + Cognitive Services.\n\nYou can browse through the suggestions, or search for Web App Bot.\n\nOnce selected, the Bot Service blade should appear; which will be familiar to users of Azure services. For those that aren’t, here you can specify information about your service for the Bot Service to use in creating your bot such as where it will live, what subscription in and so forth. In the Bot Service blade, provide the required information, and click Create. This creates and deploys the bot service and LUIS app to Azure. Some interesting fields:\n\nSet App name to your bot’s name. The name is used as the subdomain when your bot is deployed to the cloud (for example, mynotesbot.azurewebsites.net). This name is also used as the name of the LUIS app associated with your bot. Copy it to use later, to find the LUIS app associated with the bot.\nSelect the subscription, resource group, hosting plan, and location.\nFor pricing, you can choose the free pricing tier. You can go back and change that at any time if you need more.\nFor this sample, select the Language understanding (C#) template for the Bot template field.\nFor the final required field, choose the Azure Storage where you wish to store your bot’s conversation state. Think of this as where the bot keeps track of where each user is in the conversation.\n\n\nNow that you’re complete, you can click Create. Azure will set about creating your bot including the resources it needs to operate your bot and a LUIS account to host your natural language model. Once complete, you’ll receive a notification via the bell in the top right corner of the Azure portal.\r\nNext up, lets confirm that the bot service has been deployed.\n\nClick Notifications (the bell icon that is located along the top edge of the Azure portal). The notification will change from Deployment started to Deployment succeeded.\nAfter the notification changes to Deployment succeeded, click Go to resource on that notification.\n\nTry the bot\nSo now you should have a working bot. Let’s try it out.\nOnce the bot is registered, click Test in Web Chat to open the Web Chat pane. Type \"hello\" in Web Chat.\n\nThe bot responds by saying \"You have reached Greeting. You said: hello\". This confirms that the bot has received your message and passed it to a default LUIS app that it created. This default LUIS app detected a Greeting intent.\nNote: Occasionally, the first message or two after startup may need to be retried before the bot will answer.\nViola! You have a working bot! The default bot only knows a few things; it recognizes some greetings, as well as help and cancel. In the next section we’ll modify the LUIS app for our bot to add some new intents for our Note taking bot.\nModify the LUIS app\nLog in to www.luis.ai using the same account you use to log in to Azure. Click on My apps. If all has gone well, in the list of apps, you’ll find the app with the same name as the App name from the Bot Service blade when you created the Bot Service.\nAfter opening the app, you should see it has four intents: Cancel, Greeting, Help, and None. The first three we already mentioned. None is a special intent in LUIS that captures “everything else”.\nFor our sample, we’re going to add three intents for the user: Note.Create and Note.ReadAloud. Conveniently, one of the great features about LUIS are the pre-built domains that can be used to bootstrap your application, of which Note is one.\n\nClick on Pre-built Domains in the lower left of the page. Find the Note domain and click Add domain.\nThis tutorial doesn't use all the intents included in the Note prebuilt domain. In the Intents page, click on each of the following intent names and then click the Delete Intent button to remove them from your app.\r\n \nNote.ShowNext\nNote.DeleteNoteItem\nNote.Confirm\nNote.Clear\nNote.CheckOffItem\nNote.AddToNote\nNote.Delete\n\n\nIMPORTANT: The only intents that should remain in the LUIS app are the Note.ReadAloud, Note.Create, None, Help, Greeting, and Cancel intents.  If they’re still there, your app will still work, but may more often behave inconsistently.\n\nAs mentioned earlier, the Intents that we’ve now added represent the types of things we expect the user to want the bot to do.  Since these are pre-defined, we don’t have to do any further tuning to the model, so let’s jump right to training and publishing your model.\n\nClick the Train button in the upper right to train your app.  Training takes everything you’ve entered into the model by creating intents and entities, entering utterances and labeling them and generates a machine learned model, all with one click.  You can test your app here in the LUIS portal, or move on to publishing so that it’s available to your bot.\nClick PUBLISH in the top navigation bar to open the Publish page. Click the Publish to production slot button. After successful publish, copy the URL displayed in the Endpoint column the Publish App page, in the row that starts with the Resource Name Starter_Key. Save this URL to use later in your bot’s code. The URL has a format similar to this example: https://westus.api.cognitive.microsoft.com/luis/v2.0/apps/xxxxxxxxxxxxxxxxx?subscription-key=xxxxxxxxxxxxxx3&timezoneOffset=0&verbose=true&q=\n\nYour Language Understanding Application is now ready for your Bot. If the user asks to create, delete, or read back a note, Language Understanding will identify that and return the correct intent to the Bot to be acted on. In the next section we’ll add logic to the bot to handle these Intents.\nModify the bot code\nThe Bot Service is set up to work in a traditional development environment; sync your source code with GIT and work in your favorite dev environment. That said, Azure Bot Service also offers the ability to edit right in the portal; which is great for our experiment. Click Build and then click Open online code editor.\n\nFirst, some preamble. In the code editor, open BasicLuisDialog.cs. It contains the code for handling Cancel, Greeting, Help, and None intents from the LUIS app.\nAdd the following statement:\nusing System.Collections.Generic;\nCreate a class for storing notes\nAdd the following after the BasicLuisDialog constructor:\n\r\nprivate readonly Dictionary<string, Note> noteByTitle = new Dictionary<string, Note>();\r\n\r\nprivate Note noteToCreate;\r\n\r\nprivate string currentTitle;\r\n\r\n// CONSTANTS\r\n\r\n// Name of note title entity\r\n\r\npublic const string Entity_Note_Title = \"Note.Title\";\r\n\r\n// Default note title\r\n\r\npublic const string DefaultNoteTitle = \"default\";\r\n\r\n[Serializable]\r\n\r\npublic sealed class Note : IEquatable<Note>\r\n\r\n{\r\n\r\npublic string Title { get; set; }\r\n\r\npublic string Text { get; set; }\r\n\r\npublic override string ToString()\r\n\r\n{\r\n\r\nreturn $\"[{this.Title} : {this.Text}]\";\r\n\r\n}\r\n\r\npublic bool Equals(Note other)\r\n\r\n{\r\n\r\nreturn other != null\r\n\r\n&& this.Text == other.Text\r\n\r\n&& this.Title == other.Title;\r\n\r\n}\r\n\r\npublic override bool Equals(object other)\r\n\r\n{\r\n\r\nreturn Equals(other as Note);\r\n\r\n}\r\n\r\npublic override int GetHashCode()\r\n\r\n{\r\n\r\nreturn this.Title.GetHashCode();\r\n\r\n}\r\n\r\n}\nHandle the Note.Create intent\nNote.Create intent, add the following code to the BasicLuisDialog class.\n\r\n[LuisIntent(\"Note.Create\")]\r\n\r\npublic Task NoteCreateIntent(IDialogContext context, LuisResult result)\r\n\r\n{\r\n\r\nEntityRecommendation title;\r\n\r\nif (!result.TryFindEntity(Entity_Note_Title, out title))\r\n\r\n{\r\n\r\n// Prompt the user for a note title\r\n\r\nPromptDialog.Text(context, After_TitlePrompt, \"What is the title of the note you want to create?\");\r\n\r\n}\r\n\r\nelse\r\n\r\n{\r\n\r\nvar note = new Note() { Title = title.Entity };\r\n\r\nnoteToCreate = this.noteByTitle[note.Title] = note;\r\n\r\n// Prompt the user for what they want to say in the note\r\n\r\nPromptDialog.Text(context, After_TextPrompt, \"What do you want to say in your note?\");\r\n\r\n}\r\n\r\nreturn Task.CompletedTask;\r\n\r\n}\r\n\r\nprivate async Task After_TitlePrompt(IDialogContext context, IAwaitable<string> result)\r\n\r\n{\r\n\r\nEntityRecommendation title;\r\n\r\n// Set the title (used for creation, deletion, and reading)\r\n\r\ncurrentTitle = await result;\r\n\r\nif (currentTitle != null)\r\n\r\n{\r\n\r\ntitle = new EntityRecommendation(type: Entity_Note_Title) { Entity = currentTitle };\r\n\r\n}\r\n\r\nelse\r\n\r\n{\r\n\r\n// Use the default note title\r\n\r\ntitle = new EntityRecommendation(type: Entity_Note_Title) { Entity = DefaultNoteTitle };\r\n\r\n}\r\n\r\n// Create a new note object\r\n\r\nvar note = new Note() { Title = title.Entity };\r\n\r\n// Add the new note to the list of notes and also save it in order to add text to it later\r\n\r\nnoteToCreate = this.noteByTitle[note.Title] = note;\r\n\r\n// Prompt the user for what they want to say in the note\r\n\r\nPromptDialog.Text(context, After_TextPrompt, \"What do you want to say in your note?\");\r\n\r\n}\r\n\r\nprivate async Task After_TextPrompt(IDialogContext context, IAwaitable<string> result)\r\n\r\n{\r\n\r\n// Set the text of the note\r\n\r\nnoteToCreate.Text = await result;\r\n\r\nawait context.PostAsync($\"Created note **{this.noteToCreate.Title}** that says \\\"{this.noteToCreate.Text}\\\".\");\r\n\r\ncontext.Wait(MessageReceived);\r\n\r\n}\r\n\nHandle the Note.ReadAloud Intent\nThe bot can use the Note.ReadAloud intent to show the contents of a note, or of all the notes if the note title isn't detected. Paste the following code into the BasicLuisDialog class.\n\r\n[LuisIntent(\"Note.ReadAloud\")]\r\n\r\npublic async Task NoteReadAloudIntent(IDialogContext context, LuisResult result)\r\n\r\n{\r\n\r\nNote note;\r\n\r\nif (TryFindNote(result, out note))\r\n\r\n{\r\n\r\nawait context.PostAsync($\"**{note.Title}**: {note.Text}.\");\r\n\r\n}\r\n\r\nelse\r\n\r\n{\r\n\r\n// Print out all the notes if no specific note name was detected\r\n\r\nstring NoteList = \"Here's the list of all notes: \\n\\n\";\r\n\r\nforeach (KeyValuePair<string, Note> entry in noteByTitle)\r\n\r\n{\r\n\r\nNote noteInList = entry.Value;\r\n\r\nNoteList += $\"**{noteInList.Title}**: {noteInList.Text}.\\n\\n\";\r\n\r\n}\r\n\r\nawait context.PostAsync(NoteList);\r\n\r\n}\r\n\r\ncontext.Wait(MessageReceived);\r\n\r\n}\r\n\r\npublic bool TryFindNote(string noteTitle, out Note note)\r\n\r\n{\r\n\r\n// TryGetValue returns false if no match is found.\r\n\r\nbool foundNote = this.noteByTitle.TryGetValue(noteTitle, out note);\r\n\r\nreturn foundNote;\r\n\r\n}\r\n\r\npublic bool TryFindNote(LuisResult result, out Note note)\r\n\r\n{\r\n\r\nnote = null;\r\n\r\nstring titleToFind;\r\n\r\nEntityRecommendation title;\r\n\r\nif (result.TryFindEntity(Entity_Note_Title, out title))\r\n\r\n{\r\n\r\ntitleToFind = title.Entity;\r\n\r\n}\r\n\r\nelse\r\n\r\n{\r\n\r\ntitleToFind = DefaultNoteTitle;\r\n\r\n}\r\n\r\n// TryGetValue returns false if no match is found.\r\n\r\nreturn this.noteByTitle.TryGetValue(titleToFind, out note);\r\n\r\n}\nBuild the bot\nNow that the cut and paste part is done, you can right-click on build.cmd in the code editor and choose Run from Console. Your bot will be built and deployed from within the online code editor environment.\nTest the bot\nIn the Azure Portal, click on Test in Web Chat to test the bot. Try type messages like \"Create a note\", \"read my notes\", and \"delete notes\".  Because you’re using natural language you have more flexibility on how you state your request, and in turn, Language Understanding’s Active Learning feature can be used such that you can open your Language Understanding application and it can make suggestions about things you said which it didn’t understand and might make your app more effective.\n\nTip: If you find that your bot doesn't always recognize the correct intent or entities, improve your Language Understanding app's performance by giving it more example utterances to train it. You can retrain your Language Understanding app without any modification to your bot's code.\nThat's It (For Now)\nFrom here you’re just getting started.  You can go back to the bot service and connect your bot to various conversation channels.  You can remove the pre-built intents and start creating your own custom intents for your application.\nThere’s a world to discover in creating conversational applications and it’s easy to get started.  We look forward to seeing what you create and your feedback. For more information, please visit the following sites:\n\nAzure Bot Service: website\nLanguage Understanding service (LUIS): website and portal\nReference architectures, please refer to https://aka.ms/scenarios-abs\n\nHappy coding!\nThe Azure Bot Service and Language Understanding Team\n", "link": "https://azure.microsoft.com/en-us/blog/conversational-bots-deep-dive-what-s-new-with-the-general-availability-of-azure-bot-service-and-language-understanding/", "Role": null},
{"Title": "Cognitive Services September API Updates", "Date": "Posted on September 15, 2016", "Contributor": "Julia Nikitina", "Content": "\nMicrosoft Cognitive Services is a collection of intelligence and knowledge APIs that enable developers to make their applications more intelligent, engaging and discoverable. Cognitive Services includes intelligent APIs that allow systems to see, hear, speak, understand and interpret needs using natural methods of communication; and knowledge APIs that bring the power of the web to developers. Below are the September updates for Microsoft Cognitive Services APIs.\n \n   Computer Vision API\nOn September 14th, the Computer Vision API will be available in China. Additions include the ability to transact in Chinese currency.\nMicrosoft’s Computer Vision API is able to extract rich information from images to categorize and process visual data and protect your users from unwanted content. No changes have been made to the Computer Vision API but it is now available on the Mooncake Sovereign Cloud in China and includes the ability to transact in Chinese currency.\nLearn more about the Computer Vision API at Cognitive Services Computer Vision.\n \n   Face API\nOn September 14, the Face API will be available in China. Additions include the ability to transact in Chinese currency.\nMicrosoft’s FACE API can detect human faces and compare similar ones, organize people into groups according to visual similarity, and identify previously tagged people in images.  No changes have been made to the Face API but it is now available on the Mooncake Sovereign Cloud in China and includes the ability to transact in Chinese currency.\nLearn more about the Face API at Cognitive Services Face.\n \n   Speaker Recognition API\nSpeaker Recognition only needs 30 seconds of speech to identify speakers. \nSpeaker Recognition previously required 60 seconds of speech for both enrollment and identifciation operations. Speaker Recognition API now accepts any length of speech, although it is recommended that users input 30 seconds for enrollment and 10 seconds for identification to get the best API experience.\nLearn more about the Speaker Recognition API at Cognitive Services Speaker Recognition.\n \n   Language Understanding Intelligence Service (LUIS) API\nLUIS now supports German, Portuguese, and Japanese. Additionally, we created a C# SDK for LUIS endpoints. Entities have been updated for greater flexibility, hierarchies have been added, and we crated a new entity type: the composite entity. Reminder: the number of free LUIS transactions is changing from 100,000 to 10,000 a month for the trial offer.\n LUIS understands language contextually, so your app communicates with people in the way they speak. New LUIS features make the service more powerful for you and your users.\n\nNew languages – German, Portuguese and Japanese now supported in addition to the already supported English, French, Italian, Spanish, and Chinese.\nEntities have been updated to provide more flexibility.\nComposite Entity has been added.\nHierarchical entities have been added to allow editing. \n\nLearn more about the Language Understanding Intelligence Service API at Cognitive Services LUIS.\n  \n   Emotion API\nOn September 14th, the Emotion API will be available in China. Additions include the ability to transact in Chinese currency.\nMicrosoft’s Emotion API analyzes faces to detect a range of feelings and personalize your app's responses. No changes have been made to the Emotion API but it is now available on the Mooncake Sovereign Cloud in China and includes the ability to transact in Chinese currency.\nLearn more about the Emotion API at Cognitive Services Emotion.\n  \n   Bing Speech API\nThe Bing Speech API’s authentication endpoint and SDK namespaces are changing as of September 21. If you download the new SDK and used the Azure portal to obtain keys, you will need a new subscription key. Old keys will still work for six months before deprecation. All users will need to change the reference to the SDK namespace.\nThe Bing Speech API converts audio to text, understands intent, and converts text back to speech for natural responsiveness. \nLearn more about the Bing Speech API at Cognitive Services Bing Speech.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-09-15/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Microsoft Educator Community harnesses Recommendations API to engage educators around the world", "Date": "Posted on February 2, 2017", "Contributor": null, "Content": "\nThe Microsoft Educator Community website (education.microsoft.com) is a central hub for educators worldwide that allows educators everywhere to connect and collaborate, find training and lessons, and grow their professional careers with Microsoft products such as 0ffice 365.\n\nThe site offers various content and activities for educators such as courses, resources, webinars, lesson plans, Skype activities and more. Currently it has more than 14,000 content pages. Dr. Iris Yuster – the business development manager for the website -- wanted to use that content and previous user behavior to increase return visits, new registrations, personalize the customer experiences, and increase customer engagement.\nThe Educator Community team decided to use the Recommendations API, part of Micorsoft Cognitive Services and some clever user experience improvements to acomplish this goal. They use the service’s capability to provide Item-to-Item recommendations as well as User-to-Item recommendations.\nThe Educator Community used Item-to-Item Recommendations on the content pages. The Recommendations API uses machine learning techniques to present additional content related to the featured lesson/article. They called this module  “Educators also viewed” and launched it on July 2016.\nThe module displays in two different ways:\nA floating window on the right corner of content pages – visible in front of the page and floating while scrolling down  \n\nThe floating window expands while clicking on it to display the items. \n\nThe module full view is also displayed on the bottom of the content page.\n\nThe Education Community team also wanted to personalize the first impression of a customer when they visit the site.\nThe team used individual educator behavior during previous visits to train a recommendations model that provides items that are uniquely tailored for that educator. They called this module “Recommended for you, ” and it is displayed on the home page and main landing pages.\nHome page recommendations include a mix of content from all site categories that are relevant for the educator, while on category landing pages it displays only recommendations from the same category. For example, on the Courses landing page there are only recommended items from courses and related sub-categories:\n\nThe Education community team performed an A/B test. Participants were assigned randomly into one of two groups: Group A (The Recommendations Group) saw the recommendations module on the content pages, while Group B did not see any recommendations.\nThe sample included 37,000 of educators for each group. Once an educator was placed in a group, they stayed in that group during all the testing period.\nThe impact was significant! These are some of the insights they gathered:\n\nAbout 6% of the customers clicked on recommended content.  Those educators consumed more content, had longer visits, and returned for more visits as compared to the educators without recommendations.\n\nThose in the Recommendations Group were about 3 times more likely to register as members of the Education community.\n\n\nAnd educators in the Recommendations Group had significantly less bounce rate!\n\n\n\n    \n\n\n \nThe bottom line…\nPersonalized recommendations are not a nice to have feature; it’s a must have on any site. The Recommendations API, part of Microsoft Cognitive Services helped the Educator Community to achieve and improve engagement APIs and provide a more personalized experience.\nThe Educator Community uses Microsoft Cognitive Services to put educators in the center, think about their personal interests, and create a place like home where they want to stay  – before, during and after school.\nLinks\nMicrosoft Education Community Recommendations APIMicrosoft Cognitive Services\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-educator-community-harnesses-recommendations-api-to-engage-educators-around-the-world/", "Role": null},
{"Title": "Announcing Custom Speech Service (Preview) from Microsoft Cognitive Services", "Date": "Posted on February 6, 2017", "Contributor": null, "Content": "\nWe are excited to announce the public preview release of the Custom Speech Service from Microsoft Cognitive Services. The Custom Speech Service (formerly the Custom Recognition Intelligent Service) lets you customize Microsoft’s speech-to-text engine. By uploading text and/or speech data to the Custom Speech Service that reflects your application and your users, you can create custom models that can be combined with Microsoft’s state-of-the-art speech models and deployed to a custom speech-to-text endpoint, accessible from any device.\nWhy customize the speech-to-text engine?\nSpeech recognition systems are composed of several components. Two of the most important components are the acoustic model and the language model. The acoustic and language models behind Microsoft’s world-class speech recognition engine have been optimized for common usage scenarios, such as interacting with Cortana on your smart phone, tablet or PC, searching the web by voice, or sending text messages to a friend.\nIf your application contains particular vocabulary items, such as product names or jargon that rarely occur in typical speech, it is likely that you can obtain improved performance by customizing the language model.\nFor example, if you were building an app to assist automotive mechanics, terms like “powertrain” or “catalytic converter” or “limited slip differential” will appear more frequently in this application than in typical voice applications. Customizing the language model will enable the system to learn this.  \nSimilarly, customizing the acoustic model can enable the system to do a better job recognizing speech in particular environments or from particular user populations. For example, if you have a voice-enabled app designed for use in a warehouse or factory, a custom acoustic model can more accurately recognize speech in the presence of the noises found in these environments.\nHow do I get started?\nVisit www.cris.ai to learn how to create and deploy custom speech-to-text models. The site provides resources that enable you to use a a simple interface to import text and/or audio data, create custom acoustic and language models, and evaluate performance. The custom models can be deployed in conjunction with Microsoft’s existing state-of-the-art models to create custom speech-to-text endpoints.\nWe've made some sample text data for building and testing a custom language model available on Custom Speech Service GitHub page. The model will enable you to build an application that can transcribe facts about dinosaurs, because, you know, everybody loves dinosaurs.\nWe welcome your Feedback and Questions link = https://cognitive.uservoice.com/\n", "link": "https://azure.microsoft.com/en-us/blog/new-custom-speech-service-available-in-public-preview/", "Role": null},
{"Title": "Microsoft Cognitive Services – General availability for Face API, Computer Vision API and Content Moderator", "Date": "Posted on April 19, 2017", "Contributor": null, "Content": "\nThis post was authored by the Cognitive Services Team​.\nMicrosoft Cognitive Services enables developers to create the next generation of applications that can see, hear, speak, understand, and interpret needs using natural methods of communication. We have made adding intelligent features to your platforms easier.\nToday, at the first ever Microsoft Data Amp online event, we’re excited to announce the general availability of Face API, Computer Vision API and Content Moderator API from Microsoft Cognitive Services.\n\nFace API detects human faces and compares similar ones, organizes people into groups according to visual similarity, and identifies previously tagged people and their emotions in images.\nComputer Vision API gives you the tools to understand the contents of any image. It creates tags that identify objects, beings like celebrities, or actions in an image, and crafts coherent sentences to describe it. You can now detect landmarks and handwriting in images. Handwriting detection remains in preview.\nContent Moderator provides machine assisted moderation of text and images, augmented with human review tools. Video moderation is available in preview as part of Azure Media Services.\n\nLet’s take a closer look at what these APIs can do for you.\n\nAnna is presenting us the latest updates of Cognitive Services.\nBring vision to your app\nPreviously, users of Face API could obtain attributes such as age, gender, facial points, and headpose. Now, it’s also possible to obtain emotions in the same Face API call. This responds to some user scenarios in which both age and emotions were requested simultaneously. Learn more about Face API in our guides.\nRecognizing landmarks\nWe’ve added more richness to Computer Vision API by integrating landmark recognition. Landmark models, as well as Celebrity Recognition, are examples of Domain Specific Models. Our landmark recognition model recognizes 9,000 natural and man-made landmarks from around the world. Domain Specific Models is a continuously evolving feature within Computer Vision API.\nLet’s say I want my app to recognize this picture I took while traveling:\n\nYou could have an idea about where this comes from, but how could a machine easily know it?\nIn C#, we can leverage these capabilities by making a simple REST API call as the following. By the way, other languages are at the bottom of this post.\n\r\nusing System;\r\nusing System.IO;\r\nusing System.Net.Http;\r\nusing System.Net.Http.Headers;\r\n\r\nnamespace CSHttpClientSample\r\n{\r\n    static class Program\r\n    {\r\n        static void Main()\r\n        {\r\n            Console.Write(\"Enter image file path: \");\r\n            string imageFilePath = Console.ReadLine();\r\n\r\n            MakeAnalysisRequest(imageFilePath);\r\n\r\n            Console.WriteLine(\"\\n\\nHit ENTER to exit...\\n\");\r\n            Console.ReadLine();\r\n        }\r\n\r\n        static byte[] GetImageAsByteArray(string imageFilePath)\r\n        {\r\n            FileStream fileStream = new FileStream(imageFilePath, FileMode.Open, FileAccess.Read);\r\n            BinaryReader binaryReader = new BinaryReader(fileStream);\r\n            return binaryReader.ReadBytes((int)fileStream.Length);\r\n        }\r\n\r\n        static async void MakeAnalysisRequest(string imageFilePath)\r\n        {\r\n            var client = new HttpClient();\r\n\r\n            // Request headers. Replace the second parameter with a valid subscription key.\r\n            client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", \"putyourkeyhere\");\r\n\r\n            // Request parameters. You can change \"landmarks\" to \"celebrities\" on requestParameters and uri to use the Celebrities model.\r\n            string requestParameters = \"model=landmarks\";\r\n            string uri = \"https://westus.api.cognitive.microsoft.com/vision/v1.0/models/landmarks/analyze?\" + requestParameters;\r\n            Console.WriteLine(uri);\r\n\r\n            HttpResponseMessage response;\r\n\r\n            // Request body. Try this sample with a locally stored JPEG image.\r\n            byte[] byteData = GetImageAsByteArray(imageFilePath);\r\n\r\n            using (var content = new ByteArrayContent(byteData))\r\n            {\r\n                // This example uses content type \"application/octet-stream\".\r\n                // The other content types you can use are \"application/json\" and \"multipart/form-data\".\r\n                content.Headers.ContentType = new MediaTypeHeaderValue(\"application/octet-stream\");\r\n                response = await client.PostAsync(uri, content);\r\n                string contentString = await response.Content.ReadAsStringAsync();\r\n                Console.WriteLine(\"Response:\\n\");\r\n                Console.WriteLine(contentString);\r\n            }\r\n        }\r\n    }\r\n}\r\n\nThe successful response, returned in JSON would be the following:\n\r\n```json\r\n{\r\n  \"requestId\": \"b15f13a4-77d9-4fab-a701-7ad65bcdcaed\",\r\n  \"metadata\": {\r\n    \"width\": 1024,\r\n    \"height\": 680,\r\n    \"format\": \"Jpeg\"\r\n  },\r\n  \"result\": {\r\n    \"landmarks\": [\r\n      {\r\n        \"name\": \"Colosseum\",\r\n        \"confidence\": 0.9448209\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\nRecognizing handwriting\nHandwriting OCR is also available in preview in Computer Vision API. This feature detects text in a handwritten image and extracts the recognized characters into a machine-usable character stream.\r\nIt detects and extracts handwritten text from notes, letters, essays, whiteboards, forms, etc. It works with different surfaces and backgrounds such as white paper, sticky notes, and whiteboards. No need to transcribe those handwritten notes anymore; you can snap an image instead and use Handwriting OCR to digitize your notes, saving time, effort, and paper clutter. You can even decide to do a quick search when you want to pull the notes up again.\nYou can try this out yourself by uploading your sample in the interactive demonstration.\nLet’s say that I want to recognize the handwriting in the whiteboard:\n\nAn inspiration quote I’d like to keep.\nIn C#, I would use the following:\n\r\nusing System;\r\nusing System.IO;\r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing System.Net.Http;\r\nusing System.Net.Http.Headers;\r\n\r\nnamespace CSHttpClientSample\r\n{\r\n    static class Program\r\n    {\r\n        static void Main()\r\n        {\r\n            Console.Write(\"Enter image file path: \");\r\n            string imageFilePath = Console.ReadLine();\r\n\r\n            ReadHandwrittenText(imageFilePath);\r\n\r\n            Console.WriteLine(\"\\n\\n\\nHit ENTER to exit...\");\r\n            Console.ReadLine();\r\n        }\r\n\r\n        static byte[] GetImageAsByteArray(string imageFilePath)\r\n        {\r\n            FileStream fileStream = new FileStream(imageFilePath, FileMode.Open, FileAccess.Read);\r\n            BinaryReader binaryReader = new BinaryReader(fileStream);\r\n            return binaryReader.ReadBytes((int)fileStream.Length);\r\n        }\r\n\r\n        static async void ReadHandwrittenText(string imageFilePath)\r\n        {\r\n            var client = new HttpClient();\r\n\r\n            // Request headers - replace this example key with your valid subscription key.\r\n            client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", \"putyourkeyhere\");\r\n\r\n            // Request parameters and URI. Set \"handwriting\" to false for printed text.\r\n            string requestParameter = \"handwriting=true\";\r\n            string uri = \"https://westus.api.cognitive.microsoft.com/vision/v1.0/recognizeText?\" + requestParameter;\r\n\r\n            HttpResponseMessage response = null;\r\n            IEnumerable<string> responseValues = null;\r\n            string operationLocation = null;\r\n\r\n            // Request body. Try this sample with a locally stored JPEG image.\r\n            byte[] byteData = GetImageAsByteArray(imageFilePath);\r\n            var content = new ByteArrayContent(byteData);\r\n\r\n            // This example uses content type \"application/octet-stream\".\r\n            // You can also use \"application/json\" and specify an image URL.\r\n            content.Headers.ContentType = new MediaTypeHeaderValue(\"application/octet-stream\");\r\n\r\n            try {\r\n                response = await client.PostAsync(uri, content);\r\n                responseValues = response.Headers.GetValues(\"Operation-Location\");\r\n            }\r\n            catch (Exception e)\r\n            {\r\n                Console.WriteLine(e.Message);\r\n            }\r\n\r\n            foreach (var value in responseValues)\r\n            {\r\n                // This value is the URI where you can get the text recognition operation result.\r\n                operationLocation = value;\r\n                Console.WriteLine(operationLocation);\r\n                break;\r\n            }\r\n\r\n            try\r\n            {\r\n                // Note: The response may not be immediately available. Handwriting recognition is an\r\n                // async operation that can take a variable amount of time depending on the length\r\n                // of the text you want to recognize. You may need to wait or retry this operation.\r\n                response = await client.GetAsync(operationLocation);\r\n\r\n                // And now you can see the response in in JSON:\r\n                Console.WriteLine(await response.Content.ReadAsStringAsync());\r\n            }\r\n            catch (Exception e)\r\n            {\r\n                Console.WriteLine(e.Message);\r\n            }\r\n        }\r\n    }\r\n}\r\n\nUpon success, the OCR results returned include text, bounding box for regions, lines, and words through the following JSON:\n\r\n{\r\n  \"status\": \"Succeeded\",\r\n  \"recognitionResult\": {\r\n    \"lines\": [\r\n      {\r\n        \"boundingBox\": [\r\n          542,\r\n          724,\r\n          1404,\r\n          722,\r\n          1406,\r\n          819,\r\n          544,\r\n          820\r\n        ],\r\n        \"text\": \"You must be the change\",\r\n        \"words\": [\r\n          {\r\n            \"boundingBox\": [\r\n              535,\r\n              725,\r\n              678,\r\n              721,\r\n              698,\r\n              841,\r\n              555,\r\n              845\r\n            ],\r\n            \"text\": \"You\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              713,\r\n              720,\r\n              886,\r\n              715,\r\n              906,\r\n              835,\r\n              734,\r\n              840\r\n            ],\r\n            \"text\": \"must\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              891,\r\n              715,\r\n              982,\r\n              713,\r\n              1002,\r\n              833,\r\n              911,\r\n              835\r\n            ],\r\n            \"text\": \"be\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1002,\r\n              712,\r\n              1129,\r\n              708,\r\n              1149,\r\n              829,\r\n              1022,\r\n              832\r\n            ],\r\n            \"text\": \"the\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1159,\r\n              708,\r\n              1427,\r\n              700,\r\n              1448,\r\n              820,\r\n              1179,\r\n              828\r\n            ],\r\n            \"text\": \"change\"\r\n          }\r\n        ]\r\n      },\r\n      {\r\n        \"boundingBox\": [\r\n          667,\r\n          905,\r\n          1766,\r\n          868,\r\n          1771,\r\n          976,\r\n          672,\r\n          1015\r\n        ],\r\n        \"text\": \"you want to see in the world !\",\r\n        \"words\": [\r\n          {\r\n            \"boundingBox\": [\r\n              665,\r\n              901,\r\n              758,\r\n              899,\r\n              768,\r\n              1015,\r\n              675,\r\n              1017\r\n            ],\r\n            \"text\": \"you\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              752,\r\n              900,\r\n              941,\r\n              896,\r\n              951,\r\n              1012,\r\n              762,\r\n              1015\r\n            ],\r\n            \"text\": \"want\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              960,\r\n              896,\r\n              1058,\r\n              895,\r\n              1068,\r\n              1010,\r\n              970,\r\n              1012\r\n            ],\r\n            \"text\": \"to\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1077,\r\n              894,\r\n              1227,\r\n              892,\r\n              1237,\r\n              1007,\r\n              1087,\r\n              1010\r\n            ],\r\n            \"text\": \"see\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1253,\r\n              891,\r\n              1338,\r\n              890,\r\n              1348,\r\n              1006,\r\n              1263,\r\n              1007\r\n            ],\r\n            \"text\": \"in\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1344,\r\n              890,\r\n              1488,\r\n              887,\r\n              1498,\r\n              1003,\r\n              1354,\r\n              1005\r\n            ],\r\n            \"text\": \"the\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1494,\r\n              887,\r\n              1755,\r\n              883,\r\n              1765,\r\n              999,\r\n              1504,\r\n              1003\r\n            ],\r\n            \"text\": \"world\"\r\n          },\r\n          {\r\n            \"boundingBox\": [\r\n              1735,\r\n              883,\r\n              1813,\r\n              882,\r\n              1823,\r\n              998,\r\n              1745,\r\n              999\r\n            ],\r\n            \"text\": \"!\"\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n}\r\n\nTo easily get started in your preferred language, please refer to the following:\n\nThe Face API page and quick-start guides for on C#, Java, Python, and many more.\nThe Computer Vision API page and quick-start guides on C#, Java, Python, and more.\nThe Content Moderator Page and test drive Content Moderator to learn how we enable a complete, configurable content moderation lifecycle.\n\nFor more information about our use cases, don’t hesitate to take a look at our customer stories, including a great use of our Vision APIs with GrayMeta.\nHappy coding!\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-cognitive-services-general-availability-for-face-api-computer-vision-api-and-content-moderator/", "Role": null},
{"Title": "Introducing Video Indexer, a cloud service to unlock insights from your videos", "Date": "Posted on May 10, 2017", "Contributor": "Milan Gada", "Content": "\nThe amount of video on the internet is growing exponentially and will continue to grow for the foreseeable future. And the growth is across multiple industries, such as entertainment, broadcasting, enterprises, public safety and others.\nMost videos only have a title and a description as metadata associated with the videos. But a video is a lot more than what the title and description typically capture (especially when the video is more than a minute long). The lack of human understandable time-stamped metadata makes discoverability of videos, and the relevant moments within a video, a challenging task. Generating such metadata for videos is expensive and humanly impossible when you have lots of videos. This is where artificial intelligence technologies can help.\nAt Build 2017, we are announcing the public preview of a cloud service called Video Indexer as part of Microsoft Cognitive Services. Video Indexer enables customers with digital video and audio content to automatically extract metadata and use it to build intelligent innovative applications. Video Indexer is built using Azure Media Services, Microsoft Cognitive Services, Azure Search, Azure Storage and Azure Document DB. It brings the best of Microsoft AI technologies for video, in the form of a scalable cloud service for customers.\nVideo Indexer has been built based on feedback from customers on the Microsoft Garage project called Video Breakdown, which was launched in September 2016. Customers across multiple industries have experimented with Video Breakdown. The following quote from Jonathan Huberman, CEO, Ooyala is a testament of the value of Video Indexer – “As a global provider of video monetization software and services, we are constantly looking for technologies that would help us provide more value to our customers. With Azure and Microsoft’s AI technologies for processing video we were really impressed with the combination of easy to use yet powerful AI services for videos. The integrations we have built between Video Indexer and our products will help our customers enhance content discovery and captioning as well as deliver targeted advertising based on the extracted metadata - a win-win for our customers and their viewers.” \nYou don’t need to have any background in machine learning or computer vision to use Video Indexer. You can even get started without writing a single line of code. Video Indexer offers simple but powerful APIs and puts the power of AI technologies for video within the reach of every developer. You can learn more about this in the Video Indexer documentation.\nIn what follows, let’s look at some of the customer use cases enabled by Video Indexer, followed by a high level description of features.\nCustomer use cases\n\nSearch – Insights extracted from the video can be used to enhance the search experience across a video library. For example, indexing spoken words and faces can enable the search experience of finding moments in a video where a particular person spoke certain words or when two people were seen together. Search based on such insights from videos is applicable to news agencies, educational institutes, broadcasters, entertainment content owners, enterprise LOB apps and in general to any industry that has a video library that users need to search against.\nMonetization – Video Indexer can help improve the value of videos. As an example, industries that rely on ad revenue (e.g. news media, social media, etc.), can deliver more relevant ads by using the extracted insights as additional signals to the ad server (presenting a sports shoe ad is more relevant in the middle of a football match vs. a swimming competition).\nUser engagement – Video insights can be used to improve user engagement by positioning the relevant video moments to users. As an example, consider an educational video that explains spheres for the first 30 minutes and pyramids in the next 30 minutes. A student reading about pyramids would benefit more if the video is positioned starting from the 30 minute marker.\n\n\nFunctionality\nAt a high level, REST APIs includes the following functionalities. For more details, please take a look at the Video Indexer documentation.\n \n\nContent upload – You can upload videos by providing a URL. Video Indexer starts processing videos as soon as they are uploaded. Multiple AI technologies are used to extract insights across multiple dimensions (spoken words, faces, visual text, objects, etc.)\nInsights download – Once a video finishes processing, you can download the extracted insights in the form of a JSON file.\nSearch – You can submit search queries for searching for relevant moments within a video or for moments across all videos in your Video Indexer account.\nPlayer widget – You can obtain a player widget for a video, that you can embed in any web application. Player widget would enable you to stream the video using adaptive bit rate.\nInsights widget – You can also obtain an insights widget for showcasing the extracted insights. Just like the player widget, an insights widget can be embedded in any web application. You can also choose which parts of the insights widget you want to show and which you want to hide.\n\n \nThe Video Indexer portal enables you to\n\nUpload videos from a local machine.\nView the insights extracted from the video in a UI built using the various widgets mentioned above.\nCurate the insights and submit that back to the service. This would include providing names for faces that have been detected but not recognized, making corrections in text extracted based on spoken words or based on optical character recognition.\nObtain an embed code for the player or insights widget.\n\nVideo Indexer includes the following video AI technologies. Each technology listed below is applied to every video that is uploaded to Video Indexer.\n\nAudio Transcription – Video Indexer has speech-to-text functionality which enables customers to get a transcript of the spoken words. Supported languages include English, Spanish, French, German, Italian, Chinese (Simplified), Portuguese (Brazilian), Japanese and Russian (with many more to come in the future). The speech-to-text functionality is based on the same speech engine that is used by Cortana and Skype.\nFace tracking and identification– Face technologies enable detection of faces in a video. The detected faces are matched against a celebrity database to evaluate which celebrities are present in the video. Customers can also label faces that do not match a celebrity. Video Indexer builds a face model based on those labels and can recognize those faces in videos submitted in the future.\nSpeaker indexing – Video Indexer has the ability to map and understand which speaker spoke which words and when.\nVisual text recognition – With this technology, Video Indexer service extracts text that is displayed in the videos.\nVoice activity detection – This enables Video Indexer to detect silence, speech and hand-clapping. \nScene detection – Video Indexer has the ability to perform visual analysis on the video to determine when a scene changes in a video.\nKeyframe extraction – Video Indexer automatically detects keyframes in a video.\nSentiment analysis – Video Indexer performs sentiment analysis on the text extracted using speech-to-text as well as optical character recognition, and provide that information in the form of positive, negative of neutral sentiments, along with timecodes.\nTranslation – Video Indexer has the ability to translate the audio transcript from one language to another. Multiple languages (English, Spanish, French, German, Italian, Chinese-Simplified, Portuguese-Brazilian,  Japanese and Russian) are supported. Once translated, the user can even get captioning in the video player in other languages.\nVisual content moderation – This technology enables detection of adult and/or racy material present in the video and can be used for content filtering.\nKeywords extraction – Video Indexer extracts keywords based on the transcript of the spoken words and text recognized by visual text recognizer.\nAnnotation – Video Indexer annotates the video based on a pre-defined model of 2000 objects.\n\nWe hope you share our excitement about the new opportunities Video Indexer enables to transform your apps and your business. We are looking forward to seeing how you will use this new service. Try it out today at https://vi.microsoft.com.\n", "link": "https://azure.microsoft.com/en-us/blog/introducing-video-indexer-a-cloud-service-to-unlock-insights-from-your-videos/", "Role": "Principal Program Manager, Azure Media Services"},
{"Title": "At Build, Microsoft expands its Cognitive Services collection of intelligent APIs", "Date": "Posted on May 10, 2017", "Contributor": null, "Content": "\nThis blog post was authored by the Microsoft Cognitive Services Team. \nMicrosoft Cognitive Services enables developers to augment the next generation of applications with the ability to see, hear, speak, understand, and interpret needs using natural methods of communication.\nToday at the Build 2017 conference, we are excited to announce the next big wave of innovation for Microsoft Cognitive Services, significantly increasing the value for developers looking to embrace AI and build the next generation of applications.\n\nCustomizable: With the addition of Bing Custom Search, Custom Vision Service and Custom Decision Service on top of Custom Speech and Language Understanding Intelligent Service, we now have a broader set of custom AI APIs available, allowing customers to use their own data with algorithms that are customized for their specific needs.\nCutting edge technologies: Today we are launching Microsoft’s Cognitive Services Labs, which allow any developer to take part in the broader research community’s quest to better understand the future of cognitive computing, by experimenting with new services still in the early stages of development. One of the first AI services being made available via our Cognitive Services Labs is Project Prague,  which lets you use gestures to control and interact with technologies to have more intuitive and natural experiences.  This cutting edge and easy to use SDK is in private preview.\nHigh pace of innovation: We’re expanding our Cognitive Services portfolio to 29 intelligent APIs with the addition of Video Indexer, Custom Decision Service, Bing Custom Search, and Custom Vision Service, along with the new Cognitive Services Lab Project Prague, for gestures, and updates to our existing Cognitive Services, such as Bing Search, Microsoft Translator and Language Understanding Intelligent Service.\n\nToday, 568,000+ developers from more than 60 of countries are using Microsoft Cognitive Services that allow systems to see, hear, speak, understand and interpret our needs.\nWhat are the capabilities of these new services?\n\nCustom Vision Service, available today in free public preview, is an easy-to-use, customizable web service that learns to recognize specific content in imagery, powered by state-of-the-art machine learning neural networks that become smarter with training. You can train it to recognize whatever you choose, whether that be animals, objects, or abstract symbols. This technology could easily apply to retail environments for machine-assisted product identification, or in digital space to automatically help sorting categories of pictures.\nVideo Indexer, available today in free public preview, is one of the industry’s most comprehensive video AI services. It helps you unlock insights from any video by indexing and enabling you to search spoken audio that is transcribed and translated, sentiment, faces that appeared and objects. With these insights, you can improve discoverability of videos in your applications or increase user engagement by embedding this capability in sites. All of these capabilities are available through a simple set of APIs, ready to use widgets and a management portal.\nCustom Decision Service, available today in free public preview, is a service that helps you create intelligent systems with a cloud-based contextual decision-making API that adapts with experience. Custom Decision service uses reinforcement learning in a new approach for personalizing content; it’s able to plug into your application and helps to make decisions in real time as it automatically adapts to optimize your metrics over time.\nBing Custom Search, available today in free public preview, lets you create a highly-customized web search experience, which delivers better and more relevant results from your targeted web space. Featuring a straightforward User Interface, Bing Custom Search enables you to create your own web search service without a line of code. Specify the slices of the web that you want to draw from and explore site suggestions to intelligently expand the scope of your search domain. Bing Custom Search can empower businesses of any size, hobbyists and entrepreneurs to design and deploy web search applications for any possible scenario.\nMicrosoft’s Cognitive Services Labs allow any developer to experiment with new services still in the early stages of development. Among them, Project Prague is one of the services currently in private preview. This SDK is built from an intensive library of hand poses that creates more intuitive experiences by allowing users to control and interact with technologies through typical hand movements. Using a special camera to record the gestures, the API then recognizes the formation of the hand and allows the developer to tie in-app actions to each gesture.\nNext version of Bing APIs, available in public preview, allowing developers to bring the vast knowledge of the web to their users and benefit from improved performance, new sorting and filtering options, robust documentation, and easy Quick Start guides. This release includes the full suite of Bing Search APIs (Bing Web Search API Preview, Bing News Search API Preview, Bing Video Search API Preview, and Bing Image Search API Preview), Bing Autosuggest API Preview, and Bing Spell Check API Preview. Please find more information in the announcement blog.\nPresentation Translator, a Microsoft Garage project provides presenters the ability to add subtitles to their presentations, in the same language for accessibility scenarios or in another language for multi-language situations. Audience members get subtitles in their desired language on their own device through the Microsoft Translator app, in a browser and (optionally) translate the slides while preserving their formatting. Click here to be notified when it’s available.\nLanguage Understanding Intelligent Service (LUIS) improvements - helps developers integrate language models that understand users quickly and easily, using either prebuilt or customized models. Updates to LUIS include increased intents and entities, introduction of new powerful developer tools for productivity, additional ways for the community to use and contribute, improved speech recognition with Microsoft Bot Framework, and more global availability.\n\nLet’s take a closer look at what these new APIs and Services can do for you.\nBring custom vision to your app\nThank to Custom Vision Service, it becomes pretty easy to create your own image recognition service. You can use the Custom Vision Service Portal to upload a series of images to train your classifier and a few images to test it after the classifier is trained.\n\nIt’s also possible to code each step: let’s say I need to quickly create my image classifier for a specific need, this can be products my users are uploading on my website, retail merchandize or even animal images in a forest.\n\nTo get started, I would need the Custom Vision API, which can be found with this SDK. I need to create a console application and prepare the training key & the images needed for the example\n\nI can start with Visual Studio to create a new Console Application, and replace the contents of Program.cs with the following code. This code defines and calls two helper methods:\n\nThe method called GetTrainingKey prepares the training key.\nThe one called LoadImagesFromDisk loads two sets of images that this example uses to train the project, and one test image that the example loads to demonstrate the use of the default prediction endpoint.\n\n\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.IO;\r\nusing System.Linq;\r\nusing System.Threading;\r\nusing Microsoft.Cognitive.CustomVision;\r\n\r\nnamespace SmokeTester\r\n{\r\n    class Program\r\n    {\r\n        private static List<MemoryStream> hemlockImages;\r\n\r\n        private static List<MemoryStream> japaneseCherryImages;\r\n\r\n        private static MemoryStream testImage;\r\n\r\n        static void Main(string[] args)\r\n        {\r\n            // You can either add your training key here, pass it on the command line, or type it in when the program runs\r\n            string trainingKey = GetTrainingKey(\"<your key here>\", args);\r\n\r\n            // Create the Api, passing in a credentials object that contains the training key\r\n            TrainingApiCredentials trainingCredentials = new TrainingApiCredentials(trainingKey);\r\n            TrainingApi trainingApi = new TrainingApi(trainingCredentials);\r\n\r\n            // Upload the images we need for training and the test image\r\n            Console.WriteLine(\"\\tUploading images\");\r\n            LoadImagesFromDisk();\r\n        }\r\n\r\n        private static string GetTrainingKey(string trainingKey, string[] args)\r\n        {\r\n            if (string.IsNullOrWhiteSpace(trainingKey) || trainingKey.Equals(\"<your key here>\"))\r\n            {\r\n                if (args.Length >= 1)\r\n                {\r\n                    trainingKey = args[0];\r\n                }\r\n\r\n                while (string.IsNullOrWhiteSpace(trainingKey) || trainingKey.Length != 32)\r\n                {\r\n                    Console.Write(\"Enter your training key: \");\r\n                    trainingKey = Console.ReadLine();\r\n                }\r\n                Console.WriteLine();\r\n            }\r\n\r\n            return trainingKey;\r\n        }\r\n\r\n        private static void LoadImagesFromDisk()\r\n        {\r\n            // this loads the images to be uploaded from disk into memory\r\n            hemlockImages = Directory.GetFiles(@\"..\\..\\..\\..\\..\\SampleImages\\Hemlock\").Select(f => new MemoryStream(File.ReadAllBytes(f))).ToList();\r\n            japaneseCherryImages = Directory.GetFiles(@\"..\\..\\..\\..\\..\\SampleImages\\Japanese Cherry\").Select(f => new MemoryStream(File.ReadAllBytes(f))).ToList();\r\n            testImage = new MemoryStream(File.ReadAllBytes(@\"..\\..\\..\\..\\..\\SampleImages\\Test\\test_image.jpg\"));\r\n\r\n        }\r\n    }\r\n}\r\n\n\nAs next step, I would need to Create a Custom Vision Service project, adding the following code in the Main() method after the call to LoadImagesFromDisk().\n\n \n\r\n            // Create a new project\r\n            Console.WriteLine(\"Creating new project:\");\r\n            var project = trainingApi.CreateProject(\"My New Project\");\r\n\n \n \n \n\nNext, I need to add tags to my project by insert the following code after the call to CreateProject()\n\n\r\n            // Make two tags in the new project\r\n            var hemlockTag = trainingApi.CreateTag(project.Id, \"Hemlock\");\r\n            var japaneseCherryTag = trainingApi.CreateTag(project.Id, \"Japanese Cherry\");\r\n\n\nThen, I need to Upload images in memory to the project, by inserting the following code at the end of the Main() method:\n\n \n\r\n            // Images can be uploaded one at a time\r\n            foreach (var image in hemlockImages)\r\n            {\r\n                trainingApi.CreateImagesFromData(project.Id, image, new List<string>() { hemlockTag.Id.ToString() });\r\n            }\r\n\r\n            // Or uploaded in a single batch \r\n            trainingApi.CreateImagesFromData(project.Id, japaneseCherryImages, new List<Guid>() { japaneseCherryTag.Id });\r\n\n \n\nNow that I've added tags and images to the project, I can train it. I would need to insert the following code at the end of Main(). This creates the first iteration in the project. I can then mark this iteration as the default iteration.\n\n\r\n            // Now there are images with tags start training the project\r\n            Console.WriteLine(\"\\tTraining\");\r\n            var iteration = trainingApi.TrainProject(project.Id);\r\n\r\n            // The returned iteration will be in progress, and can be queried periodically to see when it has completed\r\n            while (iteration.Status == \"Training\")\r\n            {\r\n                Thread.Sleep(1000);\r\n\r\n                // Re-query the iteration to get it's updated status\r\n                iteration = trainingApi.GetIteration(project.Id, iteration.Id);\r\n            }\r\n\r\n            // The iteration is now trained. Make it the default project endpoint\r\n            iteration.IsDefault = true;\r\n            trainingApi.UpdateIteration(project.Id, iteration.Id, iteration);\r\n            Console.WriteLine(\"Done!\\n\");\r\n\n\nAs I’m now ready to use the model for prediction, I first obtain the endpoint associated with the default iteration; then I send a test image to the project using that endpoint. Insert the code below at the end of Main().\n\n \n\r\n            // Now there is a trained endpoint, it can be used to make a prediction\r\n\r\n            // Get the prediction key, which is used in place of the training key when making predictions\r\n            var account = trainingApi.GetAccountInfo();\r\n            var predictionKey = account.Keys.PredictionKeys.PrimaryKey;\r\n\r\n            // Create a prediction endpoint, passing in a prediction credentials object that contains the obtained prediction key\r\n            PredictionEndpointCredentials predictionEndpointCredentials = new PredictionEndpointCredentials(predictionKey);\r\n            PredictionEndpoint endpoint = new PredictionEndpoint(predictionEndpointCredentials);\r\n\r\n            // Make a prediction against the new project\r\n            Console.WriteLine(\"Making a prediction:\");\r\n            var result = endpoint.PredictImage(project.Id, testImage);\r\n\r\n            // Loop over each prediction and write out the results\r\n            foreach (var c in result.Predictions)\r\n            {\r\n                Console.WriteLine($\"\\t{c.Tag}: {c.Probability:P1}\");\r\n            }\r\n\r\n            Console.ReadKey();\r\n\n \n\nLast step, let’s build and run the solution: the prediction results appear on the console.\n\nFor more information about Custom Vision Service, please take a look at the following resources:\n\nThe Custom Vision Service portal and webpage\nThe full get started guides\n\nPersonalization of your site with Custom Decision Service\nWith Custom Decision Service, you can personalize content on your website, so that users see the most engaging content for them.\nLet’s say I own a news website, with a front page with links to several articles. As the page loads, I want to request Custom Decision Service to provide a ranking of articles to include on the page.\nWhen one of my users clicks on an article, a second request is going to be sent to the Custom Decision Service to log the outcome of the decision. The easiest integration mode requires just an RSS feed for the content and a few lines of javascript to be added into the application. Let’s get started!\n\nFirst, I need to register on the Decision Service Portal by clicking on My Portal menu item in the top ribbon, then I can register the application, choosing a unique identifier. It’s also possible to create a name for an action set feed, along with an RSS or Atom end point currently.\n\n\n\nThe basic use of Custom Decision Service is fairly straightforward: the front page will use Custom Decision Service to specify the ordering of the article pages. I just need to insert the following code into the HTML head of the front page.\n\n \n\r\n// Define the \"callback function\" to render UI\r\n<script> function callback(data) { … } </script>\r\n\r\n// call to Ranking API\r\n<script src=\"https://ds.microsoft.com/<domain>/rank/<actionSetId>\" async></script>\r\n\n \nThe order matters as the callback function should be defined before the call to Ranking API. The data argument contains the ranking of URLs to be rendered. For more information, see the tutorial and API reference.\n\nFor each article page, I need to make sure the canonical URL is set and matches the URLs provided your RSS feed, and insert the following code into the HTML head to call Reward API:\n\n\r\n<script src=\"https://ds.microsoft.com/DecisionService.js\"></script>\r\n<script> window.DecisionService.trackPageView(); </script>\r\n\n\nFinally, I need to provide the Action Set API, which returns the list of articles (a.k.a., actions) to be considered by Custom Decision Service. I can implement this API as an RSS feed, as shown here:\n\n \n\r\n<rss version=\"2.0\">\r\n<channel>\r\n   <item>\r\n      <title><![CDATA[title (possibly with url) ]]></title>\r\n      <link>url</link>\r\n      <pubDate>Thu, 27 Apr 2017 16:30:52 GMT</pubDate>\r\n    </item>\r\n   <item>\r\n       ....\r\n   </item>\r\n</channel>\r\n</rss>\r\n\n \nFor more information about Custom Decision Service, please take a look at the following resources:\n\nThe Custom Decision Service portal and webpage\nThe technical guides\n\nUnlock video insights\nWith Video Indexer, it’s now possible to process and extract lots of insights from video files, such as:\n\nFace detection and identification (finds, identifies, and tracks human faces within a video)\nOCR (optical character recognition, extracting text content from videos and generates searchable digital text)\nTranscript (converting audio to text based on specified language)\nOne of my favorites, differentiation of speakers (maps and understands each speaker and identifies when each speaker is present in the video)\nVoice/sound detection (separating background noise/voice activity from silence)\nSentiment analysis (performing analysis based on multiple emotional attributes - currently, Positive, Neutral, Negative options are supported)\n\n\nFrom one video to multiple insights\nLet’s say I’m a news agency with a video library that my users need to search against: I need to easily extract metadata on the videos to enhance the search experience with indexed spoken words and faces.\n\nThe easiest first step is the simply go to the Video Indexer Web Portal: I can sign-in, upload a video and let Video Indexer start indexing and analyzing the video. Once it’s done, I will receive a notification with a link to my video and a short description of what was found in your video (people, topics, OCRs,..).\nIf I want to use the Video Indexer APIs, I also need to sign-in to the Video Indexer Web Portal, select production and subscribe. This sends the Video Indexer team a subscription request, which will be approved shortly. Once approved, I will be able to see my subscription and my keys.\n\nThe following C# code snippet demonstrates the usage of all the Video Indexer APIs together.\n\n\r\n    var apiUrl = \"https://videobreakdown.azure-api.net/Breakdowns/Api/Partner/Breakdowns\";\r\n    var client = new HttpClient();\r\n    client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", \"InsertYourKey\");\r\n\r\n    var content = new MultipartFormDataContent();\r\n\r\n    Console.WriteLine(\"Uploading...\");\r\n    var videoUrl = \"https:/...\";\r\n    var result = client.PostAsync(apiUrl + \"?name=some_name&description=some_description&privacy=private&partition=some_partition&videoUrl=\" + videoUrl, content).Result;\r\n    var json = result.Content.ReadAsStringAsync().Result;\r\n\r\n    Console.WriteLine();\r\n    Console.WriteLine(\"Uploaded:\");\r\n    Console.WriteLine(json);\r\n\r\n    var id = JsonConvert.DeserializeObject<string>(json);\r\n\r\n    while (true)\r\n    {\r\n        Thread.Sleep(10000);\r\n\r\n        result = client.GetAsync(string.Format(apiUrl + \"/{0}/State\", id)).Result;\r\n        json = result.Content.ReadAsStringAsync().Result;\r\n\r\n        Console.WriteLine();\r\n        Console.WriteLine(\"State:\");\r\n        Console.WriteLine(json);\r\n\r\n        dynamic state = JsonConvert.DeserializeObject(json);\r\n        if (state.state != \"Uploaded\" && state.state != \"Processing\")\r\n        {\r\n            break;\r\n        }\r\n    }\r\n\r\n    result = client.GetAsync(string.Format(apiUrl + \"/{0}\", id)).Result;\r\n    json = result.Content.ReadAsStringAsync().Result;\r\n    Console.WriteLine();\r\n    Console.WriteLine(\"Full JSON:\");\r\n    Console.WriteLine(json);\r\n\r\n    result = client.GetAsync(string.Format(apiUrl + \"/Search?id={0}\", id)).Result;\r\n    json = result.Content.ReadAsStringAsync().Result;\r\n    Console.WriteLine();\r\n    Console.WriteLine(\"Search:\");\r\n    Console.WriteLine(json);\r\n\r\n    result = client.GetAsync(string.Format(apiUrl + \"/{0}/InsightsWidgetUrl\", id)).Result;\r\n    json = result.Content.ReadAsStringAsync().Result;\r\n    Console.WriteLine();\r\n    Console.WriteLine(\"Insights Widget url:\");\r\n    Console.WriteLine(json);\r\n\r\n    result = client.GetAsync(string.Format(apiUrl + \"/{0}/PlayerWidgetUrl\", id)).Result;\r\n    json = result.Content.ReadAsStringAsync().Result;\r\n    Console.WriteLine();\r\n    Console.WriteLine(\"Player token:\");\r\n    Console.WriteLine(json);\r\n\n\nWhen I make an API call and the response status is OK, I will get a detailed JSON output containing details of the specified video insights including keywords (topics), faces, blocks. Each block includes time ranges, transcript lines, OCR lines, sentiments, faces, and block thumbnails.\n\nFor more information, please take a look at:\n\nThe Video Indexer portal and webpage \nThe full list of technical resources in the Get started guide\n\nCreate a highly targeted search for your users\nWith Bing Custom Search, I can create a highly-customized web search experience for my targeted web space: there are a lot of integration scenarios and end-user entry points for a custom search solution.\nFor example, Amicus is building an app that changes the way global aid is funded and delivered by providing donors with full transparency. Amicus needed to help donors learn, find and fund projects specifically related to global aid that were of interest and relevant to them. With Bing Custom Search, Amicus has been able to identify its own set of  relevant web pages in advance: when users have a single concept of interest (like ‘water’, ‘education’ or ‘India’), Bing Custom Search is able to deliver highly relevant results in the context of global aid.\nFor more information about Bing Custom Search, don’t hesitate to look at the Bing Custom Search Blog announcement.\n\nLet’s imagine that I need to build a customized search for my public website on ‘bike touring’ – a very important activity in Seattle area.\n\nI can get started by signing up on the Bing Custom Search Portal and get my free trial key.\nOnce logged in, I can start creating a custom search instance: it contains all the settings that are required to define a custom search tailored towards a scenario of my choice. Here, I want to create a search to find bike touring related content, in that case, I’d create a custom search instance called ‘BikeTouring’.\nThen, I need to define the slices of the web that I want to search over for my scenario and add them to my search instance. The custom slices can include domains, subdomains, or web-pages.\nI can now adjust the default order of the results based on my needs. For example, for a specific query I can pin a specific web-page to the top. Or I can boost and demote sites, or web pages so that they show up higher or lower, respectively, in the set of results that my custom search service returns.\nAfter this, I can track my ranking adjustments in the tabs ‘Active’, ‘Blocked’, and ‘Pinned’. Also, I can revisit my adjustments at any time.\n\n\n\nThen, I publish my settings. Before calling Bing Web Search API directly and programmatically, I can try out my custom search service in the UI directly. For that, I specify a query and click ‘Test API’. I can then see the algorithmic results from my custom search service on the right-hand side.\nTo call and retrieve the results for my custom search service programmatically, I can call Bing Web Search API. In that case, I’d augment the standard Bing Web Search API call with a custom configuration parameter called costumconfig. Below is the API request URL with the costumconfig parameter:\n\n\r\nhttps://api.cognitive.microsoft.com/bingcustomsearch/v5.0/search[?q][&customconfig][&count][&offset][&mkt][&safesearch]\nBelow is a JSON response of a Bing Web Search API call with a customconfig parameter.\n \n\r\n{\r\n    \"_type\" : \"SearchResponse\",\r\n    \"queryContext\" : {...},\r\n    \"webPages\" : {...},\r\n    \"spellSuggestion\" : {...},\r\n    \"rankingResponse\" : {...}\r\n}\r\n\n \nFor more information, please take a look at the dedicated blog announcement as well as the following resources:\r\n•    Bing Custom Search portal\r\n•    The full list of resources in the Get started guide\nNew AI MVP Award Category\nThank you for reaching this far! As a reward, we’re pleased to inform you about our new AI MVP Program!\nAs you know, the world of data and AI is evolving at an unprecedented pace, and so is its community of experts.  The Microsoft MVP Award Program is pleased to announce the launch of the new AI Award Category for recognizing outstanding community leadership among AI experts. Potential “AI MVPs” include developers creating intelligent apps and bots, modeling human interaction (voice, text, speech,...), writing AI algorithms, training data sets and sharing this expertise with their technical communities.\nAn AI MVP will be award based on contributions in the follow technology contribution areas:\n\nThe AI Award Category will be a new addition to the current award categories. If you or someone you know may qualify, submit a nomination!\nThank you again and happy coding!\n", "link": "https://azure.microsoft.com/en-us/blog/at-build-microsoft-expands-its-cognitive-services-collection-of-intelligent-apis/", "Role": null},
{"Title": "Embed Video Indexer insights in your website", "Date": "Posted on June 13, 2017", "Contributor": "Ori Ziv", "Content": "\nVideo Indexer embeddable widgets is a great way to start adding AI insights to your videos. Whether you want to add deep search ability to your published videos or let your users be more engaged with the video content on your website, you can easily achieve that by using the embeddable option at Video Indexer web application or by using Video Indexer API.\nGetting Started\nTo get started embedding Video Indexer insights to your website you must have a registered account. If you don't have an account you can easily Sign-In to Video Indexer using a Microsoft, Google, LinkedIn, or Azure Active Directory and get one generated for you.\nVideo Indexer supports embedding two types of widgets into your application: Cognitive Insights and Player.\nCognitive Insights Widget\nThis widget contains all the visual insights that were extracted from the video after the indexing process such as people appearances, top keywords, sentiment analysis, transcript and search.\nIt also allows you to change the language and get all the insights based on the selected language. Please find more information about this on the Video Indexer web page. \nPlayer Widget\nThe player widget is a customized Azure Media Player that except of providing video streaming, contains extra features such as playback speed and closed captions. You can learn more about this on the Video Indexer web page.\nIn order to embed a widget in your website you need to get an embed code and paste it in your html file. The embed code contains iframe tag with embed URL.\nYou have two options to get the embed URL: via Video Indexer web application or by calling Video Indexer API specific method. We will cover both ways.\nGet your embed code via Video Indexer web application (Public videos only)\nYou can easily get the embed code for your indexed videos with a click of a button:\n1. Login to your account at VI\n2. Upload a video\n3. After indexing process has completed click “play” on the video at the main gallery page.\n4. Click the “embed button” and select the widget you want to embed with the desired options. (player/insights)\r\n \n\n5. Copy and paste the code into your html file.\nNotice: if you embed via the web application you can embed only public videos. Private videos requires accessToken parameter in the embed URL that contains 1h access token for the video.\nGet your embed code via Video Indexer API (Public or Private videos)\nIn order to get the embed URL that contains the accessToken for your video you can use Video Indexer API and call Get Insights Widget Url or Get Player Widget Url by passing the video id.\nIf you manage your own videos you can also get the embed code based on your internal video id by calling Get Insight Widget By External Id.\nIn order to start working with the API you will have to register and get your API subscription key first. The Getting started with the Video Indexer API blog post is where you will find a very detailed blog post about getting started with Video Indexer API.\nAfter you have your embed URL just paste it as “src” attribute of an iframe element which you want to locate anywhere in your website.\n\r\n<iframe width=\"580\" height=\"580\" src=\"https://www.videoindexer.ai/embed/insights/00000000-0000-0000-0000-000000000000/4452cf7e59/?widgets=people,search\" frameborder=\"0\" allowfullscreen></iframe>\r\n\nGet your embed code with editing enabled\nIf you want to provide editing insights capabilities (like we have in our web application) in your embedded widget, you will have to call Get Insights Widget Url  or Get Insight Widget By External Id and add &allowEdit=true.\nEmbedding options\nVideo Indexer widgets are customizable per your need. You can choose to embed only the insights widget or the player, or embed them both.\nEmbed both types of widgets in your application\nCopy and paste the embed codes for the player widget and the insights widget and include the following JS file before the closing <body> tag:\n\r\n<script src=\"https://breakdown.blob.core.windows.net/public/vb.widgets.mediator.js\"></script>\r\n\nThe above file is required in order to handle cross origin communications between the widgets.\nYou can read more about how it works at our docs.\nEmbed cognitive insights and use your Azure Media Player\nIf you are using Azure Media Player in your website you can easily embed Video Indexer insights widget that will communicate with your player using vi communication plugin. Just paste the following script in your page after azure media player library script and you are all set.\n\r\n<script src=\"https://breakdown.blob.core.windows.net/public/amp-vb.plugin.js\"></script>\r\n\nThe plugin let you also get the VTT file for your player and choose if you want to sync between language and transcript with your video.\nFor more information and code samples see the relevant Video Indexer docs.\nEmbed cognitive insights with any video player\nIf you are using other players like YouTube player, Vimeo or your own player you can still embed Video Indexer cognitive insights and make them communicate with your player, for example jump into the relevant moment when user clicks on one of the widgets.\nIn order to achieve that you will have to implement some functions and listen to “postMessage” JavaScript event.\nHere is a detailed demo that demonstrates this approach.\nHow to customize Video Indexer widgets?\nVideo Indexer widget are customizable per your need. You can choose to embed only the insights that you think will be more valuable for your users.\nCustomize the cognitive insights widget\nYou can choose the types of insights you want by specifying them as a value to the  following URL parameter added to the the embed code you get (from API or from the web application):\n&widgets=<list of wanted widgets>\nThe possible values are: people, keywords, sentiments, transcript, search.\r\nThe title of the iframe window can also be customized by providing &title to the iframe url.\nFor example, if you want to embed widget containing only people and search insights and you want the title to be \"myInsights\" the iframe embed URL will look like this:\r\nhttps://www.videoindexer.ai/embed/insights/00000000-0000-0000-0000-000000000000/4452cf7e59/?widgets=people,search&title=myInsights\nYou can see a detailed demo here and read more at Video Indexer docs.\nCustomize the player widget\nIf you embed Video Indexer player you can choose the size of the player by specifying the size of the iframe.\nFor example :\n\r\n<iframe width=\"640\" height=\"360\" src=\"https://www.videoindexer.ai/embed/player/{accountId}/{id}” frameborder=\"0\" allowfullscreen />\r\n\nBy default Video Indexer player will have auto generated closed captions based on the transcript of the video that was extracted from the video with the source language that was selected when the video was uploaded.\nIf you want to embed with a different language you can add &captions=< Language | ”all” | “false” > to the embed player URL or put “all” as the value if you want to have all available languages captions.\r\nIf you want the captions to be dispalyed by defualt you can pass &showCaptions=true\nThe embed URL then will look like this : https://www.videoindexer.ai/embed/player/00000000-0000-0000-0000-000000000000/4452cf7e59/?captions=italian. If you want to disable captions you can pass “false” as value for captions parameter.\nAuto play – by default the player will start playing the video. you can choose not to by passing &autoplay=false to the embed URL above.\nHere is a snippet for full widgets customization options.\nFor more details, please take a look at the Video Indexer Documentation. Follow us on Twitter @Video_Indexer to get the latest news on the Video Indexer. If you have any questions or need help, contact us at visupport@microsoft.com.\n", "link": "https://azure.microsoft.com/en-us/blog/embed-video-indexer-ai-widgets-in-your-website/", "Role": "Front End Software Engineer, Media AI"},
{"Title": "Microsoft Cognitive Services hack: Line Messenger", "Date": "Posted on June 27, 2017", "Contributor": "Ankur Badhwar", "Content": "\nAfter the devastating earthquake in Japan in 2011, it became apparent that there was a need for a communication platform that would help better connect people in such situations. Out of this need, the Line Messenger platform was launched and has since evolved into a popular social platform with users spread across the globe. The platform also has a well-supported SDK and API, enabling developers to extend their creations to hundreds of millions of users.\nWe recently worked with the development team at Line in Tokyo, building out several bot related scenarios that use Cognitive Services on the Line Messenger platform. The resulting hacks ranged from bots that detect what’s on your plate, to bots that can recognize a user’s face, and even a bot that helps people learn and practice new languages.\n \nThe group of around 20 developers had little experience on Azure going in, but within a few hours they were able to develop, host, and launch their Cognitive Service enabled bots. If you’re interested in trying it out, Line have released a Getting Started kit on their Github, and more info can found on the LINE Engineering Blog.\nHave a chat with one of their bots!\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-cognitive-services-hack-line-messenger/", "Role": "Technical Evangelist"},
{"Title": "Text Analytics API now supports analyzing sentiment in 16 languages", "Date": "Posted on June 28, 2017", "Contributor": "Ollie Newth", "Content": "\nYou can now analyze the sentiment of your text in 12 new languages. With this release, you will now be able to get a more complete view of your customer’s voice with an understanding of how your customers feel about your product or service, an international event, or news topic.\nSentiment analysis is now supported in Danish, Dutch, English, Finnish, French, German, Greek, Italian, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, and Turkish. For details on the languages supported across all the capabilities, see the Text Analytics API documentation.\nText Analytics is easy to get started with – try it yourself through the demo experience. Customers are already using the capabilities around the world, and it’s incredibly easy to do so. One such way is through Microsoft Flow, where you can start analyzing tweets and visualize the analytics in Power BI with a few clicks. See the Flow template to see this in action.\n\nThe Text Analytics API is one of Microsoft’s Cognitive Services, which let you build apps with powerful algorithms using just a few lines of code. You can get started for free with a trial account today.\n", "link": "https://azure.microsoft.com/en-us/blog/text-analytics-api-sentiment-16-languages/", "Role": "Program Manager"},
{"Title": "Now Available: Speaker & Video APIs from Microsoft Project Oxford", "Date": "Posted on December 15, 2015", "Contributor": "Julia Nikitina", "Content": "\nLast month we announced new features for Microsoft Project Oxford machine learning APIs, enabling developers across different platforms to easily add intelligence to applications without having to be AI experts. Project Oxford is just one instance of a broader class of work Microsoft is pursuing around artificial intelligence, and our vision for more personal computing experiences and enhanced productivity aided by systems that increasingly can see, hear, speak, understand and even begin to reason.\nToday, the Speaker Recognition APIs and Video APIs are available in public preview, and the Custom Recognition Intelligence Service (CRIS) is accepting invites at www.ProjectOxford.ai. Read more in the blog.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-12-15/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Now Available: Microsoft Face APIs v1.0", "Date": "Posted on December 21, 2015", "Contributor": "Julia Nikitina", "Content": "\nSince the publication of this blog post additional information has been released. To learn more please read Microsoft Cognitive Services – General availability for Face API, Computer Vision API and Content Moderator.\nGood News - We've improved the Face APIs, Bad News - You'll need to update your code.\n\r\nToday we introduced a new version of the Face APIs, still in public preview, which required a major version change from v0 to v1.0. Don’t worry, the existing v0 of the API is sticking around until 6/30/2016 to give time for everyone to migrate.If you are using Detection, Verification, or Grouping functionality, you’ll need to update from v0 to v1.0 in your code (or update to the latest package).If you are using persisted data, such as the identification functionality with Person Group and Person objects, then you’ll need to recreate and train those objects. This is a one-time change and future API updates will not impact persisted data.\n\r\nHere are some upgrade notes on major changes you should be aware of, but be sure to check the v1.0 documentation for all the latest details.\n\nSignature Changes\r\nThe root service endpoint has changed from https://api.projectoxford.ai/face/v0/ to https://api.projectoxford.ai/face/v1.0/\n\r\nThere are also several signature changes for the methods, such as Face-Detect, Face-Identify, Face-Find Similar, Face-Group, so please see the v1.0 documentation for all the details.\n\r\nWe've made these changes to be more consistent in signature to other REST APIs from Microsoft, such as the new Microsoft Graph APIs.\n\nHigher Call Limits\r\nBased on community feedback we're raising the limits on many common API calls\n\n\n\n\nQuota\n\n\nNew Limit\n\n\n\n\nGroup: Faces to be grouped\n\n\nNow returns up to 1,000 candidates (previous limit was 100)\n\n\n\n\nFind Similar: Candidate faces allowed\n\n\nNow returns up to 1,000 candidates (previous limit was 100)\n\n\n\n\nPerson: Persisted faces per Person Object\n\n\nNowstore up to 248 per Person object (previous limit was 32)\n\n\n\n\n\nNew Feature: Face List\r\nA recurring piece of feedback we heard was that Find Similar was painful to use with the 24hr FaceID expiration. We’re introducing a new API feature to address this, called Face Lists. With Face Lists, you can add multiple FaceIDs to a list which is persisted and use this Face List as a target for the Find-Similar API call. A face list holds a maximum of 1,000 faces.\n\r\nLearn more here: Face-List: Add a Face\n\nUpdated NuGet Package\r\nThe version of the NuGet component Microsoft.ProjectOxford.Face has changed from 0.5.x to 1.0.0.0.\n\r\nTo upgrade in Visual Studio:\n\nOpen your project consuming the Microsoft.ProjectOxford.Face package\nIn Visual Studio, go to this menu:\r\n\t\nTools > NuGet Package Manager > Manage NuGet Packages for Solution...\nSelect the package and choose 1.0.0.0 from the version drop-down menu.\n\n\n\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-12-21/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Mimicker Alarm helps you wake up and stay up with AI", "Date": "Posted on January 21, 2016", "Contributor": "Julia Nikitina", "Content": "\nMimicker Alarm app for Android is the latest release through the Microsoft Garage and uses three games to help clearthe cobwebs of sleep to wake you up – and keep you up. The games are a way to show off several Microsoft Project Oxford machine learning APIs that focus on emotion, computer vision and speech. Read more...\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-01-21/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Cognitive Services announced at //Build 2016", "Date": "Posted on March 30, 2016", "Contributor": "Julia Nikitina", "Content": "\nAnnounced at //Build 2016, Microsoft Cognitive Services is a new collection of intelligence and knowledge APIs that enable developers to make their applications more intelligent, engaging and discoverable. It includes intelligent APIs from Bing, the former Project Oxford, and more, that allow systems to see, hear, speak, understand and interpret our needs using natural methods of communication, and knowledge APIs that bring the power of the web to developers.\nWith Cognitive Services, developers can easily add intelligent features – such as emotion and sentiment detection, vision and speech recognition, knowledge, search and language understanding – into their applications. The collection will continuously improve, adding new APIs and updating existing ones. Today we are announcing:\n\nComputer Vision API has expanded to more than  2,000 image categories and now includes natural language captions for images;\nMicrosoft Translator API now includes speech translation;\nEmotion API now can detect emotions in videos, as well as static images;\nLanguage Understanding Intelligent Service now supports three additional languages (Spanish, Italian and French) and hierarchical entity detection;\nNew versions of the Bing Search, Bing Image Search, Bing Video Search and Bing News Search APIs;\nNew and updated APIs including Bing Auto Suggest, Spell Check, Knowledge Exploration Service, Entity Linking API and Linguistic Analysis API.\n\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-03-30/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Making apps more human: how the Computer Vision API brings intelligence to image analysis", "Date": "Posted on June 1, 2016", "Contributor": "Julia Nikitina", "Content": "\nWhen your goal is to democratize access to artificial intelligence, how do you give developers the ability to implement intelligent image analysis in their apps? You give them an API that can tear an image apart, identify and tag more than 300 elements, and continue to learn and evolve a it's trained. That's the Computer Vision API: a machine-learning image analyzer that any dev can drop into any app with just a few lines of code. And it is the next step on a long journey to make apps behave—and respond—in a more human way.\n> Get a Computer Vision API key\nLions aren't dogs\nMicrosoft didn't begin using AI to analyze images with its Computer Vision API. There was an earlier, more limited iteration—the precursor to Computer Vision. It was part of an API suite—but more limited in scope. Microsoft developers including one of Computer Vision's architects, Cornelia Carapcea, listened to the devs who used the original API suite and decided how to evolve Computer Vision from the community's responses. For example, the original API didn't recognize lions—it thought they were dogs. So improvements needed to be made.\nCarapcea and her team have spent the last year engineering the Computer Vision API to meet that demand. But her work on the API goes beyond tweaking what the machine can identify. Apps are becoming more human, she explains, and developers want ways to allow their apps to behave more like humans would behave. To do this, you must use AI and machine learning as the backbones. \"The only way you can try to match the many layers—labels and classifications—that the human brain can accomplish is with AI,\" Carapcea says. \"It's based on deep learning and neural networks, and large volumes of data.\"\n> Read the research behind Computer Vision\n\nAn API that learns\nWhat this does, she continues, is allow the API to classify elements of an image in much the same way a human brain would. For example: a four year old might see a pony, and know it's a pony. But would that four year old see a pony and know to call it a type of horse? Would that four year old know both of those creatures are called animals? That's the level of depth and intelligence Carapcea and her team strives to provide with Computer Vision: an API that recognizes the horse, the pony, and that both are types of animals. So, what you see here is that humans seem to learn by seeing the same pattern (or image) multiple times; the machine learning behind Computer Vision does the same.\nA matter of language\nBut Computer Vision goes beyond even that—it constructs sentences based on the tags it identifies. The CaptionBot app is a basic application of what could be done with Computer Vision. \"Its applications for accessibility, the visually impaired, or bots that understand images, are endless,\" Carapcea says. \"This is cutting-edge stuff. No one else out there gives you image captioning as an API right now. If you want thousands of objects recognized, this is the API to use.\"\n> Read API documentation\n> Leave a comment on the User Voice forum\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-06-01/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Bing APIs now available for purchase on Azure", "Date": "Posted on July 1, 2016", "Contributor": "Julia Nikitina", "Content": "\nThe next version of Bing APIs are here, bringing the power, knowledge, and intelligence of the web right into your app. Harness the ability to comb billions of web pages, images, videos, news, and more with one call and a few lines of code. And starting today, you can purchase Bing APIs on Azure, so get your key today and start searching. \n \nThe following three services are now available :\n\nBing Search APIs- Making apps, webpages, and other experiences smarter and more engaging. Bing Search APIs include:\r\n\t\nBing Web Search API for getting enhanced search details from billions of web documents.\nBing Image Search API for scouring the web for images, the results including thumbnails, full image URLs, publishing website info, image metadata, and more.\nBing Video Search API to find videos across the web, the results including useful metadata - creator, encoding format, video length, view count.\nBing News Search API to find news articles, the results including authoritative image of the news article, related news and categories, provider info, article URL\n\n\nBing Autosuggest API- Empowering users to complete queries faster by adding intelligent type-ahead capabilities to their app or website. This helps users to type less and do more with automated and complete search suggestions.\nBing Spell Check API- Helping users to correct spelling errors, recognize the difference among names, brand names, and slang, as well as understand homophones as they’re typing.\n\n \nAlready using Bing APIs on the Azure Data Market? Click here to learn how to migrate to the new Bing APIs.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-07-01/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Tap into smart search with Bing APIs", "Date": "Posted on August 12, 2016", "Contributor": "Julia Nikitina", "Content": "\nMicrosoft Cognitive Services recently added a new family to its API collection: Bing APIs. These open, RESTful APIs use search algorithms trained by years of Microsoft Research development to harness the power of the web.\nWith Bing APIs, your apps can tap into billions of videos, news articles, images, and webpages with a single call. Bring smarter, more engaging experiences to your next app with just a few lines of code.\n \n\n \n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-08-12/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Cognitive Services August API Updates", "Date": "Posted on August 25, 2016", "Contributor": "Julia Nikitina", "Content": "\nMicrosoft Cognitive Services is a collection of intelligence and knowledge APIs that enable developers to make their applications more intelligent, engaging and discoverable. Cognitive Services includes intelligent APIs that allow systems to see, hear, speak, understand and interpret needs using natural methods of communication; and knowledge APIs that bring the power of the web to developers. Below are the August updates for Microsoft Cognitive Services APIs.\n \n   Computer Vision API\nNow you can tag up to 2,000 recognized objects (animate and inanimate) within an image and caption in clear, written descriptions. The update includes improved celebrity recognition.\nImproved tagging provides object recognition within images with a selection of over 2,000 objects, animals and people. Once identified, new image captioning capabilities provide clear written descriptions for the 2,000 recognizable images. Our new celebrity recognition model recognizes 200,000 celebrities from business, politics, sports, and entertainment around the world. Learn more about the Computer Vision API at Cognitive Services Computer Vision.\n \n   Face API\nThe Face API now provides additional flexibility for matching and identifying faces.\n \nNew capabilities enable face to person authentication, finding similar faces, setting recognition confidence thresholds, and new list person groups.\n\nFace-to-person authentication verifies whether two faces belong to the same person, or whether one face belongs to a single person. More details can be found at Face-Verify. \nFinding similar faces helps find faces of the same person in other lists, arrays or photos. More details can be found at Face-Find Similar. \nA new optional user-specifided confidence threshold enables users to define the level of confidence of whether a face belongs to a person. More details can be found at Face-Identify.\nThe list person groups' new \"start\" and \"top\" parameters enables users to specify the starting point and total number of person groups to return. More details can be found at Person Group-List Person Groups.\n\n \nLearn more about the Face API at Cognitive Services Face.\n \n   Video API\nYou can now specify and limit locations in a video where you want to report motion, set sensitivity levels to control granularity, and detect the exact area where motion occurred.\n \n\nPolygonal detection zones: Now you can narrow down the locations where motion is reported by specifying the areas of the video you care about.\nSet sensitivity levels: Select between three levels—low, medium, and high—to control the granularity of motion you want detected.\nMerge thresholds: Combine multiple motion events into one by specifying the timespan at which multiple reports of motion are merged into one event.\nMotion location: When motion is detected, the API defines the exact area where the motion occurred by using a rectangular bounding box.\n\n \nLearn more about the Video API at Cognitive Services Video.\n \n   Speaker Recognition API\nSpeaker Recognition only needs 30 seconds of speech to identify speakers. \n \nSpeaker Recognition previously required 60 seconds of speech for both enrollment and identifciation operations. Speaker Recognition API now accepts any length of speech, although it is recommended that users input 30 seconds for enrollment and 10 seconds for identification to get the best API experience.\n \nLearn more about the Speaker Recognition API at Cognitive Services Speaker Recognition.\n \n   Language Understanding Intelligence Service (LUIS) API\nLUIS free transactions are decreasing\n \nThe number of free monthly LUIS transactions is changing from 100,000 to 10,000 transactions effective September 4, 2016.\n \nLearn more about the Language Understanding Intelligence Service API at Cognitive Services LUIS.\n  \n   Text Analytics API\nText Analytics API now available in multiple languages.\n \nText Analytics API Sentiment analysis can now be performed in English, Spanish, French, and Portuguese; and key phrases can be extracted from text in English, Spanish, German, and Japanese. These features are in addition to the Text Analytics API’s ability to recognize the language of text in 120 languages—plus topic detection for English documents. More information on the Text Analytics API's language capabilities can be found at Text Analytics API Language Support\n \nLearn more about the Text Analytics API at Cognitive Services Text Analytics.\n \n \n   Academic Knowledge API\nYou can now make an unlimited number of transactions each month at the rate of 25 cents for every 1,000 transactions as part of a paid preview.\n \nAcademic Knowledge API users have only been able to use the free tier and make up to 10,000 transactions each month. Now with the paid tier, Academic Knowledge subscribers can make an unlimited number of transactions at the rate of $0.25 per 1000 transactions. \n \nLearn more about the Academic Knowledge API at Cognitive Services Academic Knowledge.\n \n   Recommendations API\nNew capabilities have been added to the Recommendations API.\n \nThe Recommendations API, added the following capabilities:\n\nNew Recommendations UI allows you to train and test your models without having to write a single line of code.\nNew batch scoring allows you to get thousands of recommendations at once.\nSupport for business rules\nNow you can enumerate and download usage and catalog files\nBuild metrics support to query the quality of recommendations models\nRanking build support to query the quality of item features in a recommendations model\nAbility to search for a product in the catalog\n\n \nLearn more about the Recommendations API at Cognitive Services Recommendations.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-08-25/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Analyze Live Video in Near Real Time with Microsoft Cognitive Services APIs", "Date": "Posted on September 8, 2016", "Contributor": "Allison Light", "Content": "\nMicrosoft Cognitive Services Vision APIs put state-of-the art computer vision algorithms at developers' fingertips, with APIs for analyzing individual images, and for offline video processing. We want to showcase how you can use a combination of those APIs to create a solution that can perform near-real-time analysis of frames taken from a live video stream. You could, in theory, build an app that analyzes live television events, videos of crowds, or reactions from people, or gives real-time information about what a person might be feeling.\nThis solution might be especially useful for developers looking to create an app that generates useful data from video streams. For example, a developer might want to create an app that can read the reactions of a 10a-person focus group while those people are shown a new product or browse through a website. This solution can do that in near real time.In this post, I will discuss some ways you might achieve near-real-time video analysis using APIs, and a C# library we're publishing, to make it easier to build your own solution.\n\nThe basic components in such a system are:\n\nAcquire frames from a video source.\nSelect which frames to analyze.\nSubmit these frames to the API.\nConsume each analysis result that is returned from the API call.\n\n\nIf you just want the sample code, you can find it on GitHub: https://github.com/Microsoft/Cognitive-Samples-VideoFrameAnalysis/.\nA Simple Approach\nThe simplest design for a near-real-time analysis system is an infinite loop, where in each iteration we grab a frame, analyze it, and then consume the result:\nwhile (true){\nFrame f = GrabFrame(); if (ShouldAnalyze(f)) {\nAnalysisResult r = await Analyze(f); ConsumeResult(r);\n}\n}\nIf our analysis consisted of a lightweight client-side algorithm, this approach would be suitable. However, when our analysis is happening in the cloud, the latency involved means that an API call might take several seconds, during which time we are not capturing images and our thread is essentially doing nothing. Our maximum frame rate is limited by the latency of the API calls. So we need a solution that makes the APIs work in tandem.\n\nParallelizing API Calls\nThe solution to this problem is to allow the long-running API calls to execute in parallel with the program elements that grab frames. In C#, we could achieve this goal using task-based parallelism. For example:\nwhile (true){\nFrame f = GrabFrame();if (ShouldAnalyze(f)){\nvar t = Task.Run(async () =>\n{\nAnalysisResult r = await Analyze(f);\nConsumeResult(r);\n}\n}\n}\nThis launches each analysis in a separate task, which can run in the background while we continue grabbing new frames. This solution avoids blocking the main thread while waiting for an API call to return; however we have lost some of the guarantees that the simple version provided—multiple API calls might occur in parallel, and the results might get returned in the wrong order. This could also cause multiple threads to enter the ConsumeResult() function simultaneously, which could be dangerous, if the function is not thread safe. Finally, this simple code does not keep track of the tasks that get created, so exceptions will silently disappear. Thus, the final ingredient for us to add is a \"consumer\" thread that will track the analysis tasks, raise exceptions, kill long-running tasks, and ensure that the results get consumed in the correct order, one at a time.\n\nA Producer-Consumer Design\nIn our final \"producer-consumer\" system, we have a producer thread that looks very similar to our previous infinite loop. However, instead of consuming analysis results as soon as they are available, the producer puts the tasks into a queue to keep track of them.\n// Queue that will contain the API call tasks. var taskQueue = new BlockingCollection<Task<ResultWrapper>>();\n// Producer thread. while (true){\n// Grab a frame. Frame f = GrabFrame();\n// Decide whether to analyze the frame. if (ShouldAnalyze(f)){\n// Start a task that will run in parallel with this thread.  var analysisTask = Task.Run(async () =>  {\n// Put the frame, and the result/exception into a wrapper object. var output = new ResultWrapper(f);try{\noutput.Analysis = await Analyze(f);\n} catch (Exception e){\nreturn output;\n}\n}\n// Push the task onto the queue.taskQueue.Add(analysisTask);\n}\n}\nWe also have a consumer thread that is taking tasks off the queue, waiting for them to finish, and either displaying the result or raising the exception that was thrown. By using the queue, we can guarantee that results get consumed one at a time, in the correct order, without limiting the maximum frame rate of the system.\n\nSample Implementation\nTogether with this blog post, we are making available an implementation of the system described above. It is intended to be flexible enough to implement many such scenarios while being easy to use. The library contains the class FrameGrabber, which implements the producer-consumer system discussed previously to process video frames from a webcam. The user can specify the exact form of the API call, and the class uses events to let the calling code know when a new frame is acquired or a new analysis result is available.\nTo illustrate some of the possibilities, we are also publishing two sample apps that use the library. The first is a simple console app, and a simplified version of this app is reproduced below. It grabs frames from the default webcam and submits them to the Face API for face detection.\nusing System;using VideoFrameAnalyzer;using Microsoft.ProjectOxford.Face;using Microsoft.ProjectOxford.Face.Contract;\nnamespace VideoFrameConsoleApplication{\nclass Program {\nstatic void Main(string[] args) {\n// Create grabber, with analysis type Face[].  FrameGrabber<Face[]> grabber = new FrameGrabber<Face[]>(); // Create Face API Client. Insert your Face API key here. FaceServiceClient faceClient = new FaceServiceClient(\"<subscription key>\"); // Set up our Face API call. grabber.AnalysisFunction = async frame => return await faceClient.DetectAsync(frame.Image.ToMemoryStream(\".jpg\")); // Set up a listener for when we receive a new result from an API call.  grabber.NewResultAvailable += (s, e) => {\nif (e.Analysis != null) Console.WriteLine(\"New result received for frame acquired at {0}. {1} faces detected\", e.Frame.Metadata.Timestamp, e.Analysis.Length);\n}; // Tell grabber to call the Face API every 3 seconds. grabber.TriggerAnalysisOnInterval(TimeSpan.FromMilliseconds(3000)); // Start running. grabber.StartProcessingCameraAsync().Wait(); // Wait for keypress to stop Console.WriteLine(\"Press any key to stop...\"); Console.ReadKey(); // Stop, blocking until done. grabber.StopProcessingAsync().Wait();\n}\n}\n}\nThe second sample app is a bit more interesting and allows you to choose which API to call on the video frames. On the left side, the app shows a preview of the live video; on the right side, it shows the most recent API result overlaid on the corresponding frame.\nIn most modes, there will be a visible delay between the live video on the left, and the visualized analysis on the right. This delay is the time taken to make the API call. The exception to this rule is in the \"EmotionsWithClientFaceDetect\" mode, which performs face detection locally on the client computer using OpenCV before submitting any images to Cognitive Services. By doing this, we can visualize the detected face immediately, and then update the emotions later once the API call returns. This demonstrates the possibility of a \"hybrid\" approach, where some simple processing can be performed on the client, and then Cognitive Services APIs can be used to augment this approach with more advanced analysis when necessary.\n\nI hope this post has given you a sense of the possibilities for performing near-real-time image analysis on a live video stream, and how you can use our sample code to get started. Please feel free to provide feedback and suggestions in the comments below, on our UserVoice site, or in the GitHub repository for the sample code.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2016-09-08/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Using Azure Search custom skills to create personalized job recommendations", "Date": "Posted on June 18, 2019", "Contributor": "Norah  Abokhodair", "Content": "\nUpdate: the library of technical skills mentioned in the blog is now in preview through Azure Cognitive Services Text Analytics General Named Entity Recognition (NER V3) model. We recommend using the SKILL type from the new V3 Endpoint for this workflow. Read more about the NER V3 service.\nThe Microsoft Worldwide Learning Innovation Studio is an idea incubation lab within Microsoft’s Worldwide Commercial Business (WCB) organization focuses on developing personalized learning and career experiences. One of the recent experiences that the lab developed focused on offering skills-based personalized job recommendations. Research shows that job search is one of the most stressful times in someone’s life, especially searching for the first job out of college. Everyone remembers at some point looking for their next career move and how stressful it was to find a job that aligns with their various skills.\nHarnessing Azure Cognitive Search custom skills together Azure Cognitive Services Text Analytics, we were able to build a feature that offers personalized job recommendations based on identified capabilities from resumes and job descriptions. The feature parses a resume to identify technical skills (highlighted and checkmarked in the figure below.) It then ranks jobs based on the skills most relevant to the capabilities in the resume. Another helpful ability is in the UI layout, where the user can view the gaps in their skills (non-highlighted skills in the figure below) for jobs they’re interested in and work towards building those skills.\n\nFigure one: Worldwide Learning Personalized Jobs Search Demo UI\nIn this example, our user is interested in transitioning from a Software Engineering role to Program Management. Displayed in the image above you can see how the top jobs for our user are in Program Management but they are ranked based on our user’s unique capabilities in areas like AI, Machine Learning and Cloud Computing, resulting in the top ranked job on the Bing Search and AI team which deals with all three.\nHow we used Azure Search\n\nFigure two: Worldwide Learning Personalized Jobs Search Architecture\nThe above architecture diagram shows the data flow for our application. We started with around 2000 job openings pulled directly from the Microsoft Careers website as an example. We then indexed these jobs, adding a custom Azure Search cognitive skill to extract capabilities from the descriptions of each job. This allows a user to search for a job based on a capability like “Machine Learning”. Then, when a user uploads a resume, we upload it to Azure Blob storage and run an Azure Search indexer. Leveraging a mix of cognitive skills provided by Azure and our custom skill to extract capabilities, we end up with a good representation of the user’s capabilities.\nTo personalize the job search, we leverage the tag boosting scoring profile built into Azure Search. Tag boosting ranks search results by the user’s search query and the number of matching “tags” (in this case capabilities) with the target index. So, in our example, we pass the user’s capabilities along with their search query and get jobs that best match our user’s unique set of capabilities.\nWith Azure Cognitive Search skills, our team was able to make the personalization of job search, a desirable capability among job seekers and recruiters, possible through this proof of concept. You can use the same process we followed to achieve the same goal for your own careers site. The Skills Extractor Library that we used in this example is now in preview through Azure Cognitive Services Text Analytic General Named Entity Recognition (NER V3) model. You can leverage this service through using the Text Analytics General endpoint.\nPlease be aware that before running this sample, you must have the following:\n\nInstall the Azure CLI. This article requires the Azure CLI version 2.0 or later. Run az --version to find the version you have.\nYou can also use the Azure Cloud Shell.\n\nTo learn more about this feature, you can view the live demo (starts at timecode 00:50:00) and read more in our GitHub repository.\nFeedback and support\nWe’re eager to improve, so please take a couple of minutes to answer some questions about your experience using this survey. For support requests, please contact us at WWL_Skills_Service@microsoft.com.\n", "link": "https://azure.microsoft.com/en-us/blog/using-azure-search-custom-skills-to-create-personalized-job-recommendations/", "Role": "Program Manager, Worldwide Learning Innovation Lab"},
{"Title": "Introducing next generation reading with Immersive Reader, a new Azure Cognitive Service", "Date": "Posted on June 20, 2019", "Contributor": null, "Content": "\nThis blog post was authored by Tina Coll, Senior Product Marketing Manager, Azure Marketing. \nToday, we’re unveiling the preview of Immersive Reader, a new Azure Cognitive Service in the Language category. Developers can now use this service to embed inclusive capabilities into their apps for enhancing text reading and comprehension for users regardless of age or ability. No machine learning expertise is required. Based on extensive research on inclusivity and accessibility, Immersive Reader’s features are designed to read the text aloud, translate, focus user attention, and much more. Immersive Reader helps users unlock knowledge from text and achieve gains in the classroom and office.\nOver 15 million users rely on Microsoft’s immersive reading technologies across 18 apps and platforms including Microsoft Learning Tools, Word, Outlook, and Teams. Now, developers can deliver this proven literacy-enhancing experience to their users too.\nPeople like Andrzej, a child with dyslexia, have learned to read with the Immersive Reader experience embedded into apps like Microsoft Learning Tools. His mother, Mitra, shares their story:\n\nLiteracy is key to unlocking knowledge and realizing one’s potential. Educators see this reality in the classroom every day, yet hurdles to reading are commonplace for people with dyslexia, ADHD, or visual impairment, as well as emerging readers, non-native speakers, and others. In the spirit of empowering every person to achieve more, the features of Immersive Reader help readers overcome these challenges.\n\nAzure is the only major cloud provider that offers this type experience as an easy-to-use AI service. Skooler, an ISV on a mission “to do education technology better,” integrated Immersive Reader. As Tor Henriksen, Skooler’s CEO and CTO remarks, “In 27 years of software development, this was the easiest integration we’ve ever done.” Multiple businesses to date have already started embedding Immersive Reader into their apps, including: \nWith millions of users like Andrzej having discovered the power of the written word with Immersive Reader, we look forward to seeing what people can achieve with what you build.\nTo start embedding Immersive Reader into your apps, visit the Immersive Reader product page. The service is available for free while in preview.\n", "link": "https://azure.microsoft.com/en-us/blog/introducing-next-generation-reading-with-immersive-reader-a-new-azure-cognitive-service/", "Role": null},
{"Title": "Leveraging complex data to build advanced search applications with Azure Search", "Date": "Posted on June 27, 2019", "Contributor": "Liam Cavanagh", "Content": "\nData is rarely simple. Not every piece of data we have can fit nicely into a single Excel worksheet of rows and columns. Data has many diverse relationships such as the multiple locations and phone numbers for a single customer or multiple authors and genres of a single book. Of course, relationships typically are even more complex than this, and as we start to leverage AI to understand our data the additional learnings we get only add to the complexity of relationships. For that reason, expecting customers to have to flatten the data so it can be searched and explored is often unrealistic. We heard this often and it quickly became our number one most requested Azure Search feature. Because of this we were excited to announce the general availability of complex types support in Azure Search. In this post, I want to take some time to explain what complex types adds to Azure Search and the kinds of things you can build using this capability. \nAzure Search is a platform as a service that helps developers create their own cloud search solutions.\nWhat is complex data?\nComplex data consists of data that includes hierarchical or nested substructures that do not break down neatly into a tabular rowset. For example a book with multiple authors, where each author can have multiple attributes, can’t be represented as a single row of data unless there is a way to model the authors as a collection of objects. Complex types provide this capability, and they can be used when the data cannot be modeled in simple field structures such as strings or integers.\nComplex types applicability\nAt Microsoft Build 2019,  we demonstrated how complex types could be leveraged to build out an effective search application. In the session we looked at the Travel Stack Exchange site, one of the many online communities supported by StackExchange.\nThe StackExchange data was modeled in a JSON structure to allow easy ingestion it into Azure Search. If we look at the first post made to this site and focus on the first few fields, we see that all of them can be modeled using simple datatypes, including tags which can be modeled as a collection, or array of strings.\n\r\n{\r\n   \"id\": \"1\",\r\n    \"CreationDate\": \"2011-06-21T20:19:34.73\",\r\n    \"Score\": 8,\r\n    \"ViewCount\": 462,\r\n    \"BodyHTML\": \"<p>My fiancée and I are looking for a good Caribbean cruise in October and were wondering which\r\n    \"Body\": \"my fiancée and i are looking for a good caribbean cruise in october and were wondering which islands\r\n    \"OwnerUserId\": 9,\r\n    \"LastEditorUserId\": 101,\r\n    \"LastEditDate\": \"2011-12-28T21:36:43.91\",\r\n    \"LastActivityDate\": \"2012-05-24T14:52:14.76\",\r\n    \"Title\": \"What are some Caribbean cruises for October?\",\r\n    \"Tags\": [\r\n        \"caribbean\",\r\n        \"cruising\",\r\n        \"vacations\"\r\n    ],\r\n    \"AnswerCount\": 4,\r\n    \"CommentCount\": 4,\r\n    \"CloseDate\": \"0001-01-01T00:00:00\",​\nHowever, as we look further down this dataset we see that the data quickly gets more complex and cannot be mapped into a flat structure. For example, there can be numerous comments and answers associated with a single document.  Even votes is defined here as a complex type (although technically it could have been flattened, but that would add work to transform the data).\n\r\n\"CloseDate\": \"0001-01-01T00:00:00\",\r\n    \"Comments\": [\r\n        {\r\n            \"Score\": 0,\r\n            \"Text\": \"To help with the cruise line question: Where are you located? My wife and I live in New Orlea\r\n            \"CreationDate\": \"2011-06-21T20:25:14.257\",\r\n           \"UserId\": 12\r\n        },\r\n        {\r\n            \"Score\": 0,\r\n            \"Text\": \"Toronto, Ontario. We can fly out of anywhere though.\",\r\n            \"CreationDate\": \"2011-06-21T20:27:35.3\",\r\n            \"UserId\": 9\r\n        },\r\n        {\r\n            \"Score\": 3,\r\n            \"Text\": \"\\\"Best\\\" for what?  Please read [this page](\r\n            \"UserId\": 20\r\n        },\r\n        {\r\n            \"Score\": 2,\r\n            \"Text\": \"What do you want out of a cruise? To relax on a boat? To visit islands? Culture? Adventure?\r\n            \"CreationDate\": \"2011-06-24T05:07:16.643\",\r\n            \"UserId\": 65\r\n        }\r\n    ],\r\n    \"Votes\": {\r\n        \"UpVotes\": 10,\r\n        \"DownVotes\": 2\r\n    },\r\n    \"Answers\": [\r\n        {\r\n            \"IsAcceptedAnswer\": \"True\",\r\n            \"Body\": \"This is less than an answer, but more than a comment…\\n\\nA large percentage of your travel b\r\n            \"Score\": 7,\r\n            \"CreationDate\": \"2011-06-24T05:12:01.133\",\r\n            \"OwnerUserId\": 74\nAll of this data is important to the search experience. For example, you might want to:\n\nSearch for and highlight phrases not only in the original question, but also in any of the comments.\nLimit documents to those where an answer was provided by a specific user.\nBoost certain documents higher in the search results when they have a higher number of up votes.\n\nIn fact, we could even improve on the existing StackExchange search interface by leveraging Cognitive Search to extract key phrases from the answers to supply potential phrases for autocomplete as the user types in the search box.\nAll of this is now possible because not only can you map this data to a complex structure, but the search queries can support this enhanced structure to help build out a better search experience.\nNext Steps\nIf you would like to learn more about Azure Search complex types, please visit the documentation, or check out the video and associated code I made which digs into this Travel StackExchange data in more detail.\n", "link": "https://azure.microsoft.com/en-us/blog/leveraging-complex-data-to-build-advanced-search-applications-with-azure-search/", "Role": "Principal Program Manager, Azure Search"},
{"Title": "Enable receipt understanding with Form Recognizer’s new capability", "Date": "Posted on July 8, 2019", "Contributor": "Nikhil Kamath", "Content": "\nOne of the newest members of the Azure AI portfolio, Form Recognizer, applies advanced machine learning to accurately extract text, key-value pairs, and tables from documents. With just a few samples, it tailors its understanding to supplied documents, both on-premises and in the cloud. \nIntroducing the new pre-built receipt capability\nForm Recognizer focuses on making it simpler for companies to utilize the information hiding latent in business documents such as forms. Now we are making it easier to handle one the most commonplace documents in a business, receipts, “out of the box.”  Form Recognizer’s new pre-built receipt API identifies and extracts key information on sales receipts, such as the time and date of the transaction, merchant information, the amounts of taxes, totals, and more, with no training required.\n\nStreamlining expense reporting\nBusiness expense reporting can be cumbersome for everyone involved in the process. Manually filling out and approving expense reports is a significant time sink for both employees and managers. Aside from productivity lost to expense reporting, there are also pain points around auditing expense reports. A solution to automatically extract merchant and transaction information from receipts can significantly reduce the manual effort of reporting and auditing expenses.\nGiven the proliferation of mobile cameras, modern expense reports often contain images of receipts that are faded, crumpled up, or taken in suboptimal lighting conditions. Existing receipt solutions often target high quality scanned images and are not robust enough to handle such real-world conditions.\nEnhance your expense reporting process using the pre-built capability\nForm Recognizer eases common pain points in expense reporting, delivering real value back to business. By using the receipt API to extract merchant and transaction information from receipts, developers can unlock new experiences in the workforce. And since the pre-built model for receipts works off the shelf without training, it reduces the speed to deployment.\nFor employees, expense applications leveraging Form Recognizer can pre-populate expense reports with key information extracted from receipts. This saves employees time in managing expenses and travel that they can focus on their core roles. For central teams like finance within a company, it also helps expense auditing by using the key data extracted from receipts for verification. The optical character recognition (OCR) technology behind the service can handle receipts that are captured in a wide variety of conditions,  including smartphone cameras, reducing the amount of manual searching and reading of transaction documents required by auditors.\nOur customer: Microsoft’s internal finance operations\nThe pre-built receipt functionality of Form Recognizer has already been deployed by Microsoft’s internal expense reporting tool, MSExpense, to help auditors identify potential anomalies. Using the data extracted, receipts are sorted into low, medium, or high risk of potential anomalies. This enables the auditing team to focus on high risk receipts and reduce the number of potential anomalies that go unchecked.\nMSExpense also plans to leverage receipt data extraction and risk scoring to modernize the expense reporting process. Instead of identifying risky expenses during auditing, such automated processing can flag potential issues earlier in the process during the reporting or approval of the expenses. This reduces the turnaround time for processing the expense and any reimbursement.\n“The pre-built receipt feature of Form Recognizer enables our application not only to scale from sampling 5 percent of receipts to 100 percent, but more importantly to streamline employee expense report experience by auto-populating/creating expense transactions, creating happy path to payment, receipt data insights to approver managers, giving employees time back to be used in value-add activities for our company. The service was simple to integrate and start seeing value.“ \n—Luciana Siciliano, Microsoft FinOps (MSExpense)\nLearn more\nTo learn more about Form Recognizer and the rest of the Azure AI ecosystem, please visit our website and read the documentation.\nGet started by contacting us.\nFor additional questions please reach out to us at formrecog_contact@microsoft.com\n", "link": "https://azure.microsoft.com/en-us/blog/enable-receipt-understanding-with-form-recognizer-s-new-capability/", "Role": "Program Manager, Microsoft Azure"},
{"Title": "Conversational AI updates for July 2019", "Date": "Posted on July 18, 2019", "Contributor": "Yochay Kiriaty", "Content": "\nAt Build, we highlighted a few customers who are building conversational experiences using the Bot Framework to transform their customer experiences. For example, BMW discussed its work on the BMW Intelligent Personal Assistant to deliver conversational experiences across multiple canvases by leveraging the Bot Framework and Cognitive Services. LaLiga built their own virtual assistant which allows fans to experience and interact with LaLiga across multiple platforms.\nWith the Bot Framework release in July, we are happy to share new releases of Bot Framework SDK 4.5 and preview of 4.6, updates to our developer tools, and new channels in Azure Bot Service. We’ll use the opportunity to provide additional updates for the Conversational AI releases from Microsoft.\nBot Framework channels\nWe continue to expend channels support and functionality for Bot Framework and Azure Bot Service.\nVoice-first bot applications: Direct Line Speech preview\nThe Microsoft Bot Framework lets you connect with your users wherever your users are. We offer thirteen supported channels, including popular messaging apps like Skype, Microsoft Teams, Slack, Facebook Messenger, Telegram, Kik, as well as a growing number of community adapters.\nToday, we are happy to share the preview of Direct Line Speech channel. This is a new channel designed for voice-first experiences for your Bot Framework utilizing Microsoft’s Speech Services technologies.  he Direct Line Speech channel is a native implementation of speech for mobile applications and IoT devices, with support for Text-to-speech, Speech-to-text, and custom wake words.  We’re happy to share that we’re now opening the preview to all Bot Framework customers.\nGetting started with voice support to your bot is easy. Simply update to the latest Bot Framework SDK, configure the Direct Line Speech channel for your bot, and use the Speech SDK to embed voice into your mobile application or device today.\n\nBetter isolation for your bot: Direct Line App Service Extension\nDirect Line and Webchat are used broadly by Bot Framework customers to provide chat experiences on their web pages, mobile apps, and devices. For some scenarios, customers have given us the feedback that they’d like a version of Direct Line that can be deployed in isolation, such as in a Virtual Network (VNET). A VNET lets you create your own private space in Azure and is crucial to your cloud network as it offers isolation, segmentation, and other key benefits. The Direct Line App Service Extension can be deployed as part of a VNET, allowing IT administrators to have more control over conversation traffic and improve latency in conversations due to reduction in the number of hops. Feel free to get started with Direct Line App Service Extension.\nBot Framework SDK\nAs part of the Bot Framework SDK 4.6 preview we updated Adaptive Dialog, which allows developers to dynamically update conversation flow based on context and events. This is especially handy when dealing with conversation context switches and interruptions in the middle of a conversation. Learn more by reading the documentation and reviewing the samples.\nContinuing our commitment to the Open Source community and following on our promise to allow developers to use their favorite programing language, we updated Bot Framework Python SDK. The Python SDK now supports OAuth, Prompts, CosmosDB, and includes all major functionality in SDK 4.5. In addition we got new samples.\nAddressing customers’ and developers’ ask for better testing tools, the July version of the SDK introduces a new unit testing capability. The Microsoft.Bot.Builder.testing package simplifies the process of unit testing dialogs in your bot. Check out the documentation and samples.\nIntroduced at Microsoft Build 2019, the Bot Inspector is a new feature in the Bot Framework Emulator which lets you debug and test bots on channels like Microsoft Teams, Slack, Cortana, and more. As you use the bot on specific channels, messages will be mirrored to the Bot Framework Emulator where you can inspect the message data that the bot received. Additionally, a snapshot of the bot memory state for any given turn between the channel and the bot is rendered as well.\nFollowing enterprise customers asks, we put together a web chat sample for a single sign-on to enterprise apps using OAuth. In this sample, we show how to authorize a user to access resources on an enterprise app with a bot. Two types of resources are used to demonstrate the interoperability of OAuth, Microsoft Graph and GitHub API.\nSolutions\nVirtual agent solution accelerator\nWe updated the Virtual Assistant and associated skills to enable out-of-box support for Direct Line Speech opening voice assistant experiences with no additional steps. This includes middleware to enable control of the voice being used. Once a new Virtual Assistant has been deployed, you can follow instructions for configuring Virtual Assistant with the Direct Line Speech channel. The example test harness application is also provided to enable you to quickly and easily test Speech scenarios.\nAn Android app client for Virtual Assistant is also available which integrates with Direct Line Speech and Virtual Assistant, demonstrating how a device client can interact with your Virtual Assistant and render Adaptive Cards.\nIn addition, we have added out-of-box support for Microsoft Teams ensuring that your Virtual Assistant and skills work including authentication and adaptive cards. You can follow steps for creating the associated application manifest.\nThe Virtual Assistant Solution Accelerator provides a set of templates, solution accelerators, and skills to help build sophisticated conversational experiences. A new Android app client for Virtual Assistant that integrates with Direct Line Speech and Virtual Assistant demonstrates how a device client can interact with your Virtual Assistant and render adaptive cards. Updates also include support for Direct-Line Speech and Microsoft Teams.\nThe Dynamics 365 Virtual Agent for Customer Service preview provides exceptional customer service with intelligent, adaptable virtual agents. Customer service experts can easily create and enhance bots with AI-driven insights. The Dynamic 365 Virtual Agent is built on top of the Bot Framework and Azure.\n", "link": "https://azure.microsoft.com/en-us/blog/conversational-ai-updates-for-july-2019/", "Role": "Principal Program Manager, Azure Platform"},
{"Title": "New ways to train custom language models – effortlessly!", "Date": "Posted on July 18, 2019", "Contributor": "Haim Sabo", "Content": "\nVideo Indexer (VI), the AI service for Azure Media Services enables the customization of language models by allowing customers to upload examples of sentences or words belonging to the vocabulary of their specific use case. Since speech recognition can sometimes be tricky, VI enables you to train and adapt the models for your specific domain. Harnessing this capability allows organizations to improve the accuracy of the Video Indexer generated transcriptions in their accounts.\nOver the past few months, we have worked on a series of enhancements to make this customization process even more effective and easy to accomplish. Enhancements include automatically capturing any transcript edits done manually or via API as well as allowing customers to add closed caption files to further train their custom language models.\nThe idea behind these additions is to create a feedback loop where organizations begin with a base out-of-the-box language model and improve its accuracy gradually through manual edits and other resources over a period of time, resulting with a model that is fine-tuned to their needs with minimal effort.\nAccounts’ custom language models and all the enhancements this blog shares are private and are not shared between accounts.\nIn the following sections I will drill down on the different ways that this can be done.\nImproving your custom language model using transcript updates\nOnce a video is indexed in VI, customers can use the Video Indexer portal to introduce manual edits and fixes to the automatic transcription of the video. This can be done by clicking on the Edit button at the top right corner of the Timeline pane of a video to move to edit mode, and then simply update the text, as seen in the image below.\n \n\nThe changes are reflected in the transcript, captured in a text file From transcript edits, and automatically inserted to the language model used to index the video. If you were not already using a customer language model, the updates will be added to a new Account Adaptations language model created in the account.\nYou can manage the language models in your account and see the From transcript edits files by going to the Language tab in the content model customization page of the VI website.\nOnce one of the From transcript edits files is opened, you can review the old and new sentences created by the manual updates, and the differences between them as shown below.\n\nAll that is left is to do is click on Train to update the language model with the latest changes. From that point on, these changes will be reflected in all future videos indexed using that model. Of course, you do not have to use the portal to train the model, the same can be done via the Video Indexer train language model API. Using the API can open new possibilities such as allowing you to automate a recurring training process to leverage ongoing updates.\n\nThere is also an update video transcript API that allows customers to update the entire transcript of a video in their account by uploading a VTT file that includes the updates. As a part of the new enhancements, when a customer uses this API, Video Indexer also adds the transcript that the customers uploaded to the relevant custom model automatically in order to leverage the content as training material. For example, calling update video transcript for a video titled \"Godfather\" will result with a new transcript file named “Godfather” in the custom language model that was used to index that video.\nImproving your custom language model using closed caption files\nAnother quick and effective way to train your custom language model is to leverage existing closed captions files as training material. This can be done manually, by uploading a new closed caption file to an existing model in the portal, as shown in the image below, or by using the create language model and update language model APIs to upload a VTT, SRT or TTML files (similarly to what was done until now with TXT files.)\n \n\nOnce uploaded, VI cleans up all the metadata in the file and strip it down to the text itself. You can see the before and after results in the following table.\n \n\n\n\nType\nBefore\nAfter\n\n\nVTT\n\nNOTE Confidence: 0.891635\r\n   00:00:02.620 --> 00:00:05.080\r\n   but you don't like meetings before 10 AM.\n\nbut you don’t like meetings before 10 AM.\n\n\nSRT\n\n2\r\n   00:00:02,620 --> 00:00:05,080\r\n   but you don't like meetings before 10 AM.\n\nbut you don’t like meetings before 10 AM.\n\n\nTTML\n\n<!-- Confidence: 0.891635 -->\r\n   <p begin=\"00:00:02.620\" end=\"00:00:05.080\">but you don't like meetings before 10 AM.</p>\n\nbut you don’t like meetings before 10 AM.\n\n\n\nFrom that point on, all that is left to do is review the additions to the model and click Train or use the train language model API to update the model.\nNext Steps\nThe new additions to the custom language models training flow make it easy for you and your organization to get more accurate transcription results easily and effortlessly. Now, it is up to you to add data to your custom language models, using any of the ways we have just discussed, to get more accurate results for your specific content next time you index your videos.\nHave questions or feedback? We would love to hear from you! Use our UserVoice page to help us prioritize features, or email VISupport@Microsoft.com for any questions.\n", "link": "https://azure.microsoft.com/en-us/blog/new-ways-to-train-custom-language-models-effortlessly/", "Role": "Software Engineer, Azure Storage Media R&D"},
{"Title": "Beyond the printed form: Unlocking insights from documents with Form Recognizer", "Date": "Posted on September 3, 2019", "Contributor": "Neta Haiby", "Content": "\nData extraction from printed forms is by now a tried and true technology. Form Recognizer extracts key value pairs, tables and text from documents such as W2 tax statements, oil and gas drilling well reports, completion reports, invoices, and purchase orders. However, real-world businesses often rely on a variety of documents for their day-to-day needs that are not always cleanly printed.\nWe are excited to announce the addition of handwritten and mixed-mode (printed and handwritten) support. Starting now, handling handwritten and mixed-mode forms is the new norm.\nExtracting data from handwritten and mixed-mode content with Form Recognizer\nEntire data sets that were inaccessible in the past due to the limitations of extraction technology now become available. The handwritten and mixed-mode capability of Form Recognizer is available in preview and enables you to extract structured data out of handwritten text filled in forms such as:\n\nMedical forms: New patient information, doctor notes.\nFinancial forms: Account opening forms, credit card applications.\nInsurance: Claim forms, liability forms.\nManufacturing forms: Packaging slips, testing forms, quality forms.\nAnd more.\n\nBy using our vast experience in optical character recognition (OCR) and machine learning for form analysis, our experts created a state-of-the-art solution that goes beyond printed forms. The OCR technology behind the service supports both handwritten and printed. Expanding the scope of Form Recognizer allows you to tap into previously uncharted territories, by making new sources of data available to you. You may extract valuable business information from newly available data, keeping you ahead of your competition.\nWhether you are using Form Recognizer for the first time or already integrated it into your organization, you will now have an opportunity to create new business applications:\n\nExpand your available data set: If you are only extracting data from machine printed forms, expand your total data set to mixed-mode forms and historic handwritten forms.\nCreate one application for a mix of documents: If you use a mix of handwritten and printed forms, you can create one application that applies across all your data.\nAvoid manual digitization of handwritten forms: Original forms may be fed to Form Recognizer without any pre-processing, extracting the same key-value pairs and table data you would get from a machine-printed form to reduce costs, errors, and time.\n\nOur customer: Avanade\nAvanade values people as their most important asset. They are always on the lookout for talented and passionate professionals to grow their organization. One way they find these people is by attending external events, which may include university career fairs, trade shows, or technical conferences to name a few. \nDuring these events they often take the details of those interested in finding out more about Avanade, as well as their permission to contact them at a later date. Normally this is completed with a digital form using a set of tablets. But when the stand is particularly busy, they use a short paper form that attendees can fill in with their handwritten details. Unfortunately, these forms needed to be manually entered into the marketing database, requiring a considerable amount of time and resources. With the volume of potential new contacts at these events, multiplied by the number of events Avanade attends, this task can be daunting.\nAzure Form Recognizer’s new handwritten support simplifies the process, giving Avanade peace of mind knowing no contact is lost and the information is there for them immediately.\nIn addition, Avanade integrated Form Recognizer as a skill within their cognitive search solution, enabling them to quickly use the service in their existing platform and follow-up with new leads, while their competitors may be spending time digitizing their handwritten forms.\n\n“Azure Form Recognizer takes a vast amount of effort out of the process, changing the task from data entry to data validation. By integrating Form Recognizer with Azure Search, we are also immediately able to use the service in our existing platforms. If we need to find and check a form for any reason, for example to check for a valid signature there, we can simply search by any of the fields like name or job title and jump straight to that form. In our initial tests, using Form Recognizer has reduced the time taken to digitize the forms and double check the entries by 35 percent, a number we only expect to get better as we work to optimize our tools to work hand in hand with the service, and add in more automation.” - Fergus Kidd, Emerging Technology Engineer, Avanade\nGetting started\nTo learn more about Form Recognizer and the rest of the Azure AI ecosystem, please visit our website and read the documentation.\nGet started by contacting us.\nFor additional questions please reach out to us at formrecog_contact@microsoft.com\n", "link": "https://azure.microsoft.com/en-us/blog/beyond-the-printed-form-unlocking-insights-from-documents-with-form-recognizer/", "Role": "Principal Program Manager, Azure AI"},
{"Title": "Leveraging Cognitive Services to simplify inventory tracking", "Date": "Posted on October 10, 2019", "Contributor": "Matthew Calder", "Content": "\n\r\nWho spends their summer at the Microsoft Garage New England Research & Development Center (or “NERD”)? The Microsoft Garage internship seeks out students who are hungry to learn, not afraid to try new things, and able to step out of their comfort zones when faced with ambiguous situations. The program brought together Grace Hsu from Massachusetts Institute of Technology, Christopher Bunn from Northeastern University, Joseph Lai from Boston University, and Ashley Hong from Carnegie Mellon University. They chose the Garage internship because of the product focus—getting to see the whole development cycle from ideation to shipping—and learning how to be customer obsessed.\nMicrosoft Garage interns take on experimental projects in order to build their creativity and product development skills through hacking new technology. Typically, these projects are proposals that come from our internal product groups at Microsoft, but when Stanley Black & Decker asked if Microsoft could apply image recognition for asset management on construction sites, this team of four interns accepted the challenge of creating a working prototype in twelve weeks.\nStarting with a simple request for leveraging image recognition, the team conducted market analysis and user research to ensure the product would stand out and prove useful. They spent the summer gaining experience in mobile app development and AI to create an app that recognizes tools at least as accurately as humans can.\nThe problem\nIn the construction industry, it’s not unusual for contractors to spend over 50 hours every month tracking inventory, which can lead to unnecessary delays, overstocking, and missing tools. All together, large construction sites could lose more than $200,000 worth of equipment over the course of a long project. Addressing this problem is an unstandardized mix that typically involves barcodes, Bluetooth, RFID tags, and QR codes. The team at Stanley Black & Decker asked, “wouldn’t it be easier to just take a photo and have the tool automatically recognized?”\nBecause there are many tool models with minute differences, recognizing a specific drill, for example, requires you to read a model number like DCD996. Tools can also be assembled with multiple configurations, such as with or without a bit or battery pack attached, and can be viewed from different angles. You also need to take into consideration the number of lighting conditions and possible backgrounds you’d come across on a typical construction site. It quickly becomes a very interesting problem to solve using computer vision.\n\r\n \nHow they hacked it\nClassification algorithms can be easily trained to reach strong accuracy when identifying distinct objects, like differentiating between a drill, a saw, and a tape measure. Instead, they wanted to know if a classifier could accurately distinguish between very similar tools like the four drills shown above. In the first iteration of the project, the team explored PyTorch and Microsoft’s Custom Vision service. Custom Vision appeals to users by not requiring a high level of data science knowledge to get a working model off the ground, and with enough images (roughly 400 for each tool), Custom Vision proved to be an adequate solution. However, it immediately became apparent that manually gathering this many images would be challenging to scale for a product line with thousands of tools. The focus quickly shifted to find ways of synthetically generating the training images.\nFor their initial approach, the team did both three-dimensional scans and green screen renderings of the tools. These images were then overlaid with random backgrounds to mimic a real photograph. While this approach seemed promising, the quality of the images produced proved challenging.\nIn the next iteration, in collaboration with Stanley Black & Decker’s engineering team, the team explored a new approach using photo-realistic renders from computer-aided design (CAD) models. They were able to use relatively simple Python scripts to resize, rotate, and randomly overlay these images on a large set of backgrounds. With this technique, the team could generate thousands of training images within minutes.\n\r\n    \nOn the left is an image generated in front of a green screen versus an extract from CAD on the right.\nBenchmarking the iterations\nThe Custom Vision service offers reports on the accuracy of the model as shown below.\n\r\nFor a classification model that targets visually similar products, a confusion matrix like the one below is very helpful. A confusion matrix visualizes the performance of a prediction model by comparing the true label of a class in the rows with the label outputted by the model in the columns. The higher the scores on the diagonal, the more accurate the model is. When high values are off the diagonal it helps the data scientists understand which two classes are being confused with each other by the trained model.\nExisting Python libraries can be used to quickly generate a confusion matrix with a set of test images.\n \nThe result\nThe team developed a React Native application that runs on both iOS and Android and serves as a lightweight asset management tool with a clean and intuitive UI. The app adapts to various degrees of Wi-Fi availability and when a reliable connection is present, the images taken are sent to the APIs of the trained Custom Vision model on Azure Cloud. In the absence of an internet connection, the images are sent to a local computer vision model.\nThese local models can be obtained using Custom Vision, which exports models to Core ML for iOS, TensorFlow for Android, or as a Docker container that can run on a Linux App Service in Azure. An easy framework for the addition of new products to the machine learning model can be implemented by exporting rendered images from CAD and generating synthetic images.\n\r\n  \r\nImages in order from left to right: inventory checklist screen, camera functionality to send a picture to Custom Vision service, display of machine learning model results, and a manual form to add a tool to the checklist.\n\nWhat’s next\nLooking for an opportunity for your team to hack on a computer vision project? Search for an OpenHack near you.\nMicrosoft OpenHack is a developer focused event where a wide variety of participants (Open) learn through hands-on experimentation (Hack) using challenges based on real world customer engagements designed to mimic the developer journey. OpenHack is a premium Microsoft event that provides a unique upskilling experience for customers and partners. Rather than traditional presentation-based conferences, OpenHack offers a unique hands-on coding experience for developers.\nThe learning paths can also help you get hands on with the cognitive services.\n", "link": "https://azure.microsoft.com/en-us/blog/leveraging-cognitive-services-to-simplify-inventory-tracking/", "Role": "Sr. Content Developer"},
{"Title": "Start building with Azure Cognitive Services for free", "Date": "Posted on October 23, 2019", "Contributor": "Naomi Lim", "Content": "\nThis post was co-authored by Tina Coll, Sr Product Marketing Manager, Azure Cognitive Services.\nInnovate at no cost to you, with out-of-the box AI services that are newly available for Azure free account users. Join the 1.3 million developers who have been using Cognitive Services to build AI powered apps to date. With the broadest offering of AI services in the market, Azure Cognitive Services can unlock AI for more scenarios than other cloud providers. Give your apps, websites, and bots the ability to see, understand, and interpret people’s needs — all it takes is an API call — by using natural methods of communication. Businesses in various industries have transformed how they operate using the very same Cognitive Services now available to you with an Azure free account.\nGet started with an Azure free account today, and learn more about Cognitive Services.\nThese examples are just a small handful of what you can make possible with these services:\n\nImprove app security with face detection: With Face API, detect and compare human faces. See how Uber uses Face API to authenticate drivers.\nAutomatically extract text and detect languages: Easily and accurately detect the language of any text string, simplifying development processes and allowing you to quickly translate and serve localized content. Learn how Chevron applied Form Recognizer for robotic process automation, quickly extracting text from documents.\nPersonalize your business’ homepage: Use Personalizer to deliver the most relevant content and experiences to each user on your homepage.\nDevelop your own computer vision model in minutes: Use your own images to teach Custom Vision Service the concepts you want it to learn and build your own model. Find out how Minsur, the largest tin mine in the western hemisphere, harnesses Custom Vision for sustainable mining practices.\nCreate inclusive apps: With Computer Vision and Immersive Reader, your camera becomes an inclusive tool that turns pictures into spoken words for low vision users.\nBuild conversational experiences for your customers: Give your bot the ability to interact with your users with Azure Cognitive Services. See how LaLiga, the Spanish men’s soccer league, engages hundreds of millions of fans with its chatbot using LUIS, QnAMaker, and more.\n\nIt’s easy to get started\n1. Create an Azure free account.\n2. Visit the Azure portal to deploy services.\n3. Find step-by-step guidance for deploying Cognitive Services.\n", "link": "https://azure.microsoft.com/en-us/blog/start-building-with-azure-cognitive-services-for-free/", "Role": "Azure Product Marketing Manager"},
{"Title": "Multi-language identification and transcription in Video Indexer", "Date": "Posted on November 25, 2019", "Contributor": "Abed Asi", "Content": "\nMulti-language speech transcription was recently introduced into Microsoft Video Indexer at the International Broadcasters Conference (IBC). It is available as a preview capability and customers can already start experiencing it in our portal. More details on all our IBC2019 enhancements can be found here.\nMulti-language videos are common media assets in the globalization context, global political summits, economic forums, and sport press conferences are examples of venues where speakers use their native language to convey their own statements. Those videos pose a unique challenge for companies that need to provide automatic transcription for video archives of large volumes. Automatic transcription technologies expect users to explicitly determine the video language in advance to convert speech to text. This manual step becomes a scalability obstacle when transcribing multi-language content as one would have to manually tag audio segments with the appropriate language.\nMicrosoft Video Indexer provides a unique capability of automatic spoken language identification for multi-language content. This solution allows users to easily transcribe multi-language content without going through tedious manual preparation steps before triggering it. By that, it can save anyone with large archive of videos both time and money, and enable discoverability and accessibility scenarios.\nMulti-language audio transcription in Video Indexer\nThe multi-language transcription capability is available as part of the Video Indexer portal. Currently, it supports four languages including English, French, German and Spanish, while expecting up to three different languages in an input media asset. While uploading a new media asset you can select the “Auto-detect multi-language” option as shown below.\n\nOur application programming interface (API) supports this capability as well by enabling users to specify 'multi' as the language in the upload API. Once the indexing process is completed, the index JavaScript object notation (JSON) will include the underlying languages. Refer to our documentation for more details.\nAdditionally, each instance in the transcription section will include the language in which it was transcribed.\n\nCustomers can view the transcript and identified languages by time, jump to the specific places in the video for each language, and even see the multi-language transcription as video captions. The result transcription is also available as closed caption files (VTT, TTML, SRT, TXT, and CSV).\n\nMethodology\nLanguage identification from an audio signal is a complex task. Acoustic environment, speaker gender, and speaker age are among a variety of factors that affect this process. We represent audio signal using a visual representation, such as spectrograms, assuming that, different languages induce unique visual patterns which can be learned using deep neural networks.\nOur solution has two main stages to determine the languages used in multi-language media content. First, it employs a deep neural network to classify audio segments with very high granularity, in other words, very few seconds. While a good model will successfully identify the underlying language, it can still miss-identify some segments due to similarities between languages. Therefore, we apply a second stage for examining these misses and smooth the results accordingly.\n\nNext steps\nWe introduced a differentiated capability for multi-language speech transcription. With this unique capability in Video Indexer, you can become more effective about the content of your videos as it allows you to immediately start searching across videos for different language segments. During the coming few months, we will be improving this capability by adding support for more languages and improving the model’s accuracy.\nFor more information, visit Video Indexer’s portal or the Video Indexer developer portal, and try this new capability. Read more about the new multi-language option and how to use it in our documentation.\nPlease use our UserVoice to share feedback and help us prioritize features or email visupport@microsoft.com with any questions.\n", "link": "https://azure.microsoft.com/en-us/blog/multi-language-identification-and-transcription-in-video-indexer/", "Role": "Senior Applied Researcher, Azure Media Services"},
{"Title": "Azure Media Services: The latest Video Indexer updates from NAB Show 2019", "Date": "Posted on April 4, 2019", "Contributor": "Ella Ben-Tov", "Content": "\nUpdated on April 16, 2019: We are thrilled to announce that Video Indexer’s new AI Editor won a NAB Show Product of the Year Award in the AI/ML category at this year’s event! This prestigious award recognizes “the most significant and promising new products and technologies” exhibited at NAB each year.\nAfter sweeping up multiple awards with the general availability release of Azure Media Services’ Video Indexer, including the 2018 IABM for innovation in content management and the prestigious Peter Wayne award, the team has remained focused on building a wealth of new features and models to allow any organization with a large archive of media content to unlock insights from their content; and use those insights improve searchability, enable new user scenarios and accessibility, and open new monetization opportunities.\nAt NAB Show 2019, we are proud to announce a wealth of new enhancements to Video Indexer’s models and experiences that will be rolled out this week, including:\n\nA new AI-based editor that allows you to create new content from existing media within minutes\nEnhancements to our custom people recognition, including central management of models and the ability to train models from images\nLanguage model training based on transcript edits, allowing you to effectively improve your language model to include your industry-specific terms\nNew scene segmentation model (preview)\nNew ending rolling credits detection models\nAvailability in 9 different regions worldwide\nISO 27001, ISO 27018, SOC 1,2,3, HiTRUST, FedRAMP, HIPAA, and PCI certifications\nAbility to take your data and trained models with you when moving from trial to paid Video Indexer account\n\nMore about all of those great additions in this blog.\nIn addition, we have exciting news for customers who are using our live streaming platform for ingesting live feeds, transcoding, and dynamically packaging and encrypting it for delivery via industry-standard protocols like HLS and MPEG-DASH. Live transcriptions is a new feature in our v3 APIs, wherein you can enhance the streams delivered to your viewers with machine-generate text that is transcribed from spoken words in the video stream. This text will initially be only delivered as IMSC1.1 compatible TTML packaged in MPEG-4 Part 30 (ISO/IEC 14496-30) fragments, which can be played back via a new build of Azure Media Player. More information on this feature, and the private preview program, is available in the documentation, “Live transcription with Azure Media Services v3.”\nWe are also announcing two more private preview programs for multi-language transcription and animation detection, where selected customers will be able to influence the models and experiences around them. Come talk to us at NAB Show or contact your account manager to request to be added to these exciting programs!\nExtracting fresh content from your media archive has never been easier\nOne of the ways to use deep insights from media files is to create new media from existing content. This can be to create movie highlights for trailers, use old clips of videos in news casts, create shorter content for social media, or for any other business need.\nIn order to facilitate this scenario with just a few clicks, we created an AI-based editor that enables you to find the right media content, locate the parts that you’re interested in, and use those to create an entirely new video, using the metadata generated by Video Indexer. Once you’re happy with the result, it can be rendered and downloaded from Video Indexer and used in your own editing applications or downstream workflows.\n\nAll these capabilities are also available through our updated REST API. This means that you can write code that creates clips automatically based on insights. The new editor API calls are currently open to public preview.\nWant to give the new AI-based editor a try? Simply go to one of your indexed media files and click the “Open in editor” button to start creating new content.\nMore intuitive model customization and management\nVideo Indexer comes with a rich set of out-of-the-box models so you can upload your content and get insights immediately. However, AI technology always gets more accurate when you customize it to the specific content it’s employed for. Therefore, Video Indexer provides simple customization capabilities for selected models. One such customization is the ability to add custom persons models to the over 1 million celebrities that Video Indexer can currently identify out-of-the-box. This customization capability already existed in the form of training “unknown” people in the content of a video, but we received multiple customer requests to enhance it even more - so we did!\nTo accommodate an easy customization process for persons models, we added a central people recognition management page that allows you to create multiple custom persons models per account, each of which can hold up to 1 million different entries. From this location you can create new models, add new people to existing models, review, rename, and delete them if needed. On top of that, you can now train models based on your static images even before you have uploaded the first video to your account. Organizations that already have an archive of people images can now use those archives to pre-train their models. It’s as simple as dragging and dropping the relevant images to the person name, or submitting them via the Video Indexer REST API (currently in preview).\n\nWhat to learn more? Read about our advanced custom face recognition options.\nAnother important customization is the ability to train language models to your organization’s terminologies or industry specific vocabulary. To allow you to improve the transcription for your organization faster, Video Indexer now automatically collects transcript edits done manually into a new entry in the specific language model you use. All you need to do then, is click the “Train” button to add those to your own customized model. The idea is to create a feedback loop where organizations begin with a base out-of-the-box language model and improve the accuracy of it through manual edits over a period of time until it aligns to their specific industry vertical vocabulary and terms.\n\nNew additions to the Video Indexer pipeline\nOne of the primary benefits of Video Indexer is having one pipeline that orchestrates multiple insights from different channels into one timeline. We regularly work to enrich this pipeline with additional insights.\nOne of the latest additions to Video Indexer’s set of insights is the ability to segment the video by semantic scenes (currently in preview) based on visual cues. Semantic scenes add another level of granularity to the existing shot detection and keyframes extraction models in Video Indexer and aim to depict a single event composed of a series of consecutive shots which are semantically related.\nScenes can be used to group together a set of insights and refer to them as insights of the same context in order to deduct a more complex meaning from them. For example, if a scene includes an airplane, a runway, and luggage, the customer can build logic that deducts that it is happening in an airport. Scenes can also be used as a unit to be extracted as a clip from a full video.\n\nAnother cool addition to Video Indexer is the ability to identify ending rolling credits of a movie or a TV show. This can come in handy for a broadcasters in order to identify when their viewers completed watching the video and in order to identify the right moment to recommend the new show or movie to watch before losing the audience.\nVideo Indexer runs on trust (and in more regions)\nAs Video Indexer is part of the Azure Media Services family and is built to serve organizations of all sizes and industries, it is critical to us to help our customers meet their compliance obligations across regulated industries and markets worldwide. As part of that effort, we are excited to announce that Video Indexer is now ISO 27001, ISO 27018, SOC 1,2,3, HIPAA, FedRAMP, PCI, and HITRUST certified. Learn more about the most current certifications status of Video Indexer and all other Azure services.\nAdditionally, we increased our service availability around the world and are now deployed to 9 regions for your convenience. Available regions now include East US (Trial), East US 2, South Central US, West US 2, North Europe, West Europe, Southeast Asia, East Asia, and Australia East. More regions are coming online soon, so stay tuned. You can always find the latest regional availability of Video Indexer by visiting the products by region page.\nVideo Indexer continues to be fully available for trial on East US. This allows organizations to evaluate the full Video Indexer functionality on their own data before creating a paid account using their own Azure subscription. Once organizations decide to move to their Azure subscription, they can copy all of the videos and model customizations that they created on their trial account by simply checking the relevant check box in the content of the account creation wizard.\n\nWant to be the first to try out our newest capabilities?\nToday, we are excited to announce three private preview programs for features that we have been asked for by many different customers.\nLive transcription – the ability to stream a live event, where spoken words in the audio is transcribed to text and delivered along with video and audio.\nMixed languages transcription – The ability to automatically identify multiple spoken languages in one video file and to create a mixed language transcription for that file.\nAnimation characters detection – The ability to identify characters in animated content as if they were real live people!\nWe will be selecting a set of customers out of a list of those who would like to be our design partners for these new capabilities. Selected customers will be able to highly influence these new capabilities and get models that are highly tuned to their data and organizational flows. Want to be a part of this? Come visit us at NAB Show or contact your account manager for more details!\nVisit us at NAB Show 2019\nIf you are attending NAB Show 2019, please stop by booth #SL6716 to see the latest Azure Media Services innovations! We’d love to meet you, learn more about what you’re building, and walk you through the different innovations Azure Media Services and our partners are releasing at NAB Show. We will also have product presentations in the booth throughout the show.\nHave questions or feedback? We would love to hear from you! Use our UserVoice to help us prioritize features, or email VISupport@Microsoft.com for any questions.\n", "link": "https://azure.microsoft.com/en-us/blog/azure-media-services-the-latest-video-indexer-updates-from-nab-show-2019/", "Role": "Principal Program Manager, Azure Media Services, Video Indexer"},
{"Title": "Welcome to NAB Show 2019 from Microsoft Azure!", "Date": "Posted on April 4, 2019", "Contributor": "Sudheer Sirivara", "Content": "\nPutting the intelligent cloud to work for content creators, owners and storytellers.\nStories entertain us, make us laugh and cry, and are the lens through which we perceive our world. In that world, increasingly overloaded with information, they catch our attention and, if they catch our hearts, we engage. This makes stories powerful, and it’s why so many large technology companies are investing heavily in content – creating it and selling it.\nAt Microsoft, we’re not in the business of content creation.\nWhy? Our mission is to help every person and organization on the planet achieve more. So instead of creating or owning content, we want to provide platforms to help content creators and owners achieve more – from the Intelligent Cloud to the Intelligent Edge, with industry leading artificial intelligence (AI). We’re excited to see that mission come to life through customers such as Endemol Shine, Multichoice, RTL, Ericsson and partners like Avid, Akamai, Haivision, Pipeline FX and Verizon Digital Media Services. And we are excited to announce new Azure rendering, Azure Media Services, Video Indexer and Azure Networking capabilities to help you achieve more at NAB Show 2019. Cue scene.\nFix it in post: higher resolution, less time.\nThe arrival of HD led to an explosion of digital content. Today, not satisfied with even 4K resolution, the industry is moving inexorably toward 8K and beyond. With burgeoning immersive storytelling driving 360-degree / 3D content, high frame-rate, innumerable episodic and unscripted shows on fast release cycles and ever-more visually stunning cinematic features, data volumes are increasing exponentially.\nMicrosoft Azure stands ready with the storage acceleration and capacity to accept your most expansive projects. The new Azure FXT Edge Filer caching appliance delivers higher scalability and performance than ever before, with high-speed memory, SSD and support for Azure Blob storage. It’s a great fit for high-throughput, low-latency applications such as rendering where you need ultra-fast connections between on-premises storage and cloud compute capacity. We believe our Edge Filer appliances are a major differentiator for customers, and they agree – Avere vFXT for Azure has enabled visual effects studio Mr. X to recently render a feature-length film in Azure.\n\nAzure FXT Edge Filer \nAnd speaking of rendering, our new Azure Render Farm Manager Portal preview makes it much faster and easier for customers to set up hybrid or cloud-only rendering environments in Azure, including networking setup and Azure storage options, with support for commonly used render farm managers such as PipelineFX Qube.\nWhether it’s rendering, visual effects or editing, we offer the price and performance combination you need. And, our partnership with Avid continues to expand, with the company announcing Avid NEXIS: Cloudspaces on Microsoft Azure, enabling our joint customers to ingest, manage, edit, and create content in the cloud.\nGot content? Get storage. Add AI.\nYour petabytes of content + our Azure Data Box or Data Boxy Heavy (in preview) = secure, enterprise-grade, cost-effective ingest at scale. Just getting off a shoot and have 10’s of terabytes? Meet Data Box Disk. The same benefits in a portable form-factor for smaller content stores. For those on set there is Data Box Edge, which can pre-process media (e.g., remove blank footage) and efficiently transfer it to the cloud through partners such as Dejero or a private high-bandwidth connection with Azure ExpressRoute Direct 100Gbps. We are also making our global network available to you – through Azure ExpressRoute Global Reach, which lets you effectively build your WAN on the Azure backbone. ExpressRoute Direct 100Gbps and Global Reach will be generally available as of NAB.\nOnce in the cloud, you can use Video Indexer’s award-winning AI capabilities to efficiently extract deep insights. Just in time for NAB, we’ve added an AI-based editor to help you generate fresh content in minutes, improved custom face and language models and additional certifications from ISO 27001 to FedRAMP. These new capabilities, and many more, easily integrate with your existing MAMs and can be used with any application to increase accessibility or create new OTT and monetization experiences.\nVideo Indexer (VI) is part of Azure Media Services (AMS), our hyper-scale, enterprise grade, productive media workflow solution. At NAB we are announcing Dolby Atmos support for our just-in-time media packager and a new content-aware encoding preset that lets you apply custom logic, and seek the optimal bitrate for a given resolution, without extensive computational analysis.\nFrom ingest and transcoding to packaging and distribution, Media Services – and our partners – have you covered. You can learn more about these exciting updates and our new private previews for animation, multi-language transcription and live transcriptions here.\n\nVideo Indexer\nStream more content, more easily, to increasingly global audiences\nIncreasing audiences, form factors and globalization mean video workflows that are becoming more and more complex. Our partners are hard at work making this easier for you, and here are a few of the key announcements:\n\nHarmonic announced that it will make its VOS360 SaaS solution available on Microsoft Azure.\nAkamai will directly connect its edge network with Azure through ExpressRoute to give customers higher performance and more predictable costs. It also plans to enhance the delivery of live and on-demand workflows with Azure Media Services and our mutual partners.\nVerizon Digital Media Services is delivering an enterprise-grade streaming platform on Microsoft Azure to enhance video workflows.\nHaivision’s new media routing cloud service, SRTHub, will help broadcasters more securely and reliably transport live video globally. SRTHub, built on Azure, will also streamline workflow orchestration using an open ecosystem of Hublets from industry-leading partners including Avid, Wowza and Epic Labs.\nTelestream will bring its industry leading transcoding solution to Azure.\nStay tuned for more from Microsoft and partner Harmonic in the coming days.\n\nDelivering high-quality and highly available content and applications requires globally-scalable network solutions. To enable our customers to accelerate and deliver superior global applications, we’re announcing the GA of the Azure Front Door Service (AFD). AFD provides a global single point-of-entry that delivers optimized user experiences for web applications. AFD also includes an integrated web application firewall (WAF) and DDoS protection for securing those applications at the network edge.  \nThe next frontier\nAt NAB we’re showcasing how partners such as Zone TV and Nexx.TV are using Microsoft AI and Azure Cognitive Services to create more personalized content and improve monetization of existing media assets.  Stay tuned for more in this space as we work across Microsoft to put our data – and insights – to work for you.\nVisit us at NAB Show 2019 – booth #SL6716 – to learn more, meet with the team and see what our partners have to offer. I hope to see you there – or out there in the real world – and look forward to hearing how we can put Azure to work for you.\n", "link": "https://azure.microsoft.com/en-us/blog/welcome-to-nab-show-2019-from-microsoft-azure/", "Role": "General Manager, Azure Media"},
{"Title": "QnA Maker updates – April 2019", "Date": "Posted on April 15, 2019", "Contributor": "Prakul Bansal", "Content": "\nWe are excited to provide several updates for the QnA Maker service. To see previous releases for Conversational AI from Microsoft in March, see this post.\nNew Bot Framework v4 Template for QnA Maker\nThe QnA Maker service lets you easily create and manage a knowledge base from your data, including FAQ pages, support URLs, PDFs, and doc files. You can test and publish your knowledge base and then connect it to a bot using a bot framework sample or template. With this update we have simplified the bot creation process by allowing you to easily create a bot from your knowledge base, without the need for any code or settings changes. Find more details on creating a QnA bot on our tutorials page.\nAfter you publish your knowledge base, you can create a bot from the publish page with the Create Bot button. If you have previously created bots, you can click on “View all” to see all the bots that are linked to your current subscription.\n\nThis will lead you to a create template in the Azure portal with all your knowledge base details pre-filled in. Your KB ID is connected to the template automatically, and your endpoint key is pre-populated and should not be changed. You can choose to change some of your pre-filled settings (location, resource name, etc). With the single click of a button your bot can now be deployed. Once you have hit create, wait for a few minutes till your web app bot is deployed.\n\nOnce created, you can now test your bot by opening your bot resource and clicking on “Test in Web Chat”. At this point you can chat with your bot and see answers show up from your knowledge base.\n\nQnA Maker supports extraction from SharePoint files\nAdd secured SharePoint data sources to your knowledge base to enrich the knowledge base with questions and answers that may be secured with Active Directory. You can now easily collaborate with others in your organization on source data and connect it to your QnA Maker knowledge base.\nYou can add all currently supported QnA Maker file types from a SharePoint location, instead of loading offline files. From your SharePoint file select the appropriate file’s URL and add that as a source to your knowledge base in the settings page, or during creation.\n\nThe first time you add a SharePoint secured file you will need to explicitly authenticate your Active Directory account and grant permission to QnA Maker through your Active Directory manager.\nFind more details on adding SharePoint Sources in QnA Maker in \"Add a secured Sharepoint data source to your knowledge base\" documentation.\nQnA Maker help bot\nWe are also excited to announce we recently added the QnA Maker help bot to our QnA Maker page. If you have any questions on QnA Maker you can now ask the help bot which is pinned on the left corner on the QnA Make homepage. If you’d like to see what is behind this bot the sample code is available on GitHub.\n\nQnA Maker support in Healthcare Bot\nThe Microsoft Healthcare Bot empowers Microsoft partners to build and deploy compliant, AI-powered Virtual Health Assistants. This allows them to offer their users intelligent, personalized access to health-related information and interactions through a natural conversation experience.\nThe QnA Maker service is now integrated with Healthcare Bot, allowing you to extend the Healthcare Bot experience by connecting it to your knowledgebase (KB) or easily add a set of chit-chat as a starting point for your Healthcare Bot's personality.\nFind more details on adding QnA Maker model to Healthcare Bot in \"Extend your Healthcare Bot with QnA Maker\" documentation.\n\nGet Started\nView our quick start guide to start creating your QnA Maker knowledge base, and then publish and deploy it in a bot (more information on our tutorials page here). You can also give your bot a personality using chit-chat and make it more intelligent with time using Active Learning.\nAnd if you have any questions or feedback please go ahead and chat with the QnA Maker help bot!\n", "link": "https://azure.microsoft.com/en-us/blog/qna-maker-updates-april-2019/", "Role": "Program Manager, AI Products Infuse AI"},
{"Title": "Dear Spark developers: Welcome to Azure Cognitive Services", "Date": "Posted on April 24, 2019", "Contributor": "Anand Raman", "Content": "\n\nThis post was co-authored by Mark Hamilton, Sudarshan Raghunathan, Chris Hoder, and the MMLSpark contributors.\nIntegrating the power of Azure Cognitive Services into your big data workflows on Apache Spark™\nToday at Spark + AI Summit 2019, we're excited to introduce a new set of models in the SparkML ecosystem that make it easy to leverage the Azure Cognitive Services at terabyte scales. With only a few lines of code, developers can embed cognitive services within your existing distributed machine learning pipelines in Spark ML. Additionally, these contributions allow Spark users to chain or Pipeline services together with deep networks, gradient boosted trees, and any SparkML model and apply these hybrid models in elastic and serverless distributed systems.\nFrom image recognition to object detection using speech recognition, translation, and text-to-speech, Azure Cognitive Services makes it easy for developers to add intelligent capabilities to their applications in any scenario. To this date, more than a million developers have already discovered and tried Cognitive Services to accelerate breakthrough experiences in their application.\nAzure Cognitive Services on Apache Spark™\nCognitive Services on Spark enable working with Azure’s Intelligent Services at massive scales with the Apache Spark™ distributed computing ecosystem. The Cognitive Services on Spark are compatible with any Spark 2.4 cluster such as Azure Databricks, Azure Distributed Data Engineering Toolkit (AZTK) on Azure Batch, Spark in SQL Server, and Spark clusters on Azure Kubernetes Service. Furthermore, we provide idiomatic bindings in PySpark, Scala, Java, and R (Beta).\nCognitive Services on Spark allows users to embed general purpose and continuously improve intelligent models directly into their Apache Spark™ and SQL computations. This contribution aims to liberate developers from low-level networking details, so they can focus on creating intelligent, distributed applications. Each Cognitive Service is a SparkML transformer, so users can add services to existing SparkML pipelines. We also introduce a new type of API to the SparkML framework that allows users to parameterize models by either a single scalar, or a column of a distributed spark DataFrame. This API yields a succinct, yet powerful fluent query language that offers a full distributed parameterization without clutter. For more information, check out our session.\nUse Azure Cognitive Services on Spark in these 3 simple steps:\n\nCreate an Azure Cognitive Services Account\nInstall MMLSpark on your Spark Cluster\nTry our example notebook\n\nLow-latency, high-throughput workloads with the cognitive service containers\n\nThe cognitive services on Spark are compatible with services from any region of the globe, however many scenarios require low or no-connectivity and ultra-low latency. To tackle these with the cognitive services on Spark, we have recently released several cognitive services as docker containers. These containers enable running cognitive services locally or directly on the worker nodes of your cluster for ultra-low latency workloads. To make it easy to create Spark Clusters with embedded cognitive services, we have created a Helm Chart for deploying a Spark clusters onto the popular container orchestration platform Kubernetes. Simply point the Cognitive Services on Spark at your container’s URL to go local!\nAdd any web service to Apache Spark™ with HTTP on Spark\n\nThe Cognitive Services are just one example of using networking to share software across ecosystems. The web is full of HTTP(S) web services that provide useful tools and serve as one of the standard patterns for making your code accessible in any language. Our goal is to allow Spark developers to tap into this richness from within their existing Spark pipelines.\nTo this end, we present HTTP on Spark, an integration between the entire HTTP communication protocol and Spark SQL. HTTP on Spark allows Spark users to leverage the parallel networking capabilities of their cluster to integrate any local, docker, or web service. At a high level, HTTP on Spark provides a simple and principled way to integrate any framework into the Spark ecosystem.\nWith HTTP on Spark, users can create and manipulate their requests and responses using SQL operations, maps, reduces, filters, and any tools from the Spark ecosystem. When combined with SparkML, users can chain services together and use Spark as a distributed micro-service orchestrator. HTTP on Spark provides asynchronous parallelism, batching, throttling, and exponential back-offs for failed requests so that you can focus on the core application logic.\nReal world examples\nThe Metropolitan Museum of Art\n\nAt Microsoft, we use HTTP on Spark to power a variety of projects and customers. Our latest project uses the Computer Vision APIs on Spark and Azure Search on Spark to create a searchable database of Art for The Metropolitan Museum of Art (The MET). More Specifically, we load The MET’s Open Access catalog of images, and use the Computer Vision APIs to annotate these images with searchable descriptions in parallel. We also used CNTK on Spark, and SparkML’s Locality Sensitive Hash implementation to futurize these images and create a custom reverse image search engine. For more information on this work, check out our AI Lab or our Github.\n\nThe Snow Leopard Trust\nWe partnered with the Snow Leopard Trust to help track and understand the endangered Snow Leopard population using the Cognitive Services on Spark. We began by creating a fully labelled training dataset for leopard classification by pulling snow leopard images from Bing on Spark. We then used CNTK and Tensorflow on Spark to train a deep classification system. Finally, we interpreted our model using LIME on Spark to refine our leopard classifier into a leopard detector without drawing a single bounding box by hand! For more information, you can check out our blog post.\n\nConclusion\nWith only a few lines of code you can start integrating the power of Azure Cognitive Services into your big data workflows on Apache Spark™. The Spark bindings offer high throughput and run anywhere you run Spark. The Cognitive Services on Spark fully integrate with containers for high performance, on premises, or low connectivity scenarios. Finally, we have provided a general framework for working with any web service on Spark. You can start leveraging the Cognitive Services for your project with our open source initiative MMLSpark on Azure Databricks.\nLearn more\nWeb\nGithub\nEmail: mmlspark-support@microsoft.com\n", "link": "https://azure.microsoft.com/en-us/blog/dear-spark-developers-welcome-to-azure-cognitive-services/", "Role": "Group Product Manager, Azure AI"},
{"Title": "LaLiga entertains millions with Azure-based conversational AI", "Date": "Posted on May 3, 2019", "Contributor": "Jesus Serrano", "Content": "\nFor LaLiga, keeping fans entertained and engaged is a top priority. And when it comes to fans, the Spanish football league has them in droves, with approximately 1.6 billion social media followers around the world. So any time it introduces a new feature, forum, or app for fans, instant global popularity is almost guaranteed. And while this is great news for LaLiga, it also poses technical challenges—nobody wants systems crashing or going unresponsive when millions of people are trying out a fun new app.\nWhen LaLiga chose to develop a personal digital assistant running on Microsoft Azure, its developers took careful steps to ensure optimal performance in the face of huge user volume in multiple languages across a variety of voice platforms. Specifically, the league used Azure to build a conversational AI solution capable of accommodating the quirks of languages and nicknames to deliver a great experience across multiple channels and handle a global volume of millions of users.\nAlong the way, some valuable lessons emerged for tackling a deployment of this scope and scale.\n\nAccommodating the quirks of languages and nicknames\nThe LaLiga virtual assistant has launched for Google Assistant and Skype, and it will eventually support 11 platforms. The assistant was created with Azure Bot Service and the Microsoft Bot Framework, and it incorporates Azure Cognitive Services and a variety of other Azure tools. The main engine for the assistant takes advantage of the scalability and flexibility of Azure App Service —a platform as a service (PaaS) offering—to streamline development. LaLiga used it multiple times to accelerate the development of the bot logic, image service, Google Assistant connector, Alexa connector, data loaders, cache management, and two Azure functions for live data and proactive messages.\n\nFigure 1. An overview of the LaLiga virtual assistant architecture\nFans can ask the assistant questions using natural language, and the system parses this input to determine user intent by using Azure Text Analytics and Azure Cognitive Services Language Understanding in either Spanish or English. That may seem straightforward, but developers learned that subtitles of language can complicate the process. For example, the primary word for “goalkeeper” is different in the Spanish dialects used in Spain, Argentina, and Colombia. So the mapping of questions to intents needed to accommodate a many-to-one relationship for these variations.\nA similar issue arose with players whose names have complicated spellings and don’t clearly correspond to the pronunciation - for example, \"Griesman\" instead of \"Griezmann\" - resulting in a variety of translations. The solution here was to use aliases to guide the system to the correct player. Nicknames were another sticking point. Developers used Azure Monitor Application Insights to investigate user queries that weren’t mapping to any existing player and found that a lot of people were asking about a player but using his nickname rather than his official name. Once again, aliases came to the rescue\nGuaranteeing a great experience across multiple channels\nOne goal of the development team was to support a consistent, high-quality user experience across different mobile platforms and devices, each of which has its own display parameters and may also have different connection protocols. In response to every user query, the LaLiga virtual assistant returns three elements: an image, some text, and a voice reply. The image can be a picture of a player or a “hero card” showing match results or player statistics. For channels with a visual display, the image and text are customized with XAML to make them easily legible for the specific display resolution.\nAll channels aren’t created equal when it comes to user popularity, either. LaLiga expects that some channels will be used much more frequently than others, and this requires adjustments to properly manage scalability resources. Developers created an app service for each channel and optimized it for anticipated usage.\nDevelopers also needed to customize the connectors that the assistant uses for different channels depending on the channels’ capabilities and requirements. For example, the Alexa interface is based on Microsoft .NET Framework, which made it straightforward to develop a connection with Microsoft tools, but Google Assistant uses Node.js, requiring more complex development. Developers found it tricky to map messages from the LaLiga virtual assistant to types that Google Assistant understands. Adding a custom connector hosted with App Service resolved the issue. App Service also helps manage the scalability requirements for the channel. Microsoft is using the lessons learned from the LaLiga virtual assistant to help all developers streamline the creation of connectors with Azure-based bots.\n\nFigure 2. An overview of integration with Google Assistant and Alexa\nPlanning for millions of enthusiastic users\nLaLiga anticipates that the assistant will be hugely popular and that most users will ask multiple questions, generating a vast number of hits on the system each day and leading to high consumption of computing resources. Developers adopted multiple strategies to mitigate this high demand.\nIncoming queries get divided into two categories—live and non-live. A live query could be one about a match in progress, where the data could be constantly changing, whereas a non-live query might relate to a completed game or player’s basic statistics. Whenever a non-live query arrives, the result is cached, so the answer is readily available if someone else asks the same question. The LaLiga virtual assistant uses a highly optimized Azure SQL database as its main data storage, rather than a non-structured data lake, to expedite results.\nBecause scalability was a big concern, the team decided early to dedicate a developer to scalability testing. The developer created an automated system to simulate queries to the assistant, eventually testing millions of hits a day and setting the stage for a smooth launch. Bombarding the system with so many queries revealed another hitch—all those hits were genuine queries, but some web services might see that huge volume and think that the system is being hit by a distributed denial of service (DDOS) attack. So it’s essential to ensure that all components are configured to account for the popularity of the assistant.\nLearn from LaLiga and build your own great bot\nWhile some of these use cases may seem straightforward, the LaLiga virtual assistant development experience showed that sometimes a small tweak to development processes or application configuration can yield substantial rewards in terms of system performance and development time. We hope that the lessons learned during the LaLiga project will help you build your own massively popular digital assistant!\nRead the case study for more information about LaLiga’s digital transformation and its initiatives to boost fan engagement.\nGet started building your own branded virtual assistant.\nStart more simply and build your first Q&A bot with QnA Maker.\n", "link": "https://azure.microsoft.com/en-us/blog/laliga-entertains-billions-with-azure-based-conversational-ai/", "Role": "Sports Business Development Architect"},
{"Title": "A deep dive into what’s new with Azure Cognitive Services", "Date": "Posted on May 3, 2019", "Contributor": "Anand Raman", "Content": "\nThis blog post was co-authored by Tina Coll, Senior Product Marketing Manager, Azure Cognitive Services. \nMicrosoft Build 2019 marks an important milestone for the evolution of Azure Cognitive Services with the introduction of new services and capabilities for developers. Azure empowers developers to make reinforcement learning real for businesses with the launch of Personalizer. Personalizer, along with Anomaly Detector and Content Moderator, is part of the new Decision category of Cognitive Services that provide recommendations to enable informed and efficient decision-making for users.\nAvailable now in preview and general availability (GA):\nPreview\nCognitive service APIs:\n\nPersonalizer – creates personalized user experiences\nConversation transcription – transcribes in-person meetings in real-time\nForm Recognizer – automates data-entry\nInk Recognizer – unlocks the potential of digital inked content\n\nContainer support for businesses AI models at the edge and closer to the data:\n\nSpeech Services (Speech to Text & Text to Speech)\nAnomaly Detector\nForm Recognizer\n\nGenerally available\n\nNeural Text-to-Speech\nComputer Vision Read\nText Analytics Named Entity Recognition\n\nCognitive Services span the categories of Vision, Speech, Language, Search, and Decision, offering the most comprehensive portfolio in the market for developers who want to embed the ability to see, hear, translate, decide and more into their apps.  With so much in store, let’s get to it.\nDecision: Introducing Personalizer, reinforcement learning for the enterprise\nRetail, Media, E-commerce and many other industries have long pursued the holy grail of personalizing the experience. Unfortunately giving customers more of what they want often requires stringing together various CRM, DMP, name-your-acronym platforms and running A/B tests day and night. Reinforcement learning is the set of techniques that allow AI to achieve a goal by learning from what’s happening in the world in real-time. Only Azure delivers this powerful reinforcement-learning based capability through a simple-to-use API with Personalizer.\nWithin Microsoft, teams are using Personalizer to enhance the user experience. Xbox saw a 40 percent lift in engagement by using Personalizer to display content to users that will most likely interest them.\n\nSpeech: In-person meetings just got better with conversation transcription\nConversation transcription, an advanced speech-to-text feature, improves meeting efficiency by transcribing conversations in real-time, enabling all participants to engage fully, capturing who said what when so you can quickly follow up on next steps. Pair conversation transcription with a device integrating the Speech Service Device SDK, now generally available, for higher-quality transcriptions. It also integrates with a variety of meeting conference solutions including Microsoft Teams and other third-party meeting software. Visit the Speech page to see more details.\n\nVision: Unlocking the value of your content – from forms to digital inked notes\nForm Recognizer uses advanced machine learning technology to quickly and more accurately extract text and data from business’s forms and documents. With container support, this service can run on-premises and in the cloud. Automate information extraction quickly and tailor to specific content, with only 5 samples, and no manual labeling.\n\nInk Recognizer provides applications with the ability to recognize digital handwriting, common shapes, and the layout of inked documents. Through an API call, you can leverage Ink Recognizer to create experiences that combine the benefits of physical pen and paper with the best of the digital.\n\nIntegrated in Microsoft Office 365 and Windows, Ink Recognizer gives users freedom to create content in a natural way. Ink Recognizer in PowerPoint, converts ideas to professional looking slides in a matter of moments.\n\nBringing AI to the edge\nIn November 2018, we announced the Preview of Cognitive Services in containers that run on-premises, in the cloud or at the edge, an industry first.\n\nContainer support is now available in preview for:\n\nSpeech Services (Speech to Text & Text to Speech)\nAnomaly Detector\nForm Recognizer\n\nWith Cognitive Services in containers, ISVs and enterprises can transform their businesses with edge computing scenarios. Axon, a global leader in connected public safety technologies partnering with more than 17,000 law enforcement agencies in 100+ countries around the world, relies on Cognitive Services in containers for public safety scenarios where the difference of a second in response time matters:\n“Microsoft's containers for Cognitive Services allow us to ensure the highest levels of data integrity and compliance for our law enforcement customers while enabling our AI products to perform in situations where network connectivity is limited.” \n– Moji Solgi, VP of AI and Machine Learning, Axon\nFortifying the existing Cognitive Services portfolio\nIn addition to the new Cognitive Services, the following capabilities are generally available:\nNeural Text-to-Speech now supports 5 voices and is available in 9 regions to provide customers greater language coverage and support. By changing the styles using Speech Synthesis Markup Language or the voice tuning portal, you can easily refine the voice to express different emotions or speak with different tones for various scenarios. Visit the Text-to-Speech page to “hear” more on the new voices available.\nComputer Vision Read operation reads multi-page documents and contains improved capabilities for extracting text from the most common file types including PDF and TIFF.\n\nIn addition, Computer Vision has an improved image tagging model that now understands 10K+ concepts, scenes, and objects and has also expanded the set of recognized celebrities from 200K to 1M. Video Indexer has several enhancements including new AI Editor won a NAB Show Product of the Year Award in the AI/ML category at this year’s event.\nNamed entity recognition, a capability of Text Analytics, takes free-form text and identifies the occurrences of entities such as people, locations, organizations, and more. Through a API call, named entity recognition uses robust machine learning models to find and categorize more than twenty types of named entities in any text documents. Named entity recognition supports 19 language models available in Preview, with English and Spanish now generally available.\nQnA Maker supports multi-turn dialogs, enhancing its core capability of extracting dialog from PDFs or websites.\nGet started today\nToday’s milestones illustrate our commitment to bringing the latest innovations in AI to the intelligent cloud and intelligent edge.\nTo start building intelligent apps, visit the Azure Cognitive Services page.\n", "link": "https://azure.microsoft.com/en-us/blog/a-deep-dive-into-what-s-new-with-azure-cognitive-services/", "Role": "Group Product Manager, Azure AI"},
{"Title": "AI-first content understanding, now across more types of content for even more use cases", "Date": "Posted on May 3, 2019", "Contributor": null, "Content": "\nThis post is authored by Elad Ziklik, Principal Program Manager, Applied AI.\nToday, data isn’t the barrier to innovation, usable data is. Real-world information is messy and carries valuable knowledge in ways that are not readily usable and require extensive time, resources, and data science expertise to process. With Knowledge Mining, it’s our mission to close the gap between data and knowledge.\nWe’re making it easier to uncover latent insights across all your content with:\n\nAzure Search’s cognitive search capability (general availability)\nForm Recognizer (preview)\n\nCognitive search and expansion into new scenarios\nAnnounced at Microsoft Build 2018, Azure Search’s cognitive search capability uniquely helps developers apply a set of composable cognitive skills to extract knowledge from a wide range of content. Deep integration of cognitive skills within Azure Search enables the application of facial recognition, key phrase extraction, sentiment analysis, and other skills to content with a single click. This knowledge is organized and stored in a search index, enabling new experiences for exploring the data.\nCognitive search, now generally available, delivers:\n\nFaster performance - Improved throughput capabilities with increased processing speeds up to 30 times faster than in preview. Completing previously hour-long tasks in only a couple of minutes.\nSupport of complex data types - Natively supported to extend the types of data that can be stored and searched (this has been the most requested Azure Search feature.) Raw datasets can include hierarchical or nested substructures that do not break down neatly into a tabular rowset, for example multiple locations and phone numbers for a single customer.\nNew skills - Extended library of pre-built skills based on customer feedback. Improved support for processing images, added ability to create conditional skills, and shaper skills that allow for better control and management of multiple skills in a skillset. Plus, entity recognition provides additional information to each entity identified, such as the Wikipedia URL.\nEasy implementation - The solution accelerator provides all the resources needed to quickly build a prototype, including templates for deploying Azure resources, a search index, custom skills, a web app, and PowerBI reports. Use the accelerator to jump start development efforts and apply cognitive search to your business needs.\n\nSee what’s possible when you apply cognitive search to unstructured content, like art:\n\nTens of thousands of customers use Azure Search today, processing over 260 billion files each month. Now with cognitive search, millions of enrichments are performed over data ranging from PDFs to Office documents, from JSON files to JPEGs. This is possible because cognitive search reduces the complexity to orchestrate complex enrichment pipelines containing custom and prebuilt skills, resulting in deeper insight of content. Customers across industries including healthcare, legal, media, and manufacturing use this capability to solve business challenges.\n“Complex customer needs and difficult markets are our daily business. Cognitive search enables us to augment expert knowledge and experience for reviewing complex technical requirements into an automated solution that empowers knowledge workers throughout our organization.”  Chris van Ravenswaay, Business Solution Manager, Howden\nExtending AI-driven content understanding beyond search\nMany scenarios outside of search require extracted insights from messy, complicated information. Expanding cognitive search to support unique scenarios, we are excited to announce the preview of the knowledge store capability within cognitive search – allowing access to AI-generated annotations in table and JSON format for application in non-search use cases like PowerBI dashboards, machine learning models, organized data repositories, bots, and other custom applications.\nForm Recognizer, a new Cognitive Service\nThe Form Recognizer Cognitive Service, available in preview, applies advanced machine learning to accurately extract text, key-value pairs, and tables from documents.\nWith as few as 5 samples, Form Recognizer tailors its understanding to your documents. You can also use the REST interface of the Form Recognizer API to then integrate into cognitive search indexes, automate business processes, and create custom workflows for your business. You can turn forms into usable data at a fraction of the time and cost, so you can focus more time acting on the information rather than compiling it.\nContainer support for Form Recognizer supports use on the edge, on-premises, and in the cloud. The portable architecture can be deployed directly to Azure Kubernetes Service or Azure Container Instances or to a Kubernetes cluster deployed to Azure Stack.\nOrganizations like Chevron and Starbucks are using Form Recognizer to accelerate extraction of knowledge from forms and make faster decisions.\n\nWe look forward to seeing how you leverage these products to drive impact for your business.\nGetting Started\n\nRead more in docs\nGet started with the solution accelerator\nTry Azure Search's cognitive search\nExplore knowledge store capability of cognitive search\nExplore Form Recognizer\n\n", "link": "https://azure.microsoft.com/en-us/blog/ai-first-content-understanding-now-across-more-types-of-content-for-even-more-use-cases/", "Role": null},
{"Title": "Accelerate bot development with Bot Framework SDK and other updates", "Date": "Posted on May 16, 2019", "Contributor": "Yochay Kiriaty", "Content": "\nConversational experiences have become the norm, whether you’re looking to track a package or to find out a store’s hours of operation. At Microsoft Build 2019, we highlighted a few customers who are building such conversational experiences using the Microsoft Bot Framework and Azure Bot Service to transform their customer experience.\n\nLaLiga built its own virtual assistant, which allows fans to experience and interact with LaLiga across multiple platforms.\nBMW has created the BMW Intelligent Personal Assistant to deliver conversational experiences across multiple canvases.\n\nAs users become more familiar with bots and virtual assistants, they will invariably expect more from their conversational experiences. For this reason, Bot Framework SDK and tools are designed to help developers be more productive in building conversational AI solutions. Here are some of the key announcements we made at Build 2019:\nBot Framework SDK and tools\nAdaptive dialogs\nThe Bot Framework SDK now supports adaptive dialogs (preview). Adaptive dialog dynamically updates conversation flow based on context and events. Developers can define actions, each of which can have a series of steps defined by the result of events happening in the conversation to dynamically adjust to context. This is especially handy when dealing with conversation context switches and interruptions in the middle of a conversation. Adaptive dialog combines input recognition, event handling, model of the conversation (dialog) and output generation into one cohesive, self-contained unit. The diagram below depicts how adaptive dialogs can allow a user to switch contexts. In this example, a user is looking to book a flight, but switches context by asking for weather related information which may influence travel plans.\n\nYou can read more about adaptive dialogs here.\nSkills\nDevelopers can compose conversational experiences by stitching together re-usable conversational capabilities, known as skills. Implemented as Bot Framework bots, skills include language models, dialogs, and cards that are reusable across applications. Current skills, available in preview, include Email, Calendar, and Points of Interest.\n\nWithin an enterprise using skills you can now integrate multiple sub-bots owned by different teams into a central bot, or more broadly leverage common capabilities provided by other developers. With the preview of skills, developers can create a new bot (from the Virtual Assistant template) and add/remove skills with one command line operation incorporating all dispatch and configuration changes. Get started with skill developer templates (.NET, TS).\nVirtual assistant solution accelerator\nThe Enterprise Template is now the Virtual Assistant Template, allowing developers to build a virtual assistant with out of the box with skills, adaptive cards, typescript generator, updated conversational telemetry and PowerBI analytics, and ARM based automated Azure deployment. It also provides C# template simplified and aligned to ASP.NET MVC pattern with dependency injection. Developers who have already made use of the Enterprise Template and want to use the new capabilities can follow these steps to get started quickly.\nEmulator\nThe Bot Framework Emulator has released a preview of the new Bot Inspector feature: a way to debug and test your Bot Framework SDK v4 bots on channels like Microsoft Teams, Slack, Cortana, Facebook Messenger, Skype, etc. As you have the conversation, messages will be mirrored to the Bot Framework Emulator where you can inspect the message data that the bot received. Additionally, a snapshot of the bot state for any given turn between the channel and the bot is rendered as well. You can inspect this data by clicking on the \"Bot State\" element in the conversation mirror. Read more about Bot Inspector.\nLanguage generation (preview)\nStreamlines the creation of smart and dynamic bot responses by constructing meaningful, variable, and grammatically correct responses that a bot can send back to the user. Visit the GitHub repo for more details.\nQnA Maker\nEasily handle multi-turn conversation\nWith QnA Maker, you can now handle a predefined set of multi-turn question and answer flows. For example, you can configure QnA Maker to help troubleshoot a product with a customer by preconfiguring a set of questions and follow up question prompts to lead users to specific answers. QnA Maker supports extraction of hierarchical QnA pairs from a URL, .pdf, or .docx files. Read more about QnA Maker multi-turn in our docs, check out the latest samples, and watch a short video.\nSimplified deployment\nWe’ve simplified the process of deploying a bot. Using a pre-defined bot framework v4 template, you can create a bot from any published QnA Maker knowledge base. Not only can you now create a complex QnA Maker knowledge base in minutes, but you can now deploy it to supported channels like Teams, Skype, or Slack in minutes.\nLanguage Understanding (LUIS)\nLanguage Understanding has added several features that let developers extract more detailed information from text, so users can now build more intelligent solutions with less effort.\nRoles for any entity type\nWe have extended roles to all entity types, which allows the same entities to be classified with different subtypes based on context.\nNew visual analytics dashboard\nThere’s now a more detailed, visually-rich, comprehensive analytics dashboard. It's user-friendly design highlights common issues most users face when designing applications by providing simple explanations on how to resolve them to help users gain more insight into their models’ quality, potential data problems, and guidance to adopt best practices.\nDynamic lists\nData is ever-changing and different from one end-user to another. Developers now have more granular control of what they can do with Language Understanding, including being able to identify and update models at runtime through dynamic lists and external entities. Dynamic lists are used to append to list entities at prediction time, permitting user-specific information to get matched exactly.\nRead more about the new Language Understanding features, available through our new v3 API, in our docs. Customers like BMW, Accenture, Vodafone, and LaLiga are using Azure to build sophisticated bots faster and find new ways to connect with their customers.\n\nGet started\nWith these enhancements, we are delivering value across the entire Microsoft Bot Framework SDKs and tools, Language Understanding, and QnA maker in order to help developers become more productive in building a variety of conversational experiences.\nWe look forward to seeing what conversational experiences you will build for your customers. Get started today!\nWatch on-demand sessions at Microsoft Build 2019:\n\nHow to use Conversational AI to scale your business.\nYour Brand, Your Assistant – How to build your own voice-first assistant.\nBuild Live – BMW and Bot Framework Virtual Assistant solution accelerator.\nCreate a truly conversational bot in minutes using QnA Maker.\n\n", "link": "https://azure.microsoft.com/en-us/blog/accelerate-bot-development-with-bot-framework-sdk-and-other-updates/", "Role": "Principal Program Manager, Azure Platform"},
{"Title": "Using Text Analytics in call centers", "Date": "Posted on June 5, 2019", "Contributor": "Raymond Laghaeian", "Content": "\nAzure Cognitive Services provides Text Analytics APIs that simplify extracting information from text data using natural language processing and machine learning. These APIs wrap pre-built language processing capabilities, for example, sentiment analysis, key phrase extraction, entity recognition, and language detection.\nUsing Text Analytics, businesses can draw deeper insights from interactions with their customers. These insights can be used to create management reports, automate business processes, for competitive analysis, and more. One area that can provide such insights is recorded customer service calls which can provide the necessary data to:\n\nMeasure and improve customer satisfaction\nTrack call center and agent performance\nLook into performance of various service areas\n\nIn this blog, we will look at how we can gain insights from these recorded customer calls using Azure Cognitive Services.\nUsing a combination of these services, such as Text Analytics and Speech APIs, we can extract information from the content of customer and agent conversations. We can then visualize the results and look for trends and patterns.\n\nThe sequence is as follows:\n\nUsing Azure Speech APIs, we can convert the recorded calls to text. With the text transcriptions in hand, we can then run Text Analytics APIs to gain more insight into the content of the conversations.\nThe sentiment analysis API provides information on the overall sentiment of the text in three categories positive, neutral, and negative. At each turn of the conversation between the agent and customer, we can:\n \nSee how the customer sentiment is improving, staying the same, or declining.\nEvaluate the call, the agent, or either for their effectiveness in handling customer complaints during different times.\nSee when an agent is consistently able to turn negative conversations into positive or vice versa and identify opportunities for training.\n\n\nUsing the key phrase extraction API, we can extract the key phrases in the conversation. This data, in combination with the detected sentiment, can assign categories to a set of key phrases during the call. With this data in hand, we can:\n \nSee which phrases carry negative or positive sentiment.\nEvaluate shifts in sentiment over time or during product and service announcements.\n\n\n\n\n\nUsing the entity recognition API, we can extract entities such as person, organization, location, date time, and more. We can use this data, for example, to:\n \nTie the call sentiment to specific events such as product launches or store openings in an area.\nUse customer mentions of competitors for competitive intelligence and analysis.\n\n\nLastly, Power BI can help visualize the insights and communicate the patterns and trends to drive to action.\n\n\nUsing the Azure Cognitive Services Text Analytics, we can gain deeper insights into customer interactions and go beyond simple customer surveys into the content of their conversations.\nA sample code implementation of the above workflow can be found on GitHub.\n", "link": "https://azure.microsoft.com/en-us/blog/using-text-analytics-in-call-centers/", "Role": "Principal Program Manager, Azure Cognitive Services "},
{"Title": "Cognitive Services Speech SDK 1.2 – December update – Python, Node.js/NPM and other improvements", "Date": "Posted on January 7, 2019", "Contributor": "Wolfgang Manousek", "Content": "\nDevelopers can now access the latest improvements to Cognitive Services Speech Service including a new Python API and more. Details below.\nRead the updated Speech Services documentation to get started today.\n\nWhat’s new\nPython API for Speech Service\n\nPython 3.5 and later versions on the Windows and Linux operating systems are supported.\nPython is the first language that the Speech Service supports on macOS X (version 10.12 and later).\nPython modules can be conveniently installed from PyPI.\n\nNode.js support\n\nSupport for Node.js is now available, in addition to support for JavaScript in the browser. Through the npm package manager, developers can install the Speech Service module and its prerequisites.\n\n\nThe JavaScript version of Speech Service is now also available as an opensource project on GitHub.\n\nLinux support\nSupport for Ubuntu 18.04 is now available in addition to pre-existing support for Ubuntu 16.04.\nNew features by popular demand\nLightweight SDK for greater performance\nBy reducing the number of required concurrent threads, mutexes, and locks, Speech Services now offers a more lightweight SDK with enhanced error reporting.\nControl of server connectivity and connection status\nA newly added connection object enables control over when the SDK connects to the Speech Service. You can also now subscribe to receive connection notifications that report the exact time of server connection and termination.\nUnlimited audio session length support\nFor JavaScript, length restrictions for recorded audio sessions have been lifted. The SDK buffers the audio file and then automatically reconnects and retransmits audio data to the service.\nSupport for ProGuard during Android APK generation is also now available.\nFor more details and examples for how your business can benefit from the new functionality for Speech Services, check out release notes and samples in the GitHub sample repository for Speech Services.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-services-speech-december-update/", "Role": "Principal Software Engineering Manager"},
{"Title": "AI is the new normal: Recap of 2018", "Date": "Posted on January 14, 2019", "Contributor": "Anand Raman", "Content": "\nThe year 2018 was a banner year for Azure AI as over a million Azure developers, customers, and partners engaged in the conversation on digital transformation. The next generation of AI capabilities are now infused across Microsoft products and services including AI capabilities for Power BI.\nHere are the top 10 Azure AI highlights from 2018, across AI Services, tools and frameworks, and infrastructure at a glance:\nAI services\n\n1. Azure Machine Learning (AML) service with new automated machine learning capabilities.\n2. Historical milestones in Cognitive Services including unified Speech service.\n3. Microsoft is first to enable Cognitive Services in containers.\n4. Cognitive Search and basketball\n5. Bot Framework v4 SDK, offering broader language support (C#, Python, Java, and JavaScript) and extensibility models.\n\nAI tools and frameworks\n\n6. Data science features in Visual Studio Code.\n7. Open Neural Network Exchange (ONNX) runtime is now open source.\n8. ML.Net and AI Platform for Windows developers.\n\nAI infrastructure\n\n9. Azure Databricks\n10. Project Brainwave, integrated with AML.\n\nWith many exciting developments, why are these moments the highlight? Read on, as this blog begins to explain the importance of these moments.\nAI services\nThese services span pre-built AI capabilities such as Azure Cognitive Services and Cognitive Search, Conversational AI with Azure Bot Service, and custom AI development with Azure Machine Learning (AML).\n1. Azure Machine Learning\nAt Microsoft Connect, the Azure Machine Learning (AML) service with new automated machine learning (automated ML) capabilities became available. With AML, data scientists and developers can quickly and easily build, train, and deploy machine learning models anywhere from the intelligent cloud to the intelligent edge. Once the model is developed, organizations can deploy and manage their models in the cloud and on edge, including IoT devices with integrated (CI/CD) tooling.\nTo learn more, read our announcement blog, “Announcing the general availability of Azure Machine Learning service.”\nFew people know the story behind how Automated ML came to be. It all started in the gene-editing lab in 2016.\nDr. Nicolo Fusi, a machine learning researcher at Microsoft, encountered a problem while working with a new gene editing technology called CRISPR.ML. He tried to use machine learning to predict the best way to edit a gene. His model contained thousands of hyperparameters, making it too difficult and time consuming to optimize with existing methods. Then, Dr. Fusi had a breakthrough idea, why not apply the same approach and algorithms used for recommending movies and products to this problem of model optimization? The result is a recommendation system for machine learning pipelines. The approach combines ideas from collaborative filtering and Bayesian optimization to identify possible machine learning pipelines, allowing data scientists and developers to automate model selection and hyperparameter tuning.\nIn this interview, Dr.Fusi gives you an inside look at how automated ML empowers decision-making and takes the tedium out of data science.\nCheck out this Cornell-published white paper, “Probabilistic Matrix Factorization for Automated Machine Learning” to learn more.\n2. New milestones for Azure Cognitive Services\nAzure Cognitive Services is a collection of APIs that lets developers easily add the ability of vision, speech, language, and search into applications and machines. To date, more than a 1.2 million developers use Cognitive Services.\nAt the Build 2018 conference, Microsoft unveiled the next wave of innovation for Cognitive Services:\nNew Services:\n\nA unified Speech service, enabling developers to perform Speech to Text (speech transcription), Text to Speech (speech synthesis), and Speech Translation for providing real-time speech translation capabilities all through a single API.\nA Custom Vision Service that makes it effortless to train an image recognition system by simply dragging and dropping a collection of images.\nThe preview of the Speech devices SDK as well as the new Speech client SDK.\n\nEnhancements to existing services: \n\nUpdates to Video Indexer to automatically detect known brands in speech and visual text and can be trained to recognize custom brands.\nUpdates to Bing Custom Search, Custom Decision Service, and Cognitive Services Labs with previews of emerging Cognitive Services technologies. As well as, announced support for the customization of neural machine translation.\n\nFor more details, read “Microsoft Empowers developers with new cognitive services capabilities.\"\n3. Microsoft is the first company to deliver Cognitive Services in containers\nIn November Azure Cognitive Services containers became available in preview, making Azure the first platform with pre-built Cognitive Services that span the cloud and the edge.\nTo learn more, please read the technical blog, “Getting started with Azure Cognitive Services in containers.\"\n4. Azure Cognitive Search and Basketball\nAzure Cognitive Search, an AI-first approach to content understanding became available through preview. Cognitive Search expands Azure Search with built-in cognitive skills to extract knowledge. This knowledge is then organized and stored in a search index, enabling new ways for exploring the data.\nCheck out how the National Basketball Association (NBA) used Cognitive Search, Cognitive Services, and custom models to power rich data exploration in the //BUILD 2018 keynote.\nRead “Announcing Cognitive Search: Azure Search + cognitive capabilities\" for more details.\n5. Bot Framework v4 SDK\nWith the general availability of Bot Framework v4 SDK announced in September, developers can take advantage of broader language support. C# and JavaScript are generally available, while Python and Java are in preview. Also take advantage of better extensibility to harness a vibrant ecosystem of pluggable components like dialog management and machine translation. The Bot Framework also includes an emulator and a set of CLI tools to streamline the creation and management of different bot language understanding services. Today the service has over 340 thousand users and growing.\nTo learn more, check out Conversational AI Updates.\nAI tools and frameworks\nThese tools and frameworks include Visual Studio tools for AI, Azure Notebooks, Data Science VMs, Azure Machine Learning Studio, ONNX, and the AI Toolkit for Azure IoT Edge.\n6. Data science features in Visual Studio Code\nAs of November, data science features are available  in the Python extension for Visual Studio Code! With these features, developers can work with data interactively in Visual Studio Code. Whether for exploring data or for incorporating machine learning models into applications, this makes Visual Studio Code an exciting new option for those who prefer an editor for data science tasks.\nVisual Studio Tools for AI provides additional details for taking advantage of these new features.\n7. ONNX Runtime is now open source\nONNX Runtime is now open source. ONNX is an open format to represent machine learning models that enable developers and data scientists to use the frameworks and tools that work best for them including PyTorch, TensorFlow, scikit-learn, and more. ONNX Runtime is the first inference engine that fully supports the ONNX specification. Users typically see two timesthe improvement in performance gains.\nAt Microsoft, teams are using ONNX Runtime to improve the scoring latency and efficiency of their models. For models the teams converted to ONNX, average performance improved by two times compared to scoring in previous solutions. Leading hardware companies such as Qualcomm, Intel and NVIDIA are actively integrating their custom accelerators into ONNX Runtime.\nMore details are available in the blog post, \"ONNX Runtime is now open source.”\n8. ML.NET and AI Platform for Windows Developers\nDevelopers can access ML.Net, a new open-source, cross-platform machine learning framework. The technology behind AI features in Office and Windows has been released as a project on Github.\nIn addition, the AI Platform for Windows developers, allows ONNX models to run natively on Windows-based devices.\nCheck out this blog post and video, “How Three Lines of Code and Windows Machine Learning Empower .NET Developers to Run AI Locally on Windows 10 Devices” for a helpful example of how to use these platforms.\nAI infrastructure\nThis category covers Azure Data Services, compute services including Azure Kubernetes Services (AKS), and AI Silicon support including GPUs and FPGAs.\n9. Azure Databricks\nAzure Databricks, a fast, easy, and collaborative Apache® Spark™-based analytics platform optimized for Azure became widely available. Today, organizations benefit from Azure Databricks' native integration with other services like Azure Blob Storage, Azure Data Factory, Azure SQL Data Warehouse, and Azure Cosmos DB. This platform enables new analytics solutions that support modern data warehousing, advanced analytics, and real-time analytics scenarios.\nTo learn more, read “Ignite 2018 - Making AI real for your business with Azure Data.”\n10. Project Brainwave, integrated with Azure Machine Learning\nMicrosoft showcased the preview of Project Brainwave, integrated with Azure Machine Learning. This service brings hardware-accelerated real-time inference for AI to Azure. The Project Brainwave architecture is deployed on a type of computer chip from Intel called a field programmable gate array (FPGA), which makes real-time AI calculations at a competitive cost and with the industry's lowest lag time.\nIn addition, customers got a sneak peak of bringing Project Brainwave to the edge. Meaning customers can take advantage of this computing speed for their businesses and facilities, even if their systems aren't connected to a network or the Internet.\nRead “Real-time AI: Microsoft announces a preview of Project Brainwave\" for more details.\nAI is the new normal\nAI catalyzes digital transformation. Microsoft believes in making AI accessible so that developers, data scientists and enterprises can build systems that augment human ingenuity to tackle meaningful challenges.\nAI is the new normal. Microsoft has more than 20 years of AI research applied to our products and services. Everyone can now access this AI through simple, yet powerful productivity tools such as Excel and Power BI.\nIn continual support of bringing AI to all, Microsoft introduced new AI capabilities for Power BI. These features enable all Power BI users to discover hidden, actionable insights in their data and drive better business outcomes with easy-to-use AI. No code needed to get started. Here are a few highlights:\n\nIntegration of Azure Cognitive Services.\nKey driver analysis helps users understand what influences key business metrics.\nCreate machine learning models directly in Power BI using automated ML.\nSeamless integration of Azure Machine Learning within Power BI.\n\n\nMoving forward into 2019\nMany thanks to you, our customers, MVPs, developers, and partners in being a part of Microsoft’s journey to empower businesses to build globally scalable AI applications. A new year is on the way, and the possibilities are endless. We can’t wait to share what we have in store for you in 2019 and to see what you will build with Azure this upcoming year. Happy New Year from the Azure AI team!\n", "link": "https://azure.microsoft.com/en-us/blog/ai-is-the-new-normal-recap-of-2018/", "Role": "Group Product Manager, Azure AI"},
{"Title": "Azure Cognitive Services adds important certifications, greater availability, and new unified key", "Date": "Posted on January 21, 2019", "Contributor": "Greg Clark", "Content": "\nOne of the most important considerations when choosing an AI service is security and regulatory compliance. Can you trust that the AI is being processed with the high standards and safeguards that you come to expect with hardened, durable software systems?\nCognitive Services today includes 14 generally available products. Below is an overview of current certifications in support of greater security and regulatory compliance for your business.\nAdded industry certifications and compliance\nSignificant progress has been made in meeting major security standards. In the past six months, Cognitive Services added 31 certifications across services and will continue to add more in 2019. With these certifications, hundreds of healthcare, manufacturing, and financial use cases are now supported. \nThe following certifications have been added:\n\nISO 20000-1:2011, ISO 27001:2013, ISO 27017:2015, ISO 27018:2014, and ISO 9001:2015 certification\nHIPAA BAA\nHITRUST CSF certification\nSOC 1 Type 2, SOC 2 Type 2, and SOC 3 attestation\nPCI DSS Level 1 attestation\n\nFor additional details on industry certifications and compliance for Cognitive Services, visit the Overview of Microsoft Azure Compliance page.\nEnhanced data storage commitments\nCognitive Services now offers more assurances for where customer data is stored at rest. These assurances have been enabled by graduating several Cognitive Services to Microsoft Azure Core Services. The first services to make Azure Core Service commitments (effective January 1, 2019) are Content Moderator, Computer Vision, Face, Text Analytics, and QnA Maker.\nFor your reference, you can learn more about Microsoft Azure Core Services through the Online Services Terms (OST).\nGreater regional availability\nAdditionally, more customers around the world can now take advantage of these intelligence services that are closer to their data. The global footprint for Cognitive Services has expanded over the past several months — going from 15 to 25 Azure data center regions.\nFor further reference, visit the Azure product availability page for the complete list where Cognitive Services are now available.\nSimplified experience with a unified API key\nWhen building large AI systems, many use cases require multiple Cognitive Services and as such, there are efficiencies in adding more services using a single key. Recently, we launched a new bundle of multiple services, enabling the use of a single API key for most of our generally available services: Computer Vision, Content Moderator, Face, Text Analytics, Language Understanding, and Translator Text. Now developers can provision all these services in 21 Azure regions around the world1. More regions and APIs will be added to this unified service throughout 2019.\nGet started today by creating a Cognitive Service resource in the Azure portal. To learn more, watch the latest “This Week in Cognitive,” video on using the unified key.\nIf you haven’t yet started using Cognitive Services for your business, you can try it for free. Visit the Cognitive Services page to learn more.\n1 Note: When a unified key resource is provisioned, the services that are regional will be provisioned in the selected region. Services that are non-regional will still be provisioned globally even though they can now be accessed using the unified key and API endpoint.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-services-certifications-and-availability/", "Role": "Principal Program Manager, Azure Cognitive Services"},
{"Title": "QnA Maker simplifies knowledge base management for your Q&A bot", "Date": "Posted on January 30, 2019", "Contributor": "Prakul Bansal", "Content": "\nThis post was co-authored by the QnA Maker Team.\nWith Microsoft Bot Framework, you can build chatbots and conversational applications in a variety of ways. Whether you’re looking to develop a bot from scratch with the open source Bot Framework, looking to create your own branded assistant with the Virtual Assistant solution accelerator, or looking to create a Q&A bot in minutes with QnA Maker. QnA Maker is an easy-to-use web-based service that makes it easy to power a question-answer application or chatbot from semi-structured content like FAQ documents and product manuals. With QnA Maker, developers can build, train, and publish question and answer bots in minutes.\nToday, we are excited to reveal the launch of a highly requested feature, Active Learning in QnA Maker. Active Learning helps identify and recommend question variations for any question and allows you to add them to your knowledge base. Your knowledge base content won’t change unless you choose to add or edit the suggestions to the knowledge base.\nHow it works\nActive Learning is triggered based on the scores of top N answers returned by QnA Maker for any given query. If the score differences lie within a small range, then the query is considered a possible “suggestion” for each of the possible answers. The exact score difference logic is a function of the score root of the confidence score of the top answer.\nAll the suggestions are then clustered together by similarity and top suggestions for alternate questions are displayed based on the frequency of the particular queries by end users. Therefore, active learning gives the best possible suggestions in cases where the endpoints are getting a reasonable quantity and variety in terms of usage queries.\nQnA Maker learns new question variations in two possible ways.\n\nImplicit feedback – The ranker understands when a user question has multiple answers with scores which are very close and considers that as implicit feedback.\nExplicit feedback – When multiple answers with little variation in scores are returned from the knowledge base, the client application can ask the user which question is the correct question. When the user selects the correct question, the user's explicit feedback is sent to QnA Maker with the Train API.\n\nEither method provides the ranker with similar queries that are clustered. When similar queries are clustered, QnA Maker suggests the user-based questions to the knowledge base designer to accept or reject.\nHow to turn on active learning\nBy default, Active Learning will be disabled for everybody. Please follow the below steps to enable the Active Learning.\n1. To turn active learning on, go to your Service Settings in the QnA Maker portal, in the top-right corner.\n \n2. Find the QnA Maker service then toggle Active Learning.\n\nOnce Active Learning is enabled, the knowledge suggests new questions at regular intervals based on user-submitted questions. You can disable Active Learning by toggling the setting again.\nHow to add Active Learning suggestion to the knowledge base\n1. In order to see the suggested questions, on the Edit knowledge base page, select Show Suggestions.\n\n2. Filter the knowledge base with question and answer pairs to only show suggestions by selecting Filter by Suggestions.\n\n3. Each question section with suggestions shows the new questions with a check mark to accept the question or an x mark to reject the suggestions. Click on the checkmark to add the question.\n\nYou can add or delete all suggestions by selecting Add all or Reject all.\n4. Select Save and Train to save the changes to the knowledge base.\nTo use Active Learning efficiently, one should have higher traffic on the bot. Higher the number of end-user queries, the better will be quality and quantity of suggestions.\nQnA Maker active learning Dialog\nThe QnA Maker active learning Dialog does the following:\n\nGet the Top N matches from the QnA service for every query above the threshold set.\nIf the top result confidence score is significantly more than the rest of the results, show only the top answer.\nIf the Top N results have similar confidence scores, then we prompt the user asking which of the following question he meant.\nOnce the user selects the right question that matches intent, show the answer for that corresponding question.\nThis selection also triggers feedback into the QnA Maker service via the Train API.\n\n\nMigrating knowledge bases from the old preview portal \nYou may recall at //Build last May 2018, we announced the general availability (GA) of QnA Maker with new architecture built on Azure. As a result, knowledge bases created with QnA Maker free preview will need to be migrated to QnA GA, as the QnA Maker preview will be deprecated January 31, 2019. Learn how to migrate existing ones on the documentation, “Migrate a knowledge base using export-import.”\nBelow is a screenshot of the old QnA Maker preview portal for reference:\n\nFor more information about the changes in QnA Maker GA, see the QnA Maker GA announcement blog post, “Announcing General Availability of QnAMaker.”\nQnA Maker GA highlights:\n\nNew architecture. The data and runtime components of the QnAMaker stack will be hosted in the user’s Azure subscription. Learn more on the documentation, “What is QnA Maker?”\nNo more throttling. Pay for services hosted, instead of transactions. See pricing information.\nData privacy and compliance. The QnA data will be hosted within your Azure compliance boundary.\nBrand new portal experience to create and manage your knowledge base. Check out the new portal.\nScale as you go. Scale different part of the stack as per your needs. See upgrading your QnA Maker service.\n\n", "link": "https://azure.microsoft.com/en-us/blog/qna-maker-simplifies-knowledge-base-management-for-your-q-a-bot/", "Role": "Program Manager, AI Products Infuse AI"},
{"Title": "Live stream analysis using Video Indexer", "Date": "Posted on February 20, 2019", "Contributor": "Xavier Pouyat", "Content": "\nVideo Indexer is an Azure service designed to extract deep insights from video and audio files offline. This is to analyze a given media file already created in advance. However, for some use cases it's important to get the media insights from a live feed as quick as possible to unlock operational and other use cases pressed in time. For example, such rich metadata on a live stream could be used by content producers to automate TV production, like our example of EndemolShine Group, by journalists of a newsroom to search into live feeds, to build notification services based on content and more.\nTo that end, I joined forces with Victor Pikula a Cloud Solution Architect at Microsoft, in order to architect and build a solution that allows customers to use Video Indexer in near real-time resolutions on live feeds. The delay in indexing can be as low as four minutes using this solution, depending on the chunks of data being indexed, the input resolution, the type of content and the compute powered used for this process.\n\nFigure 1 – Sample player displaying the Video Indexer metadata on the live stream\nThe stream analysis solution at hand, uses Azure Functions and two Logic Apps to process a live program from a live channel in Azure Media Services with Video Indexer and displays the result with Azure Media Player showing the near real-time resulted stream.\nIn high level, it is comprised of two main steps. The first step runs every 60 seconds, and takes a sub-clip of the last 60 seconds played, creates an asset from it and indexes it via Video Indexer. Then the second step is called once indexing is complete. The insights captured are processed, sent to Azure Cosmos DB, and the sub-clip indexed is deleted.\nThe sample player plays the live stream and gets the insights from Azure Cosmos DB, using a dedicated Azure Function. It displays the metadata and thumbnails in sync with the live video.\n\nFigure 2 – The two logic apps processing the live stream every minute in the cloud.\nNear real-time indexing for video production\nAt the EBU Production Technology Seminar in Geneva last month, an end-to-end solution was demonstrated by Microsoft. Several live feeds were ingested to Azure using Dejero technology or the webRTC protocol, and sent to Make.TV Live Video Cloud to switch inputs. The selected input was sent as a transcoded stream to Azure Media Services for multi bitrate transcoding and OTT delivery in low latency mode.  The same stream was also processed in near real time with Video Indexer.\n\nFigure 3 – Example of live stream processing in Azure\nNext steps\nThe full code and a step-by-step guide to deploy the results can be found in this GitHub project for Live media analytics with Video Indexer. Need near real-time analytics for your content? Now you have a ready-made solution for that, go ahead and give it a try!\nHave questions or feedback? We would love to hear from you! Visit our UserVoice to help us prioritize features, or email VISupport@Microsoft.com with any questions.\n", "link": "https://azure.microsoft.com/en-us/blog/live-stream-analysis-using-video-indexer/", "Role": "Technology Solutions Professional, Azure & Media"},
{"Title": "Running Cognitive Services on Azure IoT Edge", "Date": "Posted on February 26, 2019", "Contributor": "Phani Mutyala", "Content": "\nThis blog post is co-authored by Emmanuel Bertrand, Senior Program Manager, Azure IoT.\nWe recently announced Azure Cognitive Services in containers for Computer Vision, Face, Text Analytics, and Language Understanding. You can read more about Azure Cognitive Services containers in this blog, “Brining AI to the edge.”\nToday, we are happy to announce the support for running Azure Cognitive Services containers for Text Analytics and Language Understanding containers on edge devices with Azure IoT Edge. This means that all your workloads can be run locally where your data is being generated while keeping the simplicity of the cloud to manage them remotely, securely and at scale.\n\nWhether you don’t have a reliable internet connection, or want to save on bandwidth cost, have super low latency requirements, or are dealing with sensitive data that needs to be analyzed on-site, Azure IoT Edge with the Cognitive Services containers gives you consistency with the cloud. This allows you to run your analysis on-site and a single pane of glass to operate all your sites.\nThese container images are directly available to try as IoT Edge modules on the Azure Marketplace:\n\nKey Phrase Extraction extracts key talking points and highlights in text either from English, German, Spanish, or Japanese.\nLanguage Detection detects the natural language of text with a total of 120 languages supported.\nSentiment Analysis detects the level of positive or negative sentiment for input text using a confidence score across a variety of languages.\nLanguage Understanding applies custom machine learning intelligence to a user’s conversational and natural language text to predict overall meaning and pull out relevant and detailed information.\n\nPlease note, the Face and Recognize Text containers are still gated behind a preview, thus are not yet available via the marketplace. However you can deploy them manually by first signing up to for the preview to get access.\nIn this blog, we describe how to provision Language Detection container on your edge device locally and how you manage it through Azure IoT. Currently these containers run only on X64 based devices.\nSet up an IoT Edge device and its IoT Hub\nFollow the first steps in this quick-start for setting up your IoT Edge device and your IoT Hub.\nIt first walks you through creating an IoT Hub and then registering an IoT Edge device to your IoT hub. Here is a screenshot of a newly created edge device called “LanguageDetection\" under the IoT Hub called “CSContainers\". Select the device, copy its primary connection string, and save it for later.\n\nNext, it guides you through setting up the IoT Edge device. If you don’t have a physical edge device, it is recommended to deploy the Ubuntu Server 16.04 LTS and Azure IoT Edge runtime virtual machine (VM) which is available on the Azure Marketplace. It is an Azure Virtual Machine that comes with IoT Edge pre-installed.\nThe last step is to connect your IoT Edge device to your IoT Hub by giving it its connection string created above. To do that, edit the device configuration file under /etc/iotedge/config.yaml file and update the connection string. After the connection string is update, restart the edge device with sudo systemctl restart iotedge.\nProvisioning a Cognitive Service (Language Detection IoT Edge module)\nThe images are directly available as IoT edge modules from the Iot Hub marketplace.\n\nHere we’re using the Language Detection image as an example, however other images work the same way. To download the image, search for the image and select Get it now, this will take you to the Azure portal “Target Devices for IoT Edge Module” page. Select your subscription with your IoT Hub, select Find Device and your IoT Edge device, then click the Select and Create buttons.\n\n\nConfiguring your Cognitive Service\nNow you’re almost ready to deploy the Cognitive Service to your IoT Edge device. But in order to run a container you need to get a valid API key and billing endpoints, then pass them as environment variables in the module details.\n\nGo to the Azure portal and open the Cognitive Services blade. If you don’t have a Cognitive Service that matches the container, in this case a Text Analytics service, then select add and create one. Once you have a Cognitive Service get the endpoint and API key, you’ll need this to fire up the container:\n\nThe endpoint is strictly used for billing only, no customer data ever flows that way. Copy your billing endpoint value to the “billing” environment variable and copy your API key value to the “apikey” environment variable.\nDeploy the container\nAll required info is now filled in and you only need to complete the IoT Edge deployment. Select Next and then Submit. Verify that the deployment is happening properly by refreshing the IoT Edge device details section.\nVerify that the deployment is happening properly by refreshing the IoT Edge device details section.\n\nTrying it out\nTo try things out, we’ll make an HTTP call to the IoT Edge device that has the Cognitive Service container running.\nFor that, we’ll first need to make sure that the port 5000 of the edge device is open. If you’re using the pre-built Ubuntu with IoT Edge Azure VM as an edge device, first go to VM details, then Settings, Networking, and Outbound port rule to add an outbound security rule to open port 5000. Also copy the Public IP address of your device.\nNow you should be able to query the Cognitive Service running on your IoT Edge device from any machine with a browser. Open your favorite browser and go to https://your-iot-edge-device-ip-address:5000.\nNow, select Service API Description or jump directly to https://your-iot-edge-device-ip-address:5000/swagger. This will give you a detailed description of the API.\n\nSelect Try it out and then Execute, you can change the input value as you like.\n\nThe result will show up further down on the page and should look something like the following image:\n\nNext steps\nYou are now up and running! You are running the Cognitive Services on your own IoT Edge device, remotely managed via your central IoT Hub. You can use this setup to manage millions of devices in a secure way.\nYou can play around with the various Cognitive Services already available in the Azure Marketplace and try out various scenarios. Have fun!\n", "link": "https://azure.microsoft.com/en-us/blog/running-cognitive-services-on-iot-edge/", "Role": "Senior Program Manager, Microsoft Azure"},
{"Title": "Latest enhancements now available for Cognitive Services' Computer Vision", "Date": "Posted on February 27, 2019", "Contributor": "Cornelia Carapcea", "Content": "\nThis blog was co-authored by Lei Zhang, Principal Research Manager, Computer Vision\nYou can now extract more insights and unlock new workflows from your images with the latest enhancements to Cognitive Services’ Computer Vision service.\n1. Enrich insights with expanded tagging vocabulary\nComputer Vision has more than doubled the types of objects, situations, and actions it can recognize per image.\nBefore\n\nNow\n\n2. Automate cropping with new object detection feature\nEasily automate cropping and conduct basic counting of what you need from an image with the new object detection feature. Detect thousands of real life or man-made objects in images. Each object is now highlighted by a bounding box denoting its location in the image.\n\n3. Monitor brand presence with new brand detection feature\nYou can now track logo placement of thousands of global brands from the consumer electronics, retail, manufacturing, entertainment industries.\n\nWith these enhancements, you can:\n\nDo at-scale image and video-frame indexing, making your media content searchable. If you’re in media, entertainment, advertising, or stock photography, rich image and video metadata can unlock productivity for your business.\nDerive insights from social media and advertising campaigns by understanding the content of images and videos and detecting logos of interest at scale. Businesses like digital agencies have found this capability useful for tracking the effectiveness of advertising campaigns. For example, if your business launches an influencer campaign, you can apply Custom Vision to automatically generate brand inclusion metrics pulling from influencer-generated images and videos.\n\nIn some cases, you may need to further customize the image recognition capabilities beyond what the enhanced Computer Vision service now provides by adding specific tagging vocabulary or object types that are relevant to your use case. Custom Vision service allows you to easily customize and deploy your model without requiring machine-learning expertise.\nSee it in action through the Computer Vision demo. If you’re ready to start building to unlock these insights, visit our documentation pages for image tagging, object detection, and brand detection.\n", "link": "https://azure.microsoft.com/en-us/blog/latest-enhancements-now-available-for-cognitive-services-computer-vision/", "Role": "Principal Product Lead, Cognitive Services"},
{"Title": "Cognitive Services Speech SDK 1.3 – February update", "Date": "Posted on February 28, 2019", "Contributor": "Leon Romaniuk", "Content": "\nDevelopers can now access the latest Cognitive Services Speech SDK which now supports:\n\nSelection of the input microphone through the AudioConfig class\nUnity in C# (beta)\nAdditional sample code\n\nRead the updated Speech Services documentation to get started today.\n\nWhat’s new\nThe Speech SDK supports a selection of the input microphone through the AudioConfig class, meaning you can stream audio data to the Speech Service from a non-default microphone. For more details see the documentation and the how-to guide on selecting an audio input device with the Speech SDK. This is not yet available from JavaScript.\nThe Speech SDK now also supports Unity in a beta version. Since this is new functionality, please provide feedback through the issue section in the GitHub sample repository. This release supports Unity on Windows x86 and x64 (desktop or Universal Windows Platform applications), and Android (ARM32/64, x86). More information is available in our Unity quickstart.\nSamples\nThe following new content is available in our sample repository.\n\nSamples for AudioConfig.FromMicrophoneInput.\nPython samples for intent recognition and translation.\nSamples for using the Connection object in iOS.\nJava samples for translation with audio output.\nNew sample for use of the Batch Transcription REST API.\n\nImprovements and changes\nA number of improvements and changes have been made since our last release including:\n\nPython\r\n \nImproved parameter verification and error messages in SpeechConfig\nAdded support for the Connection object\nSupport for 32-bit Python (x86) on Windows\nThe Speech SDK for Python is out of beta\n\n\niOS\r\n \nThe SDK is now built against the iOS SDK version 12.1. and supports iOS versions 9.2 and later\nImproved reference documentation and fixed several property names\n\n\nJavaScript\r\n \nAdded support for the Connection object\nAdded type definition files for bundled JavaScript\nInitial support and implementation for phrase hints\nReturned properties collection with service JSON for recognition\n\n\nWindows DLLs now contains a version resource.\n\nBug fixes\n\nEmpty proxy username and proxy password were not handled correctly before. With this release, if you set proxy username and proxy password to an empty string, they will not be submitted when connecting to the proxy.\nSession ID's created by the SDK were not always truly random for some languages and environments. Random generator initialization has been added to fix this.\nImproved handling of authorization token. If you want to use an authorization token, specify in the SpeechConfig and leave the subscription key empty. Then create the recognizer as usual.\nIn some cases, the Connection object wasn't released correctly. This has been fixed.\n\nFor more details and examples for how your business can benefit from the new functionality for Speech Services, check out release notes and samples in the GitHub sample repository for Speech Services.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-services-speech-sdk-1-3-february-update/", "Role": "Senior Program Manager"},
{"Title": "Conversational AI updates for March 2019", "Date": "Posted on March 5, 2019", "Contributor": "Yochay Kiriaty", "Content": "\nWe are thrilled to share the release of Bot Framework SDK version 4.3 and use this opportunity to provide additional updates for the Conversational AI releases from Microsoft.\nNew LINE Channel\nMicrosoft Bot Framework lets you connect with your users wherever your users are. We offer thirteen supported channels, including popular messaging apps like Skype, Microsoft Teams, Slack, Facebook Messenger, Telegram, Kik, and others. We have listened to our developer community and addressed one of the most frequently requested features – added LINE as a new channel. LINE is a popular messaging app with hundreds of millions of users in Japan, Taiwan, Thailand, Indonesia, and other countries.\nTo enable your bot in the new channel, follow the “Connect a bot to LINE” instructions. You can also navigate to your bot in the Azure portal. Go to the Channels blade, click on the LINE icon, and follow the instructions there.\n\nSDK 4.3\nIn the 4.3 release, the team focused on improving and simplifying message and activities handling. The Bot Framework Activity schema is the underlying schema used to define the interaction model for bots. With the 4.3 release, we have streamlined the handling of some activity types in the Bot Framework Activity Schema, exposing a simple On* methods, thus simplifying the usage of such activities. On top of the activity handling improvements, for C# we have added MVC support, allowing developers to use the standard ASP.NET core application and ApiController. As with any release, we fixed a number of bugs, continue to improve LUIS and QnA integration, and further clean our engineering practices. There were additional updates across other areas like Language, Prompt and Dialogs, and Connectors and Adapters.\n\nReview all changes that went into 4.3 in the detailed Change Log.\nStay up to date with the current list of all issues.\n\nSimplify activity message handling\nThis release introduces a new way to handle incoming messages through a new class called ActivityHandler. An ActivityHandler receives incoming activities, as defined in the Bot Framework Activity Schema, then delegates the handling of each activity to one or more handler functions based on the activity’s type and other properties. For example, ActivityHandler exposes methods such as:\n\nOnMessage – For dealing with all incoming messages\nOnMembersAdded – For dealing with messages representing members being added\nOnEvent – For generic event activities\n\nYou can find all the methods in the ActivityHandler.ts (for JavaScript) and ActivityHandler.cs (for .NET).\nUsing ActivityHandler, developers can handle events for incoming messages, application events, and a variety of conversation update events. This should make it easier to create common bot behaviors such as sending greetings and welcoming users.\nThis class provides an extensible base for handling incoming activities in an event-driven way. In JavaScript and TypeScript, the base ActivityHandler class can be used directly as main activity handler, as seen in the example code below. Developers can also derive subclasses from it to extend the core features.\nHere is a small JavaScript code snippet example:\n\r\n// Import the class from botbuilder sdk\r\nconst { ActivityHandler } = require('botbuilder');\r\n// Create the bot “controller” object\r\nconst bot = new ActivityHandler();\r\nserver.post('/api/messages', (req, res) => {\r\n      adapter.processActivity(req, res, async (context) => {\r\n          // Route incoming activities to the ActivityHandler via the run() method\r\n          await bot.run(context);\r\n      });\r\n});\r\n// bind a handler for all incoming activities of type message\r\nbot.onMessage(async (context, next) => {\r\n      // do stuff\r\n      await context.sendActivity(`Echo: ${ context.activity.text }`);\r\n      // proceed with further processing\r\n      await next();\r\n});\r\n// say hello when new members join\r\nbot.onMembersAdded(async(context, next) => {\r\nawait context.sendActivity('Hello! I am a bot!');\r\nawait next();\r\n});\r\n\nWeb API integration for .NET developers\nA core tenant for the Bot Framework team is to drive parity across .NET and JS implementations. In that spirit, the .NET implementation of the ActivityHandler.cs exposes the same functionality with the given special programing language capabilities. However, ASP.NET Core provides a rich set of infrastructures supporting Web API, which can be easily integrated and used by bot developers. Therefore, in addition to the activity handling improvements, for C# we have added Web API support, allowing developers to use standard ASP.NET core application and ApiController.\nHere is a simple code snippet for ASP.NET Web API Controller: \n\r\n[Route(\"api/messages\")]\r\n     [ApiController]\r\n     public class BotController : ControllerBase\r\n     {\r\n         private IBotFrameworkHttpAdapter _adapter;\r\n         private IBot _bot;\r\n\r\n        public BotController(IBotFrameworkHttpAdapter adapter, IBot bot)\r\n         {\r\n             _adapter = adapter;\r\n             _bot = bot;\r\n         }\r\n\r\n        [HttpPost]\r\n         public async Task PostAsync()\r\n         {\r\n             // Delegate the processing of the HTTP POST to the adapter.\r\n             // The adapter will invoke the bot.\r\n             await _adapter.ProcessAsync(Request, Response, _bot);\r\n         }\r\n     }\r\n\nNote, the _bot passed to _adapter.ProcesAsync method is the actual bot implementation and will handle any activity sent from the adapter, which has a very similar code to the above JS sample.\nQnA Maker and Language Understanding\nQnA Maker released Active Learning, which helps developers improve their knowledge base, based on real usage. Active learning helps identify and recommend question variations for any question and allows users to easily add them to their knowledge base.\n\nFor a user query, if QnA Maker returns top N answers where the difference in confidence score is low, Active Learning is triggered. Based on collective feedback across users, QnA Maker shows suggestions for alternate questions in your knowledge base.\nTo learn more about how QnA Maker Active Learning works and how to use it, read the documentation, “Use active learning to improve knowledge base.”\nTemplates and the Virtual Assistant Solution Accelerator\nTemplates and Solution Accelerators provide a mechanism to identify high growth opportunities for our Conversational AI, Speech, and broader Azure platform. These enable our customers and partners to accelerate delivery of advanced, transformational conversational experiences typically not viewed as possible or require too much effort to deliver a high-quality experience.\nIn this latest release we have provided significant updates to our Templates and Virtual Assistant solution. A high level summary of changes are covered in our Release Notes.\nWe are happy to share the availability of a JavaScript (Typescript) version of the Enterprise Template along with a Yeoman Generator. Work has started on the equivalent for the Virtual Assistant. We’ve also added coded unit tests to all Bots created by the templates providing a way to automate unit testing of dialogs along with further enhancements to the telemetry capabilities and the associated PowerBI dashboard.\nWe’ve also delivered a wide range of changes to the Virtual Assistant and Skills including a new template enabling Skills to be quickly created and added to a Virtual Assistant. There is also new support for proactive experiences, enabling the assistant and Skills to proactively reach out to a user or perform long running asynchronous operations.\nAlso, in this release are wide ranging improvements to the Productivity Skills including email, calendar, and to-do,  as well as the addition of FourSquare support to the Point of Interest Skill, and an enhanced WebChat test experience.\nWeb Chat 4.3\nWeb Chat is a popular component that lets developers add a messaging interface for their bot on the websites or mobile apps. Web Chat 4.3 release addresses the remaining accessibility issues and popular feature requests, like better indication of connectivity state for users with poor network connection.\n\nTo try Web Chat 4.3, follow the instructions on GitHub or explore code samples.\nGet started\nAs we continue to improve our conversational AI tools and framework, we look forward to seeing what conversational experiences you will build for your customers. Get started today!\n", "link": "https://azure.microsoft.com/en-us/blog/conversational-ai-updates-for-march-2019/", "Role": "Principal Program Manager, Azure Platform"},
{"Title": "Cognitive Services – Bing Local Business Search now available in public preview", "Date": "Posted on November 7, 2018", "Contributor": "Mahesh Balachandran", "Content": "\nWe are excited to share that Bing Local Business Search API on Cognitive Services is now available in public preview. Bing Local Business Search API enables users to easily find local business information within your applications, given an area of interest. The public preview of Bing Local Business Search API enables scenarios such as calling, navigation, and mapping using contact details, latitude/longitude, and other entity metadata. This metadata comes from hundreds of categories including professionals and services, retail, healthcare, food and drink, and more. Additionally, user queries can be pertaining to a single entity, such as “Microsoft City Center Plaza Bellevue”, a collection of results like “Microsoft offices in Redmond, WA”, and category queries such as “Italian Restaurant” are also supported. Alternatively, users can use one of our predefined categories to query our API.\nBelow is an example of a JSON response. Each result item contains a name, full address, phone number, website, business category, and latitude/longitude. Using these results, you can build engaging user scenarios in your applications. For instance, you could enable users to find and contact a local business. Another example is to enable navigation to the place of interest or plotting results on Bing Maps. An example from Bing.com for the “City Center Plaza Microsoft” query is shown below.\n\nGet more details about the Bing Local Business Search API offering by reading, “API Announcement: Local Business Search Public Preview” on the Bing Developer Blog.\nFor questions please reach out to us via Stack Overflow and Azure Support. We would also love to hear your feedback.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-services-bing-local-business-search-now-available-in-public-preview/", "Role": "Senior Program Manager, Bing Custom Search"},
{"Title": "Bringing AI to the edge", "Date": "Posted on November 14, 2018", "Contributor": "Eric  Boyd", "Content": "\nWe are seeing a clear trend towards a future powered by the intelligent cloud and intelligent edge. The intelligent cloud is ubiquitous computing at massive scale, enabled by the public cloud and powered by AI, for every type of application one can envision. The intelligent edge is a continually expanding set of connected systems and devices that gather and analyze data—close to end users and the data that is generated. Together, they give customers the ability to create a new class of distributed, connected applications that enable breakthrough business outcomes.\nTo accelerate this trend, today we are announcing the preview of Azure Cognitive Services containers, making it possible to build intelligent applications that span the cloud and the edge. Azure Cognitive Services allow developers to easily add cognitive features—such as object detection, vision recognition, and language understanding—into their applications without having direct AI or data science skills or knowledge. Over 1.2 million developers have discovered and tried Azure Cognitive Services to build and run intelligent applications. Containerization is an approach to software distribution in which an application or service is packaged so that it can be deployed in a container host with little or no modification.\nWith container support, customers can use Azure’s intelligent Cognitive Services capabilities, wherever the data resides. This means customers can perform facial recognition, OCR, or text analytics operations without sending their content to the cloud. Their intelligent apps are portable and scale with greater consistency whether they run on the edge or in Azure.\nRun AI on the edge\nWith ever-increasing volumes of data being generated across organizations, customers have been asking for the flexibility to deploy AI capabilities in a variety of environments. By deploying Cognitive Services in containers, customers can analyze information close to the physical world where the data resides, to deliver real-time insights and immersive experiences that are highly responsive and contextually aware. Cognitive Services containers that customers deploy on their own premises do not send customer data (e.g., the image or text that is being analyzed) to Microsoft. \nBuild consistent app architectures across the cloud and edge\nCognitive Services containers enable customers to build one application architecture that is optimized to take advantage of both robust cloud capabilities and edge locality. With containers, customers can choose when to upgrade the AI models deployed in their solutions. Customers can also test new model versions before deploying them in production in a consistent way, whether running on the edge or in Azure.\n\nCustomers and partners are already taking advantage of these enhanced capabilities to drive their businesses forward.\nPsiori is using Cognitive Services containers to analyze medical documents such as lab reports and images directly on the edge to automate and streamline insurance reimbursements. “We were eager to find a solution that gives us high-quality AI; but could also maintain our compliance boundary onsite at local hospitals and the Cognitive Services support for containers was exactly what we were looking for,” said Jona Boddinghaus, partner and senior software developer at Psiori.\nAvanade, a global professional services company, is using Cognitive Services containers to build new intelligent edge applications for their customers where connectivity is constrained. “The areas I see containers and Cognitive Services alleviating customer friction points are when there’s a semi-disconnected environment such as on remote oil rigs, or ships in the middle of the ocean or factory floors,” said Amit Bahree, CTO for AI at Avanade.\nIntel is working on hardware solutions that deliver security for Linux containers. “Azure Cognitive Services containers give you more options on how you grow and deploy AI solutions, either on or off premises, with consistent performance. You can scale up as workload intensity increases or scale out to the edge,” said Andy Vargas, Intel VP of Software and Services.\nGet started today!\nTake advantage of Azure Cognitive Services containers to build intelligent applications today. For more details, please see the technical blog, “Getting started with Azure Cognitive Services in containers”.\nTo learn more about our edge solutions, please see Julia White’s edge computing blog.\n", "link": "https://azure.microsoft.com/en-us/blog/bringing-ai-to-the-edge/", "Role": "Corporate Vice President, Azure AI "},
{"Title": "Getting started with Azure Cognitive Services in containers", "Date": "Posted on November 14, 2018", "Contributor": "Lance Olson", "Content": "\nBuilding solutions with machine learning often requires a data scientist. Azure Cognitive Services enable organizations to take advantage of AI with developers, without requiring a data scientist. We do this by taking the machine learning models and the pipelines and the infrastructure needed to build a model and packaging it up into a Cognitive Service for vision, speech, search, text processing, language understanding, and more. This makes it possible for anyone who can write a program, to now use machine learning to improve an application. However, many enterprises still face challenges building large-scale AI systems. Today we announced container support for Cognitive Services, making it significantly easier for developers to build ML-driven solutions.\nContainerization lets developers build big AI systems that run at scale, reliably, and consistently in a way that supports better data governance. For example, let’s look at a typical hospital system that works with patients. After many years of taking care of patients, they have numerous doctor’s notes, intake records, or other files that they want to process and derive insights about key trends. Using Cognitive Services containers, they can process all of these files, index millions of documents and find commonalities, and improve the patient experience while keeping the data in-house. Another example would be a large manufacturing plant that has limited connectivity where they want to track assets on the edge using remote sensors and cameras, using AI to predict maintenance needs.\nToday’s announcement includes container support for 5 key capabilities within Azure Cognitive Services. Learn more by reading the blog post, \"Bringing AI to the edge\". Read on to learn how you can get started using them today. \nGet started with these Azure Cognitive Services Containers\nText Analytics Containers\n\n\n\nContainer\nDescription\n\n\nKey Phrase Extraction\nExtracts key phrases to identify the main points. For example, for the input text \"The food was delicious and there were wonderful staff,\" the API returns the main talking points: \"food\" and \"wonderful staff.\"\n\n\nLanguage Detection\nFor up to 120 languages, detects which language the input text is written in and reports a single language code for every document submitted on the request. The language code is paired with a score indicating the strength of the score.\n\n\nSentiment Analysis\nAnalyzes raw text for clues about positive or negative sentiment. This API returns a sentiment score between 0 and 1 for each document, where 1 is the most positive. The analysis models are pre-trained using an extensive body of text and natural language technologies from Microsoft. For selected languages, the API can analyze and score any raw text that you provide, directly returning results to the calling application.\n\n\n\n\nLearn how to install and run Text Analytics Containers.\nLearn how you can configure Text Analytics Containers.\nAlready using Azure? Try Text Analytics Containers now.\n\nFace Container\nThe Face Container enables you to add face detection, verification, and emotion detection to an application or system. It uses a common configuration framework, so that you can easily configure and manage storage, logging and telemetry, and security settings for your containers.\n\nLearn how you will need to signup get access to the Face API Container.\nLearn how to install and run the Face Container.\nLearn how you can configure the  Face Container.\nAlready using Azure? Sign up for the Face Container now.\n\nRecognize Text Container\nThe Recognize Text portion of Computer Vision allows you to detect and extract printed text from images of various objects with different surfaces and backgrounds, such as receipts, posters, and business cards.\n\nLearn how you can sign up get access to the Recognize Text Container.\nLearn how to install and run the Recognize Text Container.\nLearn how you can configure the Recognize Text Container.\nAlready using Azure? Sign up for the Recognize Text container now.\n\nIf you put Cognitive Services in containers and you manage them with Azure Kubernetes Service (AKS) and have have a modern, DevOps experience to create AI systems. AKS provides a rich environment for composing services into applications using a micro-service architecture that enables managing a container separately.\nThese containerized instances operate in a very similar way to the Cognitive Services cloud APIs running in the hosted Azure endpoint. This means you can use the same API’s and samples for details on how to use the service regardless of whether you’re calling the container or the Cognitive Services cloud.\nIn this preview we’d also like to learn more about how you’d like to use the containers. To keep things simple, the billing and business model for Cognitive Services in containers is exactly the same as it would be if you were using the Cognitive Services cloud. For example, 1,000 API calls to do Key Phrase Extraction will cost the same regardless of whether you’re calling the container or the Cognitive Services cloud. To enable this, the containers must have the ability to connect to Azure both at start-up and then again at regular intervals while they’re running. Note this will only send metering information to Azure.  None of your customer data needs to go to Azure. If you need a fully disconnected solution, we’d love to hear from you to learn more about your use case as we’re interested in helping with those projects as well.\nCustom Vision Service support for logo detection\nOn a separate but related note, I also wanted to share with you that starting today, Custom Vision Service will add support for logo detection, allowing business to create their own logo detector quickly and easily. Logo detection is a specialized type of object detection suited specifically for logos that can be small, skewed, or obfuscated within a larger picture, for example on the sidelines of a soccer match, on a building sign in a cityscape, or on a scanned form. Now you can build your own logo detectors to help search and locate their logos in their media libraries or to generate analytics for their social media feeds.\n\nGet started today and take advantage of Azure Cognitive Services to build and run the intelligent applications that power your business. For more information from our engineers, join our podcast.\n--Lance\n", "link": "https://azure.microsoft.com/en-us/blog/getting-started-with-azure-cognitive-services-in-containers/", "Role": "Director of Program Management, Applied AI"},
{"Title": "Running Cognitive Service containers", "Date": "Posted on November 26, 2018", "Contributor": "Henrik Frystyk Nielsen", "Content": "\nLast week we announced a preview of Docker support for Microsoft Azure Cognitive Services with an initial set of containers ranging from Computer Vision and Face, to Text Analytics. Here we will focus on trying things out, firing up a cognitive service container, and seeing what it can do. For more details on which containers are available and what they offer, read the blog post “Getting started with these Azure Cognitive Service Containers.”\nInstalling Docker\nYou can run docker in many contexts, and for production environments you will definitely want to look at Azure Kubernetes Service (AKS) or Azure Service Fabric. In subsequent blogs we will dive into doing this in detail, but for now all we want to do is fire up a container on a local dev-box which works great for dev/test scenarios.\nYou can run Docker desktop on most dev-boxes, just download and follow the instructions. Once installed, make sure that Docker is configured to have at least 4G of RAM (one CPU is sufficient). In Docker for Windows it should look something like this:\n\nGetting the images\nThe Text Analytics images are available directly from Docker Hub as follows:\n\nKey phrase extraction extracts key talking points and highlights in text either from English, German, Spanish, or Japanese.\nLanguage detection detects the natural language of text with a total of 120 languages supported.\nSentiment analysis detects the level of positive or negative sentiment for input text using a confidence score across a variety of languages.\n\nFor the text and Recognize Text images, you need to sign up for the preview to get access:\n\nFace detection and recognition detects human faces in images as well as identifying attributes including face landmarks (nose, eyes, and more), gender, age, and other machine-predicted facial features. In addition to detection, this feature can check to see if two people in an image or images are the same by using a confidence score. It can compare it against a database to see if a similar-looking or identical face already exists, and it can also organize similar faces into groups using shared visual traits.\nRecognize Text detects text in an image using optical character recognition (OCR) and extracts the recognized words into a machine-readable character stream.\n\nHere we are using the language detection image, but the other images work the same way. To download the image, run docker pull:\n\r\ndocker pull mcr.microsoft.com/azure-cognitive-services/language\nYou can also run docker pull to check for updated images.\nProvisioning a Cognitive Service\nNow you have the image locally, but in order to run a container you need to get a valid API key and billing endpoints, then pass them as command line arguments. First, go to the Azure portal and open the Cognitive Services blade. If you don’t have a Cognitive Service that matches the container, in this case a Text Analytics service, then select add and create one. It should look something like this:\n\nOnce you have a Cognitive Service then get the endpoint and API key, you’ll need this to fire up the container:\n\nThe endpoint is strictly used for billing only, no customer data ever flows that way.\nRunning a container\nTo fire up the container, you use the docker run command to pass the required docker options and image arguments:\n\r\ndocker run --rm -it -p 5000:5000 mcr.microsoft.com/azure-cognitive-services/language eula=accept apikey=<apikey> billing=<endpoint>\nThe values for the API key and billing arguments come directly from the Azure portal as seen above. There are lots of Docker options that you can use, so we encourage you to check out the documentation. Likewise, there are lot of container arguments that control logging and lots of other features, to learn more check out the documentation.\nIf you need to configure an HTTP proxy for making outbound requests then you can do that using these two arguments:\n\nHTTP_PROXY – the proxy to use, e.g. https://proxy:8888\nHTTP_PROXY_CREDS – any credentials needed to authenticate against the proxy, e.g. username:password.\n\nWhen running you should see something like this:\n\nTrying it out\nIn the console window you can see that the container is listening on https://localhost:5000 so let’s open your favorite browser and point it to that.\n\nNow, select Service API Description or jump directly to https://localhost:5000/swagger. This will give you a detailed description of the API.\n\nSelect Try it out and then Execute, you can change the input value as you like.\n\nThe result will show up further down on the page and should look something like the following image:\n\nYou are now up and running! You can play around with the swagger UX and try out various scenarios. In our blogs to follow, we will be looking at additional aspects of consuming the API from an application as well as configuring, deploying, and monitoring containers. Have fun!\n", "link": "https://azure.microsoft.com/en-us/blog/running-cognitive-service-containers/", "Role": "Principal Architect, Applied AI"},
{"Title": "Getting started with Cognitive Services Language Understanding container", "Date": "Posted on December 4, 2018", "Contributor": "Henrik Frystyk Nielsen", "Content": "\nThis post is co-authored by Phani Mutyala, Senior Program Manager, Applied AI.\nWe recently announced a preview of Docker support for Microsoft Azure Cognitive Services with an initial set of containers covering Computer Vision, Face API, and Text Analytics. Today, we are happy to add support for our Language Understanding service. Language Understanding applies custom machine learning intelligence to a user’s conversational and natural language text to predict overall meaning and pull out relevant and detailed information. Language Understanding can be used to build conversational applications that communicate with users in natural language to complete a task.\nRunning Language Understanding in a container solves a few key problems AI developers are currently experiencing. One of those issues has to do with controlling how and where their data is used, either locally, in the cloud, or on premises. This kind of flexibility is really useful to many businesses we talk with everyday.\nAnother benefit is controlling the scaling, whether that’s scaling up or scaling down, which is especially important when AI models are being updated on a regular basis. By controlling when you scale, you plan for the right bandwidth based on your needs. Therefore, you can run the AI right next to your application logic and be very fast and scalable, all with the reliability and quality that a container provides.\nIn this blog, we describe how to get started with Language Understanding running in a Docker container on your local dev box. If you are new to Docker and need help getting set up on a local machine, please read the previously published blog post, “Running Cognitive Service containers.” You can also find much more how-to information on the documentation, as well as samples for how to use Cognitive Service containers.\nGetting the Language Understanding Docker image\nThe Language Understanding Docker image is available directly from Docker Hub and to download it you just run docker pull:\n\r\ndocker pull mcr.microsoft.com/azure-cognitive-services/luis\nYou can also use docker pull to check for updated images.\nProvisioning a Language Understanding service\nAs for the other Cognitive Services containers, to run a Language Understanding container locally, you need to provision a Language Understanding service in the Azure portal to get a valid API Key and billing endpoint. These values must be passed as command line arguments when you start the container. If you don’t have a Language Understanding service already, then open the Cognitive Services blade, then select Add, and create one. You can get the API Key and endpoint from the Getting Started page or the Overview page. In this case, we get it from the Getting Started page:\n\nGetting the Language Understanding model\nLanguage Understanding allows you to create a language model, also known as the Language Understanding app. This app is tailored to a specific area or domain that you want to cover. For example, you might want to build an application that knows about ordering milkshakes, in which case flavor, toppings, and size might be concepts you want to handle. We won’t dive into building a Language Understanding app here, but feel free to check out the many tutorials in the documentation, for example, “Build custom app to determine user intentions.”\nHere, we simply use an empty app without any intents or entities. To create an empty Language Understanding app, go to the Language Understanding portal and create an app. It should look like this:\n\nOnce you have the empty Language Understanding app, select train and then publish to make the model available for download:\n\nWith the Language Understanding app created, you will need to download it so that you can use it with the local Language Understanding container. To do that, go to My Apps in the  Language Understanding portal, select the empty Language Understanding app, and select Export/Export for container (GZIP):\n\nCreate an empty folder in your root directory called input and copy the Language Understanding app file to that folder. If you are on a Windows machine, then it will look something like this:\n\nRunning the Language Understanding container\nNow we are ready to fire up the local Language Understanding container using docker run. The special thing here is to mount the input folder so that the container can read it. To do this we use the  --mount option with docker run. With the folder named C:\\input, the command looks like this:\n\r\ndocker run --rm -it -p 5000:5000 --mount type=bind,src=C:\\input,target=/input mcr.microsoft.com/azure-cognitive-services/luis eula=accept apikey=<apikey> billing=<endpoint>\nThere are lots of other ways to mount folders so check out the Docker options documentation for all the things you can do with Docker, in addition to configuration options available for the Language Understanding container.\nTrying it out\nAs for all Cognitive Service containers you can now point your browser at https://localhost:5000/swagger to inspect the API and to try things out. You can also call the container programmatically. For more information, check out the several samples available on GitHub. By selecting Try it out you get the list of parameters needed to submit a local request to the container.\nFor the App ID you use the GUID part of the Language Understanding app name. In the example above it is the GUID starting with 2ccdc110. Enter some text, such as “Hello!” in the query field and select Execute:\n\nSince the Language Understanding app is empty, we get the None intent back. This means that the app didn’t understand it but now we can go build a better model and try again.\n", "link": "https://azure.microsoft.com/en-us/blog/getting-started-with-cognitive-services-language-understanding-container/", "Role": "Principal Architect, Applied AI"},
{"Title": "Cincinnati Children’s has a great app development experience with Azure services", "Date": "Posted on December 4, 2018", "Contributor": "Rob Caron", "Content": "\nThe healthcare industry has started to embrace mobile apps and cloud technologies, and not just to optimize internal operations. Cincinnati Children’s Hospital Medical Center wants to make visits to hospitals and doctors’ offices easier for patients and families. Cincinnati Children’s serves a diverse population, but people typically have one thing in common—they’re stressed out. Parents are worried about their children, and children can be scared and overwhelmed by all the hustle and bustle around them. So the hospital came up with the idea of a mobile digital concierge who could provide basic information and also answer specific questions about a family’s appointments and the child’s procedures.\nThe Cincinnati Children’s development team didn’t have expertise building mobile apps, but they did have lots of experience developing in .NET. The team also knew it needed a cloud deployment platform that would support the functionality they had in mind and would also grow with them as they expand and scale the app. Cincinnati Children’s was already using Azure DevOps, so it went with Microsoft Visual Studio Tools for Xamarin for cross-platform app development and chose Azure as its cloud platform. Azure offered a wide variety of services they could use without purchasing third-party tools, and it had the capability to interoperate with the hospital’s on-premises systems for electronic health records and parking—a necessity for two of the features planned for the app.\n\nThis was the Cincinnati Children’s development team’s first real step into Azure development, and once they began, everyone got up to speed quickly. The team was churning out ten to fifteen builds a day in App Center and pushing them out to devices for testing. This rapid iteration allowed for very quick feedback and a sense that they were really getting things done. With App Center the team had a build server that it didn’t have to patch and maintain, and that gave team members high confidence that the final builds didn’t have little outlier bugs that might appear on one device and not another—the results were repeatable from device to device. With App Center, developers could focus on development and not on how to get something compiled. This shaved a couple weeks off development time.\nCincinnati Children’s finished the initial pilot project in about three months with a pretty lean team—about six developers total—and then invited patients’ families to participate in beta testing. The hospital named the app Caren and aimed to make its personality part of the app. Developers did that by incorporating a chatbot based on Azure Bot Services and Azure Cognitive Services Language Understanding. If a family can’t find the answers they need in the app, they can type questions into a text box and Caren will reply to them. And if they’re just looking for a quick laugh to destress, Caren can tell them a joke. \nBecause Cincinnati Children’s tied the app to its parking system, parents no longer need to worry about keeping track of parking vouchers during a hospital visit. Caren gives them a QR code they can scan, and it helps them remember their parking place. This may seem like a small thing, but parents report that it really reduces their stress level when they have so much else on their minds. Using Azure made it easy for the hospital to have that sort of hybrid system, with key data stored on-premises and application logic in the cloud.\nCincinnati Children’s uses a hybrid model for interaction with its on-premises electronic health record (EHR) system, as well, so that Caren has access to the most current information about procedures. For families waiting during an outpatient surgery, Caren updates them about when their child will be ready for visitors. This frees parents up to take a walk or get something to drink, knowing that that won’t miss their child’s return to the recovery room.\nThe EHR system has the ability to push out events, and Cincinnati Children’s wanted to be able to respond to those quickly and update app users with new information on same day surgery cases. The hospital solved the challenge using Azure Event Grid, Azure Functions, and Azure Cosmos DB. Caren uses two Azure Functions to process same-day surgery events. The first takes an event from the EHR system and checks to see if it exists in Azure Cosmos DB. If it does exist, the system puts it into an Event Grid payload and puts it on the Event Grid.\n\nFigure 1: Overview architecture of the Caren infrastructure and its use of Azure services.\nThe second Azure Function pulls the event off of the grid and queries the EHR system for full details, processes the data, and updates Azure Cosmos DB with the latest information. From there, the system generates a push notification to the mobile device that there’s new data. When the device calls back into the Azure API hosted in Azure Kubernetes Service (AKS), the system returns the latest surgical update. Cincinnati Children’s knew that once it started scaling out the Caren deployment, there would be more hits against the API, so developers had the idea to Dockerize them and put them in Kubernetes—that way the infrastructure team can manage the performance and scaling of the containers, and developers just need to focus on the code.\n\n\nFigure 2: A Caren Azure function that pulls an event off the grid, queries the EHR system for full details, processes the data, and then updates Azure Cosmos DB with the latest information.\nDevelopers found that using the Microsoft chatbot SDK and code samples made it really easy to get going with the chatbot portion of Caren, because they didn’t have to worry about building everything from the ground up. All they had to do was program the intents, and Language Understanding service took care of the rest. According to the developers, it was very straightforward to bring Language Understanding service into the bot and they’ve been excited to see people’s reaction to the natural language interface.\nThrough the bot, parents can cancel visits, notify the hospital they’re running late, and find out whether they should take their child to urgent care or the emergency room based on symptoms. It’s all about making life a little easier for these parents. Language Understanding service also logs the questions parents ask Caren that she can’t answer definitively. Cincinnati Children’s goes through that list and figures out what the responses should be and trains the models accordingly. That way the bot continues to improve over time. Using Language Understanding service, developers were able to give Caren a personality, so it’s more than just a generic question-and-answer bot.\nThe development team found the Caren project to be an interesting change of pace—being at a hospital, most of the projects they work on are for clinicians and internal business use, so it was novel to work on a mobile app for patients and their families. They also had the chance to work with new technologies, like building Azure Functions and incorporating Azure Cosmos DB. Privacy and security are a key part of any medical app, and Cincinnati Children’s knew that Microsoft has invested heavily security and regulatory compliance, so as long as developers stayed within the boundaries of Azure, they could trust that data would be safe.\nSome people think the healthcare industry has lagged behind in terms of technology. But this type of project—using modern tools and modern practices—should open their eyes to see that healthcare can be on the cutting edge and can even contribute to moving that cutting edge forward. At Cincinnati Children’s, developers are looking forward to expanding their team and their efforts to deliver on ideas that contribute to the community and the welfare of children everywhere.\n\r\nRead the case study for more about Cincinnati Children’s and its interactive app for families.\n \n", "link": "https://azure.microsoft.com/en-us/blog/cincinnati-children-s-has-a-great-app-development-experience-with-azure-services/", "Role": "Sr. Product Marketing Manager, Microsoft Azure"},
{"Title": "How pharma sales operations benefit from centralizing data and process integration", "Date": "Posted on December 6, 2018", "Contributor": "David Houlding", "Content": "\n\nPharmaceutical companies need to meet demanding sales goals, manage intricate regulatory compliance, and maintain a competitive hold on the market. However, current sales force automation (SFA) solutions for the life sciences industry are focused primarily on sales reps, which leaves a large capability gap for sales operations departments and inhibits their ability to support the sales process.\nPrescriber360 is a Microsoft Gold Partner with a comprehensive Pharma SalesOps solution designed specifically for the life sciences industry that can reduce, and even close, the capability gap.\nProblem: Data spread out and unavailable\nPharma and biotech industries rely heavily on both internal and external data sources. However, most sales operations processes are currently performed manually in spreadsheets and without a centralized system of record. This makes it incredibly difficult to access and leverage that data.\nThe following problems often result:\n\nSales teams built around multiple therapeutic areas and prescriber specialties present a complex targeted environment requiring powerful systems and processes.\nQuarterly alignment processes require touching an entire customer universe, and it takes a lot of effort to update an SFA system with new information, maintain a history of changes to support incentive compensation (IC), and preview the impact of alignments on the sales force. In addition, a data change request (DCR) process causes incremental alignments, which need to follow the same rules or in some cases make overwrites.\nRep rosters are essential to the smooth working of pharma sales operations, but a systems perspective often overlooks them. A central roster system can produce huge productivity gains and data integrity improvements across systems.\n\nThe solution: Prescriber360’s sales operations software platform\nThe Prescriber360 sales operations software platform provides these valuable capabilities:\n\nKeeps up with constantly changing customer master data, validating compliance-approved prescriber specialties and state licenses.\nProcesses quarterly alignments on the sales operations platform, letting organizations load alignments before an IC period starts and they can view field impact.\nProcesses data change requests initiated by the SFA system and governance workflow to make sure that prescribers have a valid specialty, address, and alignment.\nGives sales teams, including reps, district managers, and regional sales directors, easy-to-use reporting, including weekly/monthly data, change requests, quota attainment, and baseline reports.\nEases the burden on sales operations teams caused by quarterly IC processes for analytics. The solution helps to maintain a balance between providing accurate demographic data to analytics for future quarter planning, while supporting the field with needed changes of information.\nUsers are authenticated through Azure Active Directory (AAD), which is built on the Azure cloud. Data stored in Azure Blob storage or Azure SQL Database is encrypted at rest and in-transit. Power BI also relies on AAD for authentication and authorization.\n\nPrescriber360 offers comprehensive capabilities across an organization.\n\nBenefits of an integrated sales ops platform\nManaging all sales operations processes in one convenient place with Prescriber360 gives organizations many tangible benefits:\n\nOwn all new and existing data, maintaining control and making changes as needed.\nTake advantage of machine learning, artificial intelligence, chatbots, and cloud computing technology to separate yourself from competitors.\nUse prebuilt integrations with other major vendors to provide rapid implementation, cost savings, and reliability.\nMake Microsoft technology part of daily operations as a line of business (LOB) system.\n\nAzure services that make a difference\nThis platform is built entirely using Microsoft Azure, Dynamics 365, and Power BI:\n\nSQL Server on Virtual Machines for hyper-scalability and a secured cloud database\nCosmos DB for a globally distributed, multi-model database\nAzure machine learning for advanced analytics\nMicrosoft cognitive services APIs for vision, search, and knowledge\nPower BI for a cloud-based reporting solution\nDynamics 365 for providing an operational customer master data (handling data synchronization between field and support) and a workflow engine\n\nNext steps\nRead about the Prescriber360 SalesOps solution to get more information. Then click the “Contact Me” button on the same page to have someone from Prescriber360 contact you about their solution.\n", "link": "https://azure.microsoft.com/en-us/blog/how-pharma-sales-operations-benefit-from-centralizing-data-and-process-integration/", "Role": "Principal Healthcare Program Manager, Industry Experiences"},
{"Title": "How news platforms can improve uptake with Microsoft Azure’s Video AI service", "Date": "Posted on December 12, 2018", "Contributor": "Anna Thomas", "Content": "\nI’m Anna Thomas, an Applied Data Scientist within Microsoft Engineering. My goals are to enable the field and partners to better integrate various AI tools into their applications. Recently, my team reached out to Microsoft News to see how they’re analyzing their data, and how our services may be able to help.\nMicrosoft News ingests more than 100,000 articles and videos every day from various news providers. With so many different aspects such as classifying news topics, tagging and translating content, I was immediately interested in understanding how they process all of that information.\nAs it turns out, Microsoft News has been working on some pretty advanced algorithms that analyze their articles and determine how to increase personalization, which ultimately increases consumption, for years. However, when I asked them if there were any gaps, they were quick to answer that they would love more insight on their videos.\nAnalyzing videos at scale to obtain insights is a nontrivial task. Having insights on videos, especially for a news platform, can help with increasing search quality, user engagement through personalization, and the accessibility of videos through captioning, translating, and more. There are so many different aspects related to this: classifying different news topics (potentially even opinion detection/provider authority/sentiment), tagging various content, translating content, summarizing content, grouping similar content together, etc.\nExploring Video Indexer\nI set off to determine how we could meet the requested requirements from Microsoft News by using what I thought would be a combination of Video Indexer, Cognitive Services (Text Analytics, Language Understanding, Computer Vision, Face, Content Moderator, and more), and maybe even some custom solutions.\nHere are the results of my research: \n\n\n\n\nRequested feature\n\n\nService that could help\n\n\n\n\nSpeech-to-text extraction\n\n\nVideo Indexer API\n\n\n\n\nSubtitling and captions for browsing the video\n\n\nVideo Indexer API\n\n\n\n\nFor profane language detection\n\n\nVideo Indexer API\n\n\n\n\nVisual profanity detection – moderation use case\n\n\nVideo Indexer API\n\n\n\n\nTopic identification – based on OCR and transcript – classification use case \n\n\nVideo Indexer API\n\n\n\n\nAbility to extract all frames and detect salient frames from keyframes using vision services (no reprocessing expected)\n\n\nVideo Indexer API\n\n\n\n\nTo choose the best frame for the promo card image\n\n\nVideo Indexer API\n\n\n\n\nCould use to enhance the site with filmstrip for users navigating videos\n\n\nVideo Indexer API\n\n\n\n\nKeywords/Labels/Entity extraction (Bing/Satori IDs) – based on vision and text\n\n\nVideo Indexer API\n\n\n\n\nI must admit I was surprised – the Video Indexer API does an impressive job of combining the various insights that other Cognitive Service APIs could give you, such as profane language detection from the Content Moderator API or Speech-to-text from the Speech API. You can see the full list of features in the documentation.\nIntrigued by how much the Video Indexer API claimed to be capable of, I decided to check it out myself. It was pretty easy to get a sample created (you can check out my sample on GitHub) in C# to create a simple console application to bulk upload and process Microsoft News videos.\nFor the purposes of this blog, I wanted to share the results from one video, an episode I did with the AI Show a while back on Bots and Cognitive Services. Since it’s only one video, I decided to use the Video Indexer portal, which has a simple UI for uploading and processing videos. The video is about 12 minutes long, and it took about 4 minutes to process (I checked with the team, and they reported you can expect it to take 30-50 percent of the timed length of the video to process, so this number seems about right).\nOnce the video was processed, I was provided with a link to a widget that I could use to embed my video with the enhanced filmstrip in a website. I’m also able to download all the insights as a JSON file. I’m able to change the language of the insights and transcripts and even just download the transcript.\nBelow, you’re able to see who was in the video. And, if Rodrigo Souza and I happened to be famous (apparently we’re not and now it’s proven!), Bing would return our names and the beginning of our Wikipedia biographies.\n\nYou can see that with the widget, I can actually skip through to clips when Rodrigo is the focus and talking. There are also keywords, and similar to as with clips of people, I can skip through the video to hear the clips containing different topics.\n\nVideo Indexer creates several other enhanced filmstrips that I can use to search the video, including visual labels, brands present, emotions, and keyframes. Video Indexer also gives the option to layer the transcript. Here, I’ve chosen to include people and the transcript.\n\nI can also decide that I want all the insights or the layered transcript in another language. See a snippet of the same layered transcript in Spanish below.\n\nVideo Indexer makes it really easy to unlock insights from videos. While this specific scenario shows how valuable using Video Indexer for news can be, I think that using Video Indexer is relevant in other scenarios as well. Here are a few other ideas of how and where:\n\n\nIn large enterprises, there are tons of documents (and videos!) circulating the intranet. Videos could be related to sales, marketing, engineering, learning, and more. As an employee, it can be hard to find the videos that you need. In efforts to reuse IP and increase the accessibility of existing materials, you could use the Video Indexer (which has Azure Search built-in) to create an enhanced pipeline for searching videos.\n\n\nTaking the previous example one step further, you could create a custom skill in Azure Cognitive Search to get insights from videos via an index on a schedule. You may also choose to use Predefined Skills to get insights on other types of documents (i.e., images, PDFs, PowerPoints, etc.). By using Azure Search to configure an enhanced indexing pipeline, you can provide users with access to an intelligent search service for all documents. Learn more about Cognitive Search in this LearnAI workshop.\n\n\nIn the education space, sometimes a student or researcher could spend a very long time trying to find the answer to a specific question, from a lecture or presentation. With the help of Video Indexer, finding videos that contain certain keywords or topics can become easier. To take it one step further, you could use the insights obtained from the Video Indexer API to create a transfer learning model that can perform Machine Reading Comprehension (basically training a model to retrieve the answer to a question a student/researcher may have).\n\n\nHave other use-cases or tips for using Video Indexer or Azure Cognitive Services? Reach out, and together we can continue to democratize AI. Follow me on Twitter and LinkedIn.\n", "link": "https://azure.microsoft.com/en-us/blog/how-news-platforms-can-improve-uptake-with-microsoft-azure-video-ai-service/", "Role": "Data & Applied Scientist, Engineering"},
{"Title": "Microsoft previews neural network text-to-speech", "Date": "Posted on December 13, 2018", "Contributor": "Xuedong Huang", "Content": "\nApplying the latest in deep learning innovation, Speech Service, part of Azure Cognitive Services now offers a neural network-powered text-to-speech capability. Access the preview available today.\nNeural Text-to-Speech makes the voices of your apps nearly indistinguishable from the voices of people. Use it to make conversations with chatbots and virtual assistants more natural and engaging, to convert digital texts such as e-books into audiobooks and to upgrade in-car navigation systems with natural voice experiences and more.\nThis release includes significant enhancements since we first revealed Neural Text-to-Speech at Ignite earlier this year.\nEnhanced voice quality\nThe voices sound more robust and natural across a wider variety of user scenarios, achieved by harnessing the following:\n\nA large supervised training with transfer learning across diverse speakers\nMore features from unsupervised pretraining\nAdded robust neural model design \n\nAccelerated runtime performance\nRuntime performance of the Neural Text-to-Speech engine is near-instantaneous through extensive code optimization with hardware accelerators, applying parallel inference models and model simplifications considering the balance of sound quality and performance. The real-time factor has been improved from the previous version to less than 0.05X, meaning 1 second of audio can be generated in less than 50 milliseconds. Producing the first byte of audio now runs 6 times faster than before.\nGreater service availability\nNeural Text-to-Speech has since expanded to three datacenters across the US, Europe, and Asia. Wherever you are in the world, you can integrate neural voices with reduced latency overhead.\n \nWith these updates, Speech Services Neural Text-to-Speech capability offers the most natural-sounding voice experience for your users in comparison to the traditional and hybrid system approaches.\nYou can use this capability starting today with two pre-built neural voices in English – meet Jessa and Guy. Hear what they sound like.\nDiscounts are available during the preview. Visit the Speech Services pricing page for more details.\nIf you would like to access this capability in Chinese or German, please submit your request.\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-previews-neural-network-text-to-speech/", "Role": "Technical Fellow, Cloud and AI"},
{"Title": "Speech Devices SDK and Dev Kits news (August 2018)", "Date": "Posted on August 9, 2018", "Contributor": "Grace Sturman", "Content": "\nWe have just released the v0.5.0 version of the Speech Devices SDK to its download site a few days ago. If you want access to it, please apply for it via the Microsoft Speech Devices SDK Sign Up Form.\nPlease use the latest version of the Speech Devices SDK and its matching sample app. The Speech Devices SDK consumes the Speech SDK. In addition, it uses an advanced audio processing algorithm and enables a custom Key Word Spotting (KWS) feature. The Speech Devices SDK's version matches the version of the Speech SDK, so that all the APIs are consistent. Right now we only have support for Java, see the Java API reference. Please see the Speech Devices SDK’s release notes for details. We currently have one developer centric Java sample app. The source code can be found posted in the GitHub repository’s under the Samples/Android example. We will post additional Java sample apps as they become available.\nThe microphone array dev kits from our hardware provider Roobo, have also gone on sale recently. Please visit ROOBO Dev Kits for Microsoft Speech Services for the hardware specs and product details. If you have questions regarding the device hardware, including ordering and shipping, please contact Roobo at rooboddk@roobo.com. Currently, they can only be shipped to the U.S. and China.\nPlease also read the LinkedIn article about the dev kits by Xuedong Huang (technical fellow) of Microsoft Cloud & AI group.\n\nWe can’t wait to see the cool devices and applications that you will build with the Microsoft Speech Devices SDK and the Roobo Smart Audio Dev Kits!\nFor Speech Devices SDK or Speech Services questions, please visit our support page.\n", "link": "https://azure.microsoft.com/en-us/blog/speech-devices-sdk-and-dev-kits-news-august-2018/", "Role": "Senior Program Manager, Speech Services"},
{"Title": "Logic Apps, Flow connectors will make Automating Video Indexer simpler than ever", "Date": "Posted on August 21, 2018", "Contributor": "Uri Greenberg", "Content": "\nUpdate May 7, 2020: For the most up to date information on the Video Indexer Logic App and Power Automate connectors, please see our Tutorial: Use Video Indexer with Logic App and Power Automate documentation.\n \nVideo Indexer recently released a new and improved Video Indexer V2 API. This RESTful API supports both server-to-server and client-to-server communication and enables Video Indexer users to integrate video and audio insights easily into their application logic, unlocking new experiences and monetization opportunities.\nTo make the integration even easier, we also added new Logic Apps and Flow connectors that are compatible with the new API. Using the new connectors, you can now set up custom workflows to effectively index and extract insights from a large amount of video and audio files, without writing a single line of code! Furthermore, using the connectors for your integration gives you better visibility on the health of your flow and an easy way to debug it. \nTo help you get started quickly with the new connectors, we’ve added Microsoft Flow templates that use the new connectors to automate extraction of insights from videos. In this blog, we will walk you through those example templates.\nUpload and index your video automatically\nThis scenario is comprised of two different flows that work together. The first flow is triggered when a new file is added to a designated folder in a OneDrive account. It uploads the new file to Video Indexer with a callback URL to send a notification once the indexing operation completes. The second flow is triggered based on the callback URL and saves the extracted insights back to a JSON file in OneDrive. The reason that two flows are used is to support async upload and indexing of larger files effectively.\nSetting up the file upload flow\nNavigate to the first template page.\nTo set up this flow, you will need to provide your Video Indexer API Key and OneDrive credentials.\n\nOnce both keys are provided, green marks will appear near your accounts, and you can click to continue to the flow itself and configure it for your needs:\nSelect a folder that you will place videos in:\n\nFill in your account Location and ID to get the Video Indexer account token and call the file upload request.\n\nFor file upload, you can decide to use the default values, or click on the connector to add additional settings. Notice that you will leave the callback URL empty for now … you’ll add it only after finishing the second template, where the callback URL is created).\n\nClick “Save flow,” and let’s move on to configure the second flow, to extract the insights once the upload completed.\nSetting up the JSON extraction flow\nNavigate to the second template page.\nTo set up this flow, you will need to provide your Video Indexer API Key and OneDrive credentials. You will need to update the same parameters as you did for the first flow.\n\nThen, continue to configure the flow.\nFill in your account Location and ID to get your Video Indexer Account Token and the Indexing result.\n\n\nAnd select the OneDrive folder to save the insights to. You can also edit the default parameter, if you would like to change the name of the JSON file containing the insights.\n\nClick “Save flow.”\nOnce the flow is saved, a URL is created in the trigger. Copy the URL from the trigger:\n\nNow, go back to the first flow created and paste the URL in the “Upload video and index” operation under the Callback URL parameter:\n\nMake sure both templates are saved, and you’re good to go!\nTry out your newly created flow by adding a video to your OneDrive folder, and go back a few minutes later to see that the insights appear in the destination folder.\nEndless integration possibilities\nAnd this is just one example! You can use the new connector for any API call provided by Video Indexer. to upload and retrieve insights, translate the results, get delightful embeddable widgets and even customize your models. Additionally, you can choose to trigger those actions based on different sources like updates to file repositories or emails sent, and to have the results update to our relevant infrastructure or application, or even to generate any number of action items. And you can do all that for large number of files and without coding or having to do a repetitive manual work. Go ahead and try it now for any flow that works for your business needs, it's easy.\nHave questions or feedback? We would love to hear from you!\nUse our UserVoice to help us prioritize features, or email VISupport@Microsoft.com for any question.\n", "link": "https://azure.microsoft.com/en-us/blog/logic-apps-flow-connectors-will-make-automating-video-indexer-simpler-than-ever/", "Role": "Video Indexer SDE"},
{"Title": "Speech Services August 2018 update", "Date": "Posted on August 23, 2018", "Contributor": "Grace Sturman", "Content": "\nWe are pleased to announce the release of another update to the Cognitive Services Speech SDK (version 0.6.0). With this release, we have added the support for Java on Windows 10 (x64) and Linux (x64). We are also extending the support for .NET Standard 2.0 to the Linux platform. The changes are highlighted in the table below. The sample section of the SDK has been updated with samples showcasing the use of the newly supported languages. The UWP support was added in the Speech SDK version 0.5.0 release; and starting from now the UWP apps built with the Speech SDK can be published to the Microsoft Store.\n\nWe also included several bug fixes which were reported by early adopters. Most notable this should fix errors in long-running speech transcriptions, as well as reducing the amount of in-use socket connections and threads.\nOther functional changes, breaking changes and bug fixes can be found in the Speech SDK’s release notes. For questions regarding Speech SDK and Speech Services, please visit our support page.\nThere are also changes that impact the Speech Devices SDK. To provide a little bit of the background, the Speech Devices SDK is for our devices solution. It consumes the Speech SDK, and uses the Speech SDK to send the audio to the Speech Service for speech recognition. The Speech Devices SDK also has the advanced audio processing algorithm that’s fined tuned to the Roobo dev kits to significantly improve the audio quality of the audio data collected via the dev kits’ microphones, for high speech recognition accuracy.\nThe custom keyword spotting feature is only available for the Speech Devices SDK. In this release, the custom keyword spotting feature’s KeywordRecognitionModel function now supports fromFile(), and fromStream() in the zip format, that contains the kws.table as well as (optional) keyword adaptation files. We generally release an update to the Speech Devices SDK shortly after the release of the Speech SDK. For any update specific to the Speech Devices SDK, please visit its Speech Devces SDK’s release notes.\nPlease let us know if you have questions, by visiting our support page.\n", "link": "https://azure.microsoft.com/en-us/blog/speech-services-august-2018-update/", "Role": "Senior Program Manager, Speech Services"},
{"Title": "Global scale AI with Azure Cognitive Services", "Date": "Posted on September 24, 2018", "Contributor": "Lance Olson", "Content": "\nTo build an effective and scalable solution, developers need technology that can be deployed around the world and still provide results with high confidence. To that end, we’ve spent the last year investing in making our Cognitive Services enterprise-ready and bringing them to general availability, ready for production use. Cognitive Services are a set of intelligent APIs and services that are used by more than 1.2 million developers and thousands of businesses throughout 150 countries across every industry from retail to healthcare to public sector to manufacturing and non-profit organizations.\nWe’ve deployed more services into the Azure data centers around the world, written more documentation in multiple developer languages, re-architected products to change the way we store and retain data in order to give controls to users over their data, adhering to the highest standards available. We’ve localized our services into multiple languages across the globe with over 10 of them now available in 15 languages. All while meeting strict SLA standards that we require for every Azure service. And we’re not stopping there, our work continues.\nJust recently we’ve refactored our speech services and are launching a single unified speech service accessible via one endpoint to enable high speed (low latency) speech interactions. We expect that this will enable you to be more successful meeting your business needs with speech.\nHere’s specifics on what we’re launching:\n\nSpeech service – A single endpoint for all Azure Cognitive Services speech capabilities. Use one Azure key to access all speech services around the world.\nGenerally available versions include: \r\n \nSpeech to Text – Benefit from updated and improved models, deployed to new Azure data centers and full compliance with ISO, SOC and HIPAA, as well as 17 new languages for a total of 30 languages available now. Customize your models and boost accuracy right within the service. For call centers, you can count on high-quality transcriptions.\nText to Speech – Create an end-to-end speech recognition app for global audiences.\nSpeech Translation – For customer support bots, make a request in your language and have it translated into different languages.\nCustom Voice – For conversational AI, use your own voice with our custom voice font in your preferred language.\n\n\nNatural Digital Voice in preview – For those early adopters wanting the most advanced technology, check out our natural digital voice, or Neural Text-to-Speech (TTS) service. Neural TTS is ground-breaking technology that makes speech synthesis sound natural and nearly indistinguishable from human recordings. By using Microsoft Neural TTS, you will see significantly higher quality and accuracy as well as making speech synthesis closer to human parity. If you are interested in the preview of Neural TTS, please submit a request.\nSpeech Devices SDK in public preview lets you optimize audio capturing on your own hardware that connects to our speech services.\n\nKnowledge mining\nAnother exciting area we’re investing in is Cognitive Search. Our customers today have expressed the need to optimize workflows, reduce manual labor inefficiencies, process large volumes of complex data, and derive insights across siloed environments quickly and accurately. We’re building systems that will automatically do this across all your data regardless of format, so you can easily gain new insight from your data.\n\nWith Cognitive Search, we combine the power of Azure Search with Cognitive Services such as Text Analytics and Computer Vision to analyze and identify the important entities within your documents. You can also extend the platform by bringing in your own Custom AI skills to enrich your data and extract additional business-specific insights. The solution is already used in large enterprises of all industries to improve understanding and processing of large documents with greater speed and precision.\nSince we first made this capability available in preview in May we’ve seen thousands of developers in over 100 countries use it, running millions of AI skills over their data to extract knowledge and gain a deeper understanding of their business.\nWe think that AI at scale is the future and we’re investing in enabling that for you. I invite you to learn more about Cognitive Services here and don’t hesitate to let me know if you have any questions about building your own AI system using Azure.  Find out more about Cognitive Services and try them out yourself.\n", "link": "https://azure.microsoft.com/en-us/blog/global-scale-ai-with-azure-cognitive-services/", "Role": "Director of Program Management, Applied AI"},
{"Title": "Azure AI – Making AI real for business", "Date": "Posted on September 24, 2018", "Contributor": "Eric  Boyd", "Content": "\nAI, data and cloud are ushering the next wave of transformative innovations across industries. With Azure AI, our goal is to empower organizations to apply AI across the spectrum of their business to engage customers, empower employees, optimize operations and transform products. We see customers using Azure AI to derive tangible benefits across three key solution areas. \n\nFirst, using machine learning to build predictive models that optimize business processes.\nSecond, building AI powered apps and agents to deliver natural user experience by integrating vision, speech and language capabilities into web and mobile apps. \nThird, applying knowledge mining to uncover latent insights from documents.  \n\nToday, at Microsoft Ignite, we are excited to announce a range of innovations across these areas to make Azure the best place for AI. Let me walk you through them.\nMachine Learning \nFrom pre-trained models to powerful services to help you build your own models, Azure provides the most comprehensive machine learning platform.\nTo simplify development of speech, vision, and language machine learning solutions, we provide a powerful set of pre-trained models as part of Azure Cognitive Services.  When it comes to building your own deep learning models, in addition to supporting popular frameworks such as PyTorch and TensorFlow, we have worked with Facebook to co-develop ONNX (Open Neural Network Exchange) that enables model interoperability across frameworks. We offer a choice of Machine Learning services with Azure Databricks, Azure Machine Learning and Machine Learning VMs so you can apply machine learning at any scale. To help you build and train models faster, we provide distributed deep learning capabilities over massive GPU clusters. You can also access powerful hardware (FPGA) accelerated models for very high-speed image classification and recognition scenarios at low cost. Once you have built your model, you can deploy on-premises, in the cloud or on the edge, including offline environments.\n\n\nToday, we are announcing several updates to Azure Machine Learning, available in preview.\n\nAutomated machine learning and hyper-parameter tuning to help you accelerate the rate of model development by identifying suitable algorithms, features and ML pipelines faster.\nDistributed deep learning to enable you to develop deep learning solutions faster with massive, managed GPU clusters. \nHardware accelerated inferencing for high speed image classification and recognition scenarios using powerful FPGAs. Supported models include ResNet 50, ResNet 152, VGG-16, SSD-VGG, and DenseNet-121 that can be trained using your data.   \nA new Python SDK to enable you to access Azure Machine Learning from your favorite Python development environment, including Visual Studio Code, Visual Studio, PyCharm, Azure Databricks notebooks, or Jupyter notebooks.\nModel management capabilities to help you manage you manage Dockerized models using models and images registry that integrate into your continuous integration (CI/CD) pipeline.\n\n“By using Azure Machine Learning, we can meet a variety of customer needs, learn from all the analytics, and put that learning into new products and services to improve our offerings”.\n– Rosemary Yeilding Radich Director, Data Science, AccuWeather\nAI apps and agents\nAzure provides a number of services specifically designed to help you build AI powered apps & agents. As outlined in the previous section, Azure Cognitive Services allow you to infuse your apps with powerful pre-trained models that can help with vision, speech, language and web search. We have over 1.2 million developers using Cognitive Services to deliver AI led experiences to their users. We continue to build on the momentum and we are excited to announce the general availability. It combines the following capabilities into one service:\n\n\nSpeech to Text\n\n\nText to Speech\n\n\nCustom Speech\n\n\nSpeech Translation\n\n\nCustom Voice\n\n\nAdditionally, Content Moderator, Computer Vision, Face, Text Analytics, Translator Text, and Language Understanding APIs will be generally available in US Government regions in the next few weeks. \nMicrosoft has reached another AI milestone in text-to-speech synthesis with a system that uses deep neural networks to make the voices of computers nearly indistinguishable from recordings of people. This capability is available in preview through Azure Cognitive Speech Service and you can sign up for access.\nAzure Bot Service helps you build, connect, deploy, and manage intelligent agents to interact naturally with your users in app or on web. Native integration with Cognitive Services, enables you to easily leverage cognitive capabilities to deliver an engaging experience to your users. For example, you can use Language Understanding to better interact with your users by understanding context and continuing a conversation in a very natural manner. The Bot Service leverages the Microsoft Bot Framework, which is a set of open source SDKs for building conversational experiences. Bot Framework also includes an emulator and a set of CLI tools to streamline the creation and management of different bot language understanding services. There are over 340K users of the Bot Framework. Today, we are making Bot Framework v4 generally available, offering broader language support with C# and JavaScript generally available, Python and Java in preview, and better extensibility to benefit from a rich ecosystem of pluggable components like dialog management and machine translation. \nWe have a number of customers such as Progressive, Dixons Carphone, UPS, Progressive and Adobe building AI powered apps & agents.\n\nKnowledge Mining\nOrganizations have valuable information lying latent in historical records across multiple data formats that are difficult to explore such as forms, PDFs and Images. Unfortunately, this information is hidden, not searchable and hence not useful. Azure Cognitive Search (in preview) adds Cognitive Services on top of Azure Search, helping you glean knowledge from vast amounts of data. With Cognitive Search, you can easily add object recognition, computer vision, entity extraction and other cognitive skills to explore various data types and then bring all the output together to uncover opportunities. Since announcing the preview of Cognitive Search at Microsoft Build earlier this year, we have seen rapid adoption, with tens of millions of documents getting processed every month.\n\n“Azure Cognitive Search has helped us deliver an intelligent, easy-to-use platform to our customers that surfaces actionable insights from tons of unstructured data – using entity recognition, language translation and other cognitive capabilities”.\n  – Monish Darda, CTO and co-founder, Icertis\nAs we continue to make Azure the best place for AI, we are most excited to see what customers can do with AI. The opportunities are limitless, and we are looking forward to seeing what you create with Azure AI. Get started today.\nAdditional resources\n\n\nWhat’s New in Azure Machine Learning service\n\n\nAutomated Machine Learning capabilities\n\n\nMilestone in text-to-speech synthesis\n\n\nGlobal scale AI with Azure Cognitive Services\n\n\n", "link": "https://azure.microsoft.com/en-us/blog/azure-ai-making-ai-real-for-business/", "Role": "Corporate Vice President, Azure AI "},
{"Title": "Microsoft’s new neural text-to-speech service helps machines speak like people", "Date": "Posted on September 24, 2018", "Contributor": "Xuedong Huang", "Content": "\nMicrosoft has reached a milestone in text-to-speech synthesis with a production system that uses deep neural networks to make the voices of computers nearly indistinguishable from recordings of people. With the human-like natural prosody and clear articulation of words, Neural TTS has significantly reduced listening fatigue when you interact with AI systems.\nOur team demonstrated our neural-network powered text-to-speech capability at the Microsoft Ignite conference in Orlando, Florida, this week. The capability is currently available in preview through Azure Cognitive Services Speech Services.\nNeural text-to-speech can be used to make interactions with chatbots and virtual assistants more natural and engaging, convert digital texts such as e-books into audiobooks and enhance in-car navigation systems.\nThe milestone in text-to-speech joins a string of breakthroughs that our group has achieved over the past two years, including human parity in conversational speech recognition and human parity in machine translation.\n\nOur text-to-speech capability uses deep neural networks to overcome the limits of traditional text-to-speech systems in matching the patterns of stress and intonation in spoken language, called prosody, and in synthesizing the units of speech into a computer voice.\nTraditional text-to-speech systems break down prosody into separate linguistic analysis and acoustic prediction steps that are governed by independent models. That can result in muffled, buzzy voice synthesis. Our neural capability does prosody prediction and voice synthesis simultaneously. The result is a more fluid and natural-sounding voice.\n\n\n\nSentence\nRecording\nNeural TTS\n\n\nThe third type, a logarithm of the unsigned fold change, is undoubtedly the most tractable.\n\n\n\n\nAs the name suggests, the original submarines came from Yugoslavia.\n\n\n\n\nThis is easy enough if you have an unfinished attic directly above the bathroom.\n\n\n\n\n\nBy using the computational power of Azure, we can deliver real-time streaming, which is useful for situations such as interacting with a chatbot or virtual assistant. The capability is served in the Azure Kubernetes Service. This ensures high scalability and availability and gives customers the ability to use neural text-to-speech and traditional text-to-speech from a single endpoint.\nThe preview service is currently offering two pre-built neural text-to-speech voices in English – Aria and Guy. More languages will be available soon, as well as customization services in 49 languages for customers who want to build branded voices optimized for their specific needs.\nTo learn more, visit us.\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-s-new-neural-text-to-speech-service-helps-machines-speak-like-people/", "Role": "Technical Fellow, Cloud and AI"},
{"Title": "Snip Insights – Cross-platform open source AI tool for intelligent screen capture", "Date": "Posted on October 8, 2018", "Contributor": "Tara Jana", "Content": "\nDevices and technologies are moving forward at a rapid pace, though the everyday tools we use remain relatively unchanged. What if we could infuse AI into everyday tools to delight and inspire developers to do more using Microsoft AI platform? With just a little bit of creativity and using Microsoft's current AI offerings, we can bring AI capabilities closer to customers and create applications that will inspire every organization, every developer, and every person on this planet.\nIntroducing Snip Insights\nAn open source cross-platform AI tool for intelligent screen capture. Snip Insights revolutionizes the way users can generate insights from screen captures. The initial prototype of Snip Insights, built for Windows OS and released at Microsoft Build 2018 in May, was created by Microsoft Garage interns based out of Vancouver, BC. Our team at Microsoft AI Lab in collaboration with the Microsoft AI CTO team took Snip Insights to the next level by giving the tool a new intuitive UX, cross-platform availability (MacOS, Linux, and Windows), and free download and usage under MSA license. Snip Insights leverages Microsoft Azure's Cognitive Services APIs to increase users' productivity by reducing the number of steps needed to gain intelligent insights.\n\nThe Solution\nSnip Insights is an open source desktop utility that enables cross-platform users to retrieve intelligent insights over a snip or screenshot. Screenshots are essentially snapshots of moments. Snip Insights leverages cloud AI services to convert images to translated text, automatically detect and tag image content, along with many smart image suggestions that improve workflows while showcasing Azure Cognitive Services’ potential. By combining a familiar tool with Cognitive Services, we have created a one-stop shop for all your image insights. Imagine that you have a scan of a textbook or work report. Rather than having to manually type out the information, snipping it will return editable text in just one click using OCR. Or maybe you’re scrolling through your social media feed and come across someone wearing a cool pair of shoes, you can simply snip to find out where to purchase them. Snip Insights can show you relevant information based on what you’ve just snipped, including identifying famous people and landmarks. It streamlines the process of screenshotting, saving the picture, uploading it to reverse image search engines, and then drawing results from there.\n\nSnip Insights highlights\nCelebrity Search – Snip an image of a celebrity and the tool will provide you with all the news and information about the celebrity.\n\n\nObject Detection and Bing Visual Search – You like a t-shirt or pair of shoes your friend is wearing in a pic, and want to know where to buy it from. Just use snip insights and it will give you recommendations, similar product images and where to buy from in a matter of a second.\n\nOCR, Language Translation and cross-platform compatibility – You like a phrase in a specific language, let’s say English, and wish to convert that to French or another language to personalize it and vice versa. Just use Snip Insights to get those insights. The tool is free and works on Windows, Linux,  and MacOS.\n\n\n\nSupported Platforms: Windows, Mac OS, and Linux\nThe Snip Insights app is available for three platforms:\n\nUniversal Windows Platform (UWP)\nMac OS\nLinux\n\nXamarin.Forms GTK# App Mac OS and Linux for Snip Insights\nXamarin.Forms enables you to build native UIs for iOS, Android, macOS, Linux, and Windows from a single shared codebase. You can dive into app development with Xamarin.Forms by following our free self-guided learning from Xamarin University.\nXamarin.Forms has preview support for GTK# apps. GTK# is a graphical user interface toolkit that links the GTK+ toolkit and a variety of GNOME libraries, allowing the development of fully native GNONE graphics apps using Mono and .NET. Xamarin.Forms GTK#.\nInstallation\nWindows\n\nDownload the zip.\nInstall the certificate (\".cer\" file) according to the instructions detailed on the tInstall Certificate section.\nInstall Snip Insights by double click over the .appx package file.\n\nLinux\n\nInstall Mono by following the official steps depending on your Linux distribution.\nInstall the .deb package.\nLaunch the app from the applications section.\n\nMac OS\n\nDownload and install Mono (Stable channel).\r\n\r\n \nSuch includes GTK#, the UI toolkit on which Xamarin.Forms relies on this project.\n\n\nInstall the .pckg as a normal macOS application.\nSnip Insights app is available on the Applications section on macOS.\n\nRequirements\nVisual Studio 2017 version 15.8 or Visual Studio for Mac version 7.6.3\nUsing your own subscription\nTo add the keys to Snip Insights, a Microsoft Garage Project, start the application. Once running, click/tap the settings icon in the toolbar. Scroll down until you find the Cognitive Services, enable AI assistance toggle, and toggle it to the on position. You should now see the Insight Service Keys section.\n\nEntity Search - Create new Entity Search Cognitive Service. Once created, you can display the keys. Select one and paste into Settings.\nImage Analysis - In Azure, create a Computer Vision API Cognitive Service and use its key.\nImage Search - In Azure, create a Bing Search v7 API Cognitive Service and use its key.\nText Recognition - You can use the same key as used in Image Analysis. Both Image Analysis and Text Recognition use Computer Vision API.\nTranslator - Use the Translator Text API Cognitive Service.\nContent Moderator - Use the Content Moderator API Cognitive Service.\n\nFor the LUIS App ID and Key, you will need to create a Language Understanding application in the Language Understanding Portal. Use the following steps to create your LUIS App and retrieve an App ID\n\nClick on create new app button.\nProvide an app name. Leave culture (English) and description as defaults.\nClick done.\nIn the left navigation pane, click entities.\nClick manage prebuild entities.\nSelect datetimeV2 and email.\nClick done.\nClick the train button at the top of the page.\nClick the publish tab.\nClick the publish to production slot button.\nAt the bottom of the screen, you will see a list with a Key String field. Click the copy button and paste that key value into the LUIS Key field in settings for Snip Insights.\nClick the settings tab at the top.\nCopy the application ID shown and paste into the LUIS App Id field in Settings for Snip Insights.\n\nYou can now paste each key in the settings panel of the application. Remember to click the save button after entering all the keys.\nFor each key entered there is a corresponding service endpoint. There are some default endpoints included you can use as an example, but when you copy each key also check and replace the service endpoint for each service you are using. You will find the service endpoint for each Cognitive Service on the overview page. Remember to click the save button after updating all the service endpoints.\nCongratulations! You should now have a fully working application to get started. Have fun testing the project and thank you for your contribution!\nYou can find the code, solution development process, and all other details on GitHub.\n", "link": "https://azure.microsoft.com/en-us/blog/snip-insights-cross-platform-open-source-ai-tool-for-intelligent-screen-capture/", "Role": "Sr. Technical Product Marketing Manager, Artificial Intelligence"},
{"Title": "Driving identity security in banking using biometric identification", "Date": "Posted on October 9, 2018", "Contributor": "Howard Bush", "Content": "\nCombining biometric identification with artificial intelligence (AI) enables banks to take a new approach to verifying the digital identity of their prospects and customers. Biometrics is the process by which a person’s unique physical and personal traits are detected and recorded by an electronic device or system as a means of confirm identity. Biometric identifiers are unique to individuals, so they are more reliable in confirming identity than token and knowledge-based methods, such as identity cards and passwords. Biometric identifiers are often categorized as physiological identifiers that are related to a person’s physicality and include fingerprint recognition, hand geometry, odor/scent, iris scans, DNA, palmprint, and facial recognition.\n\nBut how do you ensure the effectiveness of identifying a customer when they are not physically in the presence of the bank employee? As the world of banking continues to go digital, our identity is becoming the key to accessing these services. Regulators require banks to verify that users are who they say they are, not bad actors like fraudsters or known money launderers. And verifying identities online without seeing the person face to face is one of the biggest challenges online and mobile services face today.\nIt’s problematic because identity documents were created to be verified in person. For example, you can shine an infrared light, you can feel the texture, or you can see if a photo has been stuck on. But with remote verification, you’re just dealing with an image. Without the physical artifact, you only have the human eye to rely on and this makes the task of verification much harder to do quickly, or accurately.\nA complicating factor is the online user experience. Users expect a fast, frictionless process in all that they do. If they have to wait, or if the process is too fiddly, they’ll go elsewhere. In the banking sector, almost half of all people who start opening an online bank account drop off due to a bad user experience.\nThis is where identity assurance is needed for online and mobile onboarding processes. Identity assurance is the ability for the bank to determine, with a high level of certainty, that an electronically provided credential representing a person can be trusted to serve as a proxy for that individual and not someone else. Assurance levels (ALs) are levels of trust associated with a credential as measured by the supporting technology, processes, procedures, policies, and operational practices.\nTo facilitate the assurance part of the customer onboarding process, the bank must have an innovative identity verification technology (IVT) to ensure that customers provide information that is associated with the identity of a real person. Physical authenticity identity documents like passports, drivers permits, and other documents are used to compare against government or service databases. Fraudsters continually strive to debunk bank processes to perform account takeovers, system infiltrations, and unauthorized transactions. Fraud detection is tough! In many cases it’s easy to alter content, images, and verification digits of common identification resources.\nTo combat these methods, Microsoft has many partners leveraging our Azure Cognitive Services – Vision API Platform and Azure Machine Learning. One such partner Onfido, provides a multi-factor identity verification service that helps accurately verify online users, uses a cloud-based risk assessment platform that leverages artificial intelligence to automate and scale traditionally human-based fraud expertise to derive identity assurance. The Onfido service validates physical identity documents (document validation), verifies biometric inputs (biometric identity verification), and analyzes information an end user provides about themselves (ID validation). These techniques give companies a measurable assurance that the person is who they say they are.\nBiometric identity verification\nOnfido verifies identities through two factor verification:\n\nSomething you have, such as a government issued ID (driver’s license, passport or ID card).\r\n \nDocument validation answers this question. Is it authentic?\n\n\nSomething you are, such as your facial biometrics.\r\n \nFeature and attribute validation answers these questions. Is there a match, and are they alive?\n\n\nBiometric identity is quite robust, an identity document is the most legally binding proof of identity, while remaining user friendly. The face is the easiest biometric to capture using mobile devices.\r\n \nIn figure below, the first step is to verify that a document is genuine. Onfido has several different algorithms to test for different fraud techniques.\nThe second step is to match the photo on the document with the selfie taken by the customer. Rather than take a static selfie, customers can also choose a video option which asks the user to perform randomised movements such as turning their head and voice commands. This prevents using deceitful practices, impersonation, or spoofing attempts. \n\n\n\n\nFigure 2\nHarnessing the power of Microsoft Azure's Cognitive Services, Onfido helps clients adhere to their due diligence requirements via an effective, compliant and robust digital verification experience.\nTogether, Microsoft and Onfido deliver an easy onboarding experience for users through a scalable and automated process. The solution addresses compliance needs and reduces fraud costs associated with identity theft. This helps our clients to build trust and integrity within their community.\nWant to learn more about combating online and mobile fraud? First, read the Detecting Online and Mobile Fraud with AI use case providing actionable recommendations and solutions. This will provide information on solutions and guidance for you to get started, as well as information on many other partners that also provide identity verification solutions.\nMake sure you also check out more Azure partners on the Azure Marketplace. Then, engage with the author on this topic by reaching out to me via LinkedIn and Twitter.\n", "link": "https://azure.microsoft.com/en-us/blog/driving-identity-security-in-banking-using-biometric-identification/", "Role": "Principal Program Manager, Banking and Capital Markets"},
{"Title": "How developers can get started with building AI applications", "Date": "Posted on October 22, 2018", "Contributor": "Wilson Lee", "Content": "\nThis blog is co-authored by Wee Hyong Tok, Principal Data Scientist Manager, Office of the CTO AI.\nIn recent years, we have seen a leap in practical AI innovations catalyzed by vast amounts of data, the cloud, innovations in algorithms, hardware, and more. So how do developers begin to design AI applications that engage and delight your customers, optimize operations, empower your employees, and transform products?\nUsing Azure Cognitive Services you can now infuse your applications, websites, and bots with intelligent capabilities. These capabilities build on years of research done on vision, speech, knowledge, search, and language. Using different cognitive services, developers can now easily add AI capabilities without training the machine learning models from scratch.\nO’Reilly and Microsoft are excited to bring you a free e-book on AI, titled A Developer’s Guide to Building AI Applications. In this e-book, Anand Raman and Wee Hyong Tok of Microsoft provide a gentle introduction to use Azure AI for building intelligent, AI applications. They provide a practical example of a bot called “Conference Buddy”, that is used by conference attendees. The e-book walks through the use case, the architecture, and how to create the bot while infusing it with AI. The code is made available on GitHub. Specifically, the e-book will help you:\n\nGain an in-depth understanding of the tools, infrastructure, and services that are available on the Azure AI platform.\nGet started with developing an Intelligent Chatbot, with plug and play intelligence that enriches your bot to support engaging experiences.\nLearn about the resources available on AI.\n\n\nGet started today!\n\nDownload A Developer’s Guide to Building AI Applications.\nLearn more about Azure AI.\nTry Cognitive Services APIs for free.\nCreate your Azure free account.\nWatch the Ignite 2018 AI sessions.\n\n", "link": "https://azure.microsoft.com/en-us/blog/how-developers-can-get-started-with-building-ai-applications/", "Role": "Senior Software Engineer, Office of the CTO AI"},
{"Title": "Offline media import for Azure", "Date": "Posted on April 10, 2018", "Contributor": "Dean Paron", "Content": "\nSo many customers I talk to want to upload their offline data stores into the cloud. Yet, no one wants to spend hours and hours inserting tapes, connecting older hard disks, or figuring out how to digitize and upload film. Well, I’m excited to announce that together with our partners Microsoft Azure is making it easy with our Offline Media Import Program. This partner-enabled service makes it easy to move data into Azure from almost any media, such as tapes, optical drives, hard disks, or film. \nWhy migrate your current storage media to Azure? Azure provides a range of flexible storage options from low-cost, archive storage to high-performance, SSD-based storage. You simply choose the storage tier and we take care of the rest. And once the data is available in Azure, higher-value scenarios around analysis, transformation, and distribution can be unlocked. Here are some of the common uses:\nMedia and entertainment\nOffline media import is a great way for entertainment companies to modernize their content assets and take advantage of an array of cloud services such as cognitive services and media analytics. I’m actually at NAB this week talking to media companies about how this program can transform production workflows and breathe new life into existing content. You can learn more about Microsoft Azure, and how it can help your media workflows and business by reading Tad Brockway’s blog post.\nBackup and archive\nAzure’s global, highly-available, tiered storage model makes it a natural cloud destination for your backup and archival media. Additionally, with broad ISV support and rich business continuity solutions including Azure Backup and Azure Site Recovery, you can easily use Azure as the DR platform for your data and applications. This reduces downtime, and provides for faster recovery in the event of an outage.\nData and analytics\nTape is also a common retention media for large data repositories such as seismic surveys and genomics sequencing. While this information is locked away on tape, it is effectively inert to higher-order analysis. Migrating the data to Azure can enable richer analytics tools, such as HDInsight and Machine Learning, to derive deeper insights.\nPartners help you move to the cloud\nOur partners work with virtually all types of media, whether that be disks or tapes, SAN or NAS, or even video cassettes or 35mm. And they can work with you, where your media currently is, all over the globe. Check our Offline Media Import website​ to find one near you.\nWhether your data is terabytes, petabytes, or exabytes, the Offline Media Import Program is a great opportunity to move data off existing legacy media and into the Azure cloud. Best of all, Azure can be your data’s enduring home – and the last data migration you’ll have to do!\n", "link": "https://azure.microsoft.com/en-us/blog/offline-media-import-for-azure/", "Role": "General Manager, Azure Stack Edge and Data Box"},
{"Title": "Microsoft Conversational AI tools enable developers to build, connect and manage intelligent bots", "Date": "Posted on May 7, 2018", "Contributor": "Lili Cheng", "Content": "\nConversational AI is the next user interface (UI) wave in computing. We’ve evolved from a world of having to learn and adapt to computers to one where they’re learning how to understand and interact with us. Natural interactions with computers start with language, speech, and semantic understanding, and continues through supporting rich multi model interactions.\nToday at the Build conference, we are announcing major updates related to our Conversational AI tools including updates to Aure Bot Service, Microsoft Cognitive Services Language Understanding, and QnAMaker, as well as the release of new experimental projects from the Cognitive Services Labs including Conversation Learner and Personality Chat. This blog post provides a brief recap of all Conversational AI announcements from Build and takes a quick dive into some of our newly updated services.\nWith Microsoft’s Conversational AI tools developers can build, connect, deploy, and manage intelligent bots that naturally interact with their users on a website, app, Cortana, Microsoft Teams, Skype, Facebook Messenger, Slack, and more. It’s quick, free, and easy to get started with the Microsoft Bot Builder software development kit (SDK) and its related tools, for a complete bot building experience. Building intelligent bot requires stitching together several components. Developers can quickly add layers of sophistication to their bots by using the collection of Azure Cognitive Services that include Language Understanding, QnA, dialog, speech, vision knowledge, and much more.  As always, you can host your bot directly on Azure at scale using Azure Bot Service, or in your preferred hosting location.\nFor a more in-depth look at each of these service updates, check out their respective web pages and associated blogs or peruse the Build blog homepage, where there may be a more thorough review from one of the service team’s developers.\nHappy Build and happy building!   \nBot Builder SDK (v4 preview)\nBotBuilder SDK V4 was designed for extensibility and modularity based on feedback from our vibrant developer community. You can quickly start with a simple bot design and increase its sophistication and intelligence by adding different components such as Cognitive Services Language Understanding models and QnAMaker knowledge bases. With the Bot Builder SDK v4 you can develop bots that have more guided interactions and provide the user with discrete choices or actions. Or you can design freeform conversation using Microsoft Cognitive Services Language Understanding (LUIS). With LUIS, you can add natural language interactions to allow your users to interact with your bots more naturally and expressively. Your bot’s conversation can use plain text as well as more sophisticated cards containing text, images, and actions buttons.\nBot Builder SDKv4 is free and easy to use and now supports coding in 4 languages: C#, JavaScript, Python, and Java. Check out the Bot Builder homepage or the Bot Builder GitHub pages to get started today!\nImproved Bot Framework Emulator (v4 Preview)\nThe Bot Framework Emulator is an evolution of our open-source, cross-platform application for Mac, Windows, and Linux that allows bot developers to test and debug their bots on their local machine or in the cloud. The new emulator includes features like an improved dynamic layout model, support for multiple bot configurations, simplified bot components management, and the ability to inspect responses from and deep link to connected services such as LUIS and QnA. The Bot Framework Emulator also introduces new transcript functionality that enables bot debugging based on transcript logs and transcript playback in presentation mode. The new v4 preview emulator runs side by side with the current GA v3 Bot Emulator and supports bots targeting either Bot Builder v3 or the new v4 preview.\nThe Bot Framework Emulator is available as open source on Github. Check it out and get started today.\n\nNew Bot Builder Tools\nBot Builder Tools are designed to enable an end-to-end conversational app-centric development workflow. From planning and testing mockups, through adding intelligence to your bot with LUIS models and QnAMaker knowledge bases, to publishing your bot and connecting with your audience using Azure Bot Services supported channels. The tools provide a cross-platform command line experience for managing bots, bot’s parts, and channels. The Bot Builder tools are used to manage your bot's integration with LUIS models, QnAMaker KBs, and the Bot Builder Dispatch module. You can also use Azure CLI to manage your Azure bot deployment and channel registration.\nThe Bot Builder tools are hosted as open source on GitHub and distributed through npm. Also, developers can visit the Install Azure CLI 2.0 page for more information about the AZ CLI tool.\nBot Builder Dispatch \nDevelopers are often required to use different components (parts) to increase their bot intelligence and provide a more natural conversation experience to their customers. Developers include multiple LUIS models, QnA Knowledge Bases and other Azure Cognitive Services to infuse intelligence to their bot.  Bot Builder Dispatch is a tool used evaluate potential intents conflicts and overlap across multiple bot components such as LUIS models and QnA knowledge bases.\r\nDetermining which bot component is best suited to handle a given user utterance or provide the best possible answer for any given user query can be challenging. The Dispatch is a component of the new Bot Builder SDKv4 that helps makes bots smarter by using Machine Learning and Natural Language Understanding to determine which part of your bot is best suited to address a user’s given needs.\nThe Dispatch takes representative utterances from your bot's one or more LUIS models, and QnAMaker knowledge bases to build a machine-learned dispatch model that is used to route requests to the right component. The Dispatch evaluation phase allows you to identify improvements this routing model that can be used to increase your bot overall end-user experience.  The resulting Dispatch LUIS model, created as part of the evaluation phase, is then used to direct incoming utterances to the right LUIS or Q&AMaker Knowledge Base.\nThe new Bot Builder Dispatch component is now available as part of the Bot Builder Tools preview.\nBot Service Features\nMost productivity bots depend on external, authenticated services to help users complete requests; whether that be finding a ride, interacting with a source repo, or accessing services via the Office 365 Graph API's.  Azure Bot Service is now introducing authentication as a capability for developers.  Through it a developer can easily configure a bot with an authentication provider including AAD and many named OAuth providers. Once configured, a few simple lines of code and the bot will authenticate users as part of the conversation.\n\nBot Service has now also passed the audit for SOC, adding to the existing ISO 27001, 27018, PCI and HIPAA compliance audits it has completed. Azure Bot Service, as all Azure services, is GDPR compliant and offers API's that can help you manage your users' data privacy.\nLanguage Understanding\nMicrosoft Cognitive Services Language Understanding (LUIS) offers a fast and effective way to add language understanding to your applications. You can use LUIS’s pre-existing, pre-built models for your scenario and, LUIS can guide you through the process of specializing and quickly rebuilding your models at any time.\nText Analytics has now expanded capabilities including Entity Linking that allows users to identify well-known entities in their text (people, places, organizations, etc.) and provide links to the web for more information.\nLUIS  offers a fully integrated experience with both Text Analytics and speech. Combining the Text Analytics capabilities enables developers to receive sentiment analysis for each utterance along with intents and entities in LUIS response. Integration with Text Analytics makes it possible to detect key phrases in an utterance through the new key phrase entity type.\nThe new Language Understanding offering integrates a new ‘Speech to Intent’ bundle that combines the speech to text capability with text to intent using a single Azure key. The new offering is designed to simplify the speech to intent process and provide developers a streamlined, integrated, and customized experience for specific language scenarios, like call center analytics.\nAdditionally, LUIS offers two new features that enable an improved language understanding of intents and entities. Regex entities allow the identification of an entity in the utterance based on a regular expression. Patterns, on the other hand, enable developers to effectively define intents without having to provide many utterances. Patterns could also encompass entities with variable length represented as Patterns.any entities.\nFor more information on how to get started, check out the Language Understanding page and its associated blogs.\nQnAMaker\nMicrosoft Cognitive Services QnAMaker is generally available today. QnAMaker is an easy-to-use web-based service to train AI to respond to user’s questions in a more natural, conversational way. With QnAMaker, developers can build, train, and publish question and answer bots in minutes. Thanks to QnAMaker’s graphical interface service, you don’t need to have natural language understanding expertise or experience to leverage it. This update represents an important step in QnAMaker’s evolution and makes the product more accessible to all users, regardless of expertise. Along with general availability, QnAMaker is now available as a hosted model to improve the service’s latency, scale, and compliance. GA also brings a brand new portal with new features that manage knowledge base management easier.\nCheck out the updated QnAMaker portal and associated blogs to learn more and get started.\nProject Conversation Learner\nNewly available as a Cognitive Services lab, Project Conversation Learner enables you to build and teach conversational interfaces that learn directly from example interactions.  Spanning a broad set of task-oriented use cases, Project Conversation Learner applies machine learning behind-the-scenes to decrease manual coding of dialogue control logic.  The Project Conversation Learner SDK works in conjunction with the Bot Builder SDK v4 preview, which makes bots, skills, and other conversational UIs built with Project Conversation Learner easy to deploy via the Bot Framework.\n\nTeaching interface for Project Conversation Learner. A developer or domain expert chooses the actions the bot should take, and the resulting dialogs are used to train a recurrent neural network.\nCheck out the Cognitive Services labs homepage for more information on getting started with Project Conversation Learner.\nProject Personality Chat\nPersonality Chat helps make intelligent agents more complete and conversational by handling common small talk and reducing fallback responses. You can have your agent do small talk in the flavor of one of three personalities – Professional, Friendly, Humorous. Try out a demo at Personality Chat Cognitive Labs and learn about the response generation technology.\nA customizable editorial library of the 100 most common small talk scenarios is available to integrate with the Bot Builder SDK v4.\nTo learn more and get started visit the Azure Bot Service website.\nThank you again and happy coding!\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-conversational-ai-tools-enable-developers-to-build-connect-and-manage-intelligent-bots/", "Role": "Corporate Vice President, Conversational AI"},
{"Title": "Microsoft empowers developers with new and updated Cognitive Services", "Date": "Posted on May 7, 2018", "Contributor": null, "Content": "\nThe blog post was authored by Andy Hickl, Principal Group Program Manager, Microsoft Cognitive Services.​\nToday at the Build 2018 conference, we are unveiling several exciting new innovations for Microsoft Cognitive Services on Azure.\nAt Microsoft, we believe any developer should be able to integrate the best AI has to offer into their apps and services. That’s why we started Microsoft Cognitive Services three years ago – and why we continue to invest in AI services on Azure today.\nMicrosoft Cognitive Services make it easy for developers to easily add high-quality vision, speech, language, knowledge and search technologies in their apps -- with only a few lines of code. Cognitive Services make it possible for anyone to create intelligent apps, including ones that can talk to users naturally, identify relevant content in images, and confirm someone’s identity using their voice or appearance.\nAt Build this year with Cognitive Services, we're offering even more services and more innovation with announcements such as a unified Speech service and Bing Visual Search, as well as the expansion of our Cognitive Services Labs of emerging technologies. We’re also empowering developers to customize the pre-built AI offered by Cognitive Services, with customized object detection, added features for Bing Custom Search (with custom image search, custom autosuggest,..) and new speech features with custom translator, custom voice, custom speech, and more. We’re also bringing AI to developers everywhere by making Cognitive Services available on all of the computing platforms developers build on today, with the ability to export Custom Vision models to support deployments on edge and mobile devices. Lastly, we also have some of the most powerful bot tools in the market to create more natural human and computer interactions.\nTo date more than a million developers have already discovered and tried Microsoft Cognitive Services, including top companies such as British Telecom, Box, KPMG, Big Fish Games, and through an API integration with PubNub.\nCognitive Services updates\nLet’s talk in detail about some of the exciting new updates coming to Cognitive Services at Build.\nVision\n\nComputer Vision unlocks the knowledge stored in images. It now integrates an improved OCR model for English (in preview), and captioning expanded to new languages (Simplified Chinese, Japanese, Spanish and Brazilian Portuguese).\nCustom Vision (in preview) makes it easy to build and refine customized image classifiers that can identify specific content in images. Today, we’re announcing that Custom Vision now performs object recognition as well.  Developers can now use Custom Vision to train models that can recognize the precise location of specific objects in images. In addition, developers can now download Custom Vision models in three formats: TensorFlow, CoreML, and ONNX.\nContent Moderator offers machine-assisted content moderation and human review tool. It now offers text classification to flag potentially adult, racy and offensive content, and human review capabilities for text and video moderation insights.\nVideo Indexer, the Cognitive Service that extracts insights from videos, can now be connected to an Azure account.\n\nSpeech & Machine Translation\n\nWe’re announcing a new unified Speech service in preview. Developers will now have access to a single-entry point to perform:\r\n\r\n \nSpeech to Text (speech transcription) –  converting spoken audio to text with standard or custom models tailored to specific vocabulary or speaking styles of users (language model customization), or to better match the expected environment, such as with background noise (acoustic model customization). Speech to Text technology enables a wide range of use cases like voice commands, real-time transcriptions, and call center log analysis.\nText to Speech (speech synthesis) – bringing voice to any app by converting text to audio in near real time with the choice of over 75 default voices, or with the new custom voice models, creating a unique and recognizable brand voice tuned to your own recordings.\nSpeech Translation – providing real-time speech translation capabilities with models based on neural machine translation (NMT) technologies. Three elements of the Speech Translation pipeline can now be customized through a dedicated tool: speech recognition, text to speech and machine translation.\n\n\nWe’re also announcing the availability of the Speech devices SDK in preview by invitation (a pre-packaged software fine-tuned to specific hardware) as well as the new Speech client SDK in preview (bringing together the various speech features, support for multiple platforms and different language bindings).\nWe continue to build on our amazing machine translation Cognitive Services. We’re announcing support for the customization of neural machine translation. In addition, we’re making significant updates to the Microsoft Translator text API (version 3), unveiling a new version of the service natively using Neural Machine Translation. We also announced a new Microsoft Translator feature for Android that will make it possible to integrate translation capabilities into any app – even without an Internet connection.\n\nLanguage\n\nText Analytics, a cloud-based service that provides advanced natural language processing over raw text, performing sentiment analysis, key phrase extraction and language detection. Today, Text Analytics introduces the ability to perform entity identification and linking from raw text. Entity linking recognizes and disambiguates well-known entities found in text such as people, places, organizations, and more while linking to more information on the web.\nOur Language Understanding (LUIS) service simplifies the speech to intent process with a new integrated offer.\n\nKnowledge\n\nQnA Maker is a service that makes it possible respond to user's questions in a more natural, conversational way. Today, we’re announcing that QnA Maker is Generally Available on Azure.\n\nSearch\nSearch has been another key area of AI investment: we’re empowering developers to leverage it through multiple search APIs. Today we’re announcing several major updates for Bing Search APIs:\n\nBing Visual Search, a new service in general availability that lets you harness the capabilities of Bing Search to perform intelligent search by image capabilities, such as searching similar products for a given product (e.g. in retail scenario, wherein user would like to see products similar to the one being considered), identifying barcodes, text, celebrities and many more! We’re also announcing a preview by invite portal for intelligent Visual Search.\nBing Statistics-add-in, an add-in which provides various metrics and rich slicing and dicing capabilities for several Bing APIs\nBing Custom Search updates including custom image search, custom autosuggest, specific statistics information, instance staging and sharing capabilities;\nThe Bing APIs SDK now in general availability.\n\nLabs\n\nWe’re also announcing new Cognitive Services labs, providing developers with an early look at the emerging Cognitive Services technologies, such as:\nProject Answer Search – enables you to enhance your users’ search experience by automatically retrieving and displaying commonly known facts and information from across the internet.\nProject URL Preview - informs your users’ social interactions by enabling creation of web page previews from a given URL or flagging adult content to suppress it.\nProject Anomaly Finder enables developers to monitor their data over time and detect anomalies by automatically applying a statistical model.\nProject Conversation Learner by invite - enables you to build and teach conversational interfaces that learn directly from example interactions. Spanning a broad set of task-oriented use cases, Project Conversation Learner applies machine learning behind the scenes to decrease manual coding of dialogue control logic.\nProject Personality Chat – available soon by invitation– makes intelligent agents more complete and conversational by handling common small talk in a consistent tone and fallback responses. Give your agent a personality by choosing from multiple default personas, and enhance it to create a character that aligns with your brand voice.\nProject Ink Analysis – available soon by invitation- provides digital ink recognition and layout analysis through REST APIs. Using various machine learning techniques, the service analyzes the ink stroke input and provides information about the layout and contents of those ink strokes. Project Ink Analysis can be used for a variety of scenarios that involve ink to text as well as ink to shape conversions.\n\nLet’s take a closer look at what some of these new announced services can do for you.\nObject Detection with the Custom Vision service\nThe Custom Vision service preview is an easy-to-use, customizable web service that learns to recognize specific content in imagery, powered by state-of-the-art machine learning neural networks that become more accurate with training. You can train it to recognize whatever you choose, whether that be animals, objects, or abstract symbols. This technology could easily apply to retail environments for machine-assisted product identification, or in digital space to automatically help sorting categories of pictures.\nWith today’s update, it’s now possible to easily create custom image classification and object recognition models.\nUsing the Custom Vision service portal, you can upload and annotate images, train image classification models, and run the classifier as a Web service. Custom Vision now supports custom object recognition as well. You can use the Custom Vision Portal to find – and label – specific objects within an image. You’ll be able to create bounding boxes to specify the exact location of the objects you’re most interested in each image.\n\nAn easy way to label objects within the image with bounding box\nLast but not least, thanks to Custom Vision’s latest updates, it’s also possible to export the trained model to use in a container which can be deployed on-premises, mobile devices and even IoT.\r\nWe created fresh tutorials to easily get started with Custom Vision:\n\nFeel free to explore in the documentation, Feel free to explore the C# tutorial and Python tutorial of a basic Windows application to create an object detection project. This includes creating both console application and custom vision service project, as well as adding tags, upload various images to the project and identifying the regions of the object by using coordinates and tag. You can also find the detailed guide to export your model as a container.\nFeel free to get started on the Custom Vision service portal and webpage.\n\nBuild your own one-of-a-kind Custom Voice with the new Speech service\nPart of the unified Speech service preview announced today at Build, the text-to-speech service allows you to create a unique recognizable voice from your own acoustic data (currently for English and Chinese). The possibilities are endless: smart IVR, bot or application that could use iconic brand voice on the go, without the need of deep expertise in data science.\nIn order to proceed, you’ll be guided through the process of developing the necessary audio datasets and transcripts, and uploading them in the Custom Voice portal (accessible by invite).\nOnce it’s done, you will train the service and create a new voice model, which will be deployed into a Text-to-Speech endpoint.\n\nCustom Voice customization tool\nFor more information about the new Custom Voice capabilities of the new Speech service, please take a look at the following resources:\n\nThe Text-to-Speech webpage and custom voice portal\nThe detailed technical guide to build your own custom voice.\n\nPerform smart product search with Bing Visual Search\nBing Visual Search provides insights with a similar experience to the image details shown on Bing.com/images. For example, you can upload a picture and get back search insights about the image such as visually similar images, shopping sources, webpages that include the image, and more.\r\nInstead of uploading an image, you can also provide a URL of the image along with the optional crop box to guide your visual search.\nFor example, you can receive visually similar images (a list of images that are visually and semantically similar to the input image or regions within) or visually similar products (a list of images that contain products that are visually and semantically similar to the products in the input image, you can restrict this to your domain), and more.\n\nSimilar image results with Bing Visual Search, powered by Bing\nThe Bing Visual Search results also provide bounding boxes for regions of interest in the image. For example, if the image contains celebrities, the results may include bounding boxes for each of the recognized celebrities in the image. Similarly, if Bing recognizes a fashion object in the image, the result may provide a bounding box for the recognized product.\nWhen making a Bing Visual Search API request, if there are insights available for the image, the response will contain one or more tags that contain the insights, such as webpages that include the image, visually similar images, and shopping sources for items found in the image.\r\nFor more information about Bing Visual Search, please take a look at:\n\nThe Bing Visual Search webpage\nDetailed technical walkthrough in the documentation\n\nThank you again and happy coding!\nAndy\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-empowers-developers-with-new-and-updated-cognitive-services/", "Role": null},
{"Title": "Announcing Cognitive Search: Azure Search + cognitive capabilities", "Date": "Posted on May 7, 2018", "Contributor": "Pablo Castro", "Content": "\nToday we are announcing Cognitive Search, an AI-first approach to content understanding. Cognitive Search is powered by Azure Search with built-in Cognitive Services. It pulls data from a variety of Azure data sources and applies a set of composable cognitive skills which extract knowledge. This knowledge is then organized and stored in a search index enabling new experiences for exploring the data.\nFinding latent knowledge in all data\nReal-world data is messy. It often spans media types (e.g. text documents, PDF files, images, databases), changes constantly, and carries valuable knowledge in ways that is not readily usable. In our team we see the same challenges that emerge from this on a daily basis: our customers apply information retrieval solutions, such as Azure Search- combined with AI models, either pre-built models such as Cognitive Services or custom ones, to extract latent knowledge in their vast data stores.\nThe typical solution pattern for this is a data ingestion, enrichment and exploration model.  Each of these brings its own challenges to the table—from large scale change tracking to file format support, and even composition of multiple AI models. Developers can do this today, but it takes a huge amount of effort, requires branching into multiple unrelated domains (from cracking PDFs to handling AI model composition), and distracts from the primary goal. This is where Cognitive Search comes in.\nCognitive Search\nThe new Cognitive Search capability in Azure Search is a concrete implementation of the ingest-enrich-explore pattern.\n\nWhen you use Azure Search, you get direct support for each aspect of the process:\n\nIngest: pull data from Azure Blob Storage, SQL DB, CosmosDB, MySQL, and Table Storage. For unstructured data in Blob Storage, the service not only reads the raw data but also supports extracting contents from several popular file formats such as PDFs and Office documents. The service supports change detection to keep up with changes through incremental processing, without having to go over the entire data set after initial ingestion.\nEnrich: use cognitive skills to augment data as it’s ingested using a powerful composition system. The service integrates with Cognitive Services to offer built-in support for OCR (for print and handwritten text), named entity recognition, key phrase extraction, language detection, image analysis with scene description/tagging capabilities, and more. We also included a webhook-based extensibility mechanism that allows you to add your own cognitive skills while fitting into the rest of the picture for ingestion and composition; custom skills are easy to build as Azure Functions or other tools that can expose webhooks. Knowledge created during enrichment often not only augments individual data items, but also connects entities and facts across different items, different stores and even different media types.\nExplore: the outcome of enrichment is additional knowledge derived from the ingested data combined with the outcome of applying various AI models to it. Different applications will want to surface different exploration experiences over this data. The original data and all annotations produced during enrichment are put in an Azure Search index—a powerful data store that supports keyword search, understands 56 languages, handles structured queries in addition to unstructured search, and offers faceted navigation.  Best of all, the index processes all of this at lightning-fast speeds.\n\nCustomer Scenarios\nWe’re lucky to have multiple customers work with us during the early stages of product development for Cognitive Search. This helped us understand real-world requirements and iterate on product adjustments.\nHere are a few examples of how we’ve seen customers apply Cognitive Search:\nOur Healthcare customers face a similar challenge with clinical data. Large volume of text includes references to general entities (e.g. people’s names) and domain-specific ones (e.g. drug and disease names) that need to be connected and related. Sometimes they also need to combine this with imagery that’s analyzed in well-known ways (e.g. OCR) as well as applying leading-edge methods (e.g. AI-assisted diagnostics).\nIn the Financial Services space, customers need to handle the challenge of extensive regulation described as large volume of documents, forms produced by their customers, contracts they handle with customers and providers, and more. Generally-applicable natural language processing techniques combined with specialized content understanding models enables them to provide their employees and customers with a global view of their information assets.\nOil & Gas companies have teams of geologists and other specialists that need to understand seismic and geologic data. They often have decades of PDFs with pictures of samples over sample sheets full of handwritten field notes. They need to connect places, people (domain experts), events, and navigate all this information to make key decisions.\nStart today\nCheck out Scott Guthrie’s announcement in his //BUILD 2018 keynote, where you can see one of our early adopter customers, the NBA, talk about their scenario and how Cognitive Search brought together Azure Search, Cognitive Services and custom models built with Azure ML to power a rich data exploration experience powered by AI.\nIf you’re ready to try this out on your own data, head to the Azure portal, create an Azure Search service and you’ll see the “Cognitive Search” step in the Import Data flow.\n\nWhen you’re ready to go past what the UX can do, check out the documentation to learn how to use more cognitive skills and how to extend the enrichment process with your own data.\nTo explore a scenario where we applied Cognitive Search to a public dataset, check out the JFK Files in AI.lab. We published a live version of that app and posted the code in GitHub in case you want to use it as a starting point for something you want to build.\nWe look forward to seeing what you’ll build with Cognitive Search! If you want to get in touch with the team, tweet with the #azuresearch hashtag or email us at azuresearch_contact@microsoft.com.\nPablo Castro - on behalf of the entire Content Search and Intelligence team\n", "link": "https://azure.microsoft.com/en-us/blog/announcing-cognitive-search-azure-search-cognitive-capabilities/", "Role": "Director of Engineering, AI Platform"},
{"Title": "Full-integrated experience simplifying Language Understanding in conversational AI systems", "Date": "Posted on May 10, 2018", "Contributor": "Nayer Wanas", "Content": "\nCreating an advanced conversational system is now a simple task with the powerful tools integrated into Microsoft’s Language Understanding Service (LUIS) and Bot Framework. LUIS brings together cutting-edge speech, machine translation, and text analytics on the most enterprise-ready platform for creation of conversational systems. In addition to these features, LUIS is currently GDPR, HIPPA, and ISO compliant enabling it to deliver exceptional service across global markets.\nTalk or text?\nBots and conversational AI systems are quickly becoming a ubiquitous technology enabling natural interactions with users. Speech remains one of the most widely used input forms that come natural when thinking of conversational systems. This requires the integration of speech recognition within the Language Understanding in conversational systems. Individually, speech recognition and language understanding are amongst the most difficult problems in cognitive computing. Introducing the context of Language Understanding improves the quality of speech recognition. Through intent-based speech priming, the context of an utterances is interpreted using the language model to cross-fertilize the performance of both speech recognition and language understanding. Intent based speech recognition priming uses the utterances and entity tags in your LUIS models to improve accuracy and relevance while converting audio to text. Incorrectly recognized spoken phrases or entities could be rectified by adding the associated utterance to the LUIS model or correctly labeling the entity.\nIn this release, we have simplified the process of integrating speech priming into LUIS. You no longer must use multiple keys or interact through other middleware. This more streamlined integration also reduces the latency that your users would experience when using speech as an input to your conversational system. All you need to do is to enable speech priming in the publish setting of your LUIS application. Speech priming will be invoked on the same subscription key used in LUIS and transferred to the speech APIs seamlessly. \n\nText Analytics: Understand your text?\nLUIS continues to bring together different technologies to help understand your user. It already includes the power of Bing Spell Check and now we are adding functionality from the Text Analytics Cognitive Service. Integrating text analytics into your LUIS model enabling sentiment detection on utterances is a simple configuration. Through this integration your bot can tell you if your customer is happy or sad. Text analytics also enables the detection of key phrases within utterances without requiring labeling or training. These advanced natural language processing tools enable better, more personalized interaction with your customers.\nThe JSON object returns the sentiment of the utterance as a value from 0 – 1, with values closer to 1 are more positive while closer to 0 are more negative. Additionally, adding key phrases pre-built entity enables identifying key phrases in the returned object.\n\nCreate a Global Bot, without being \"lost in translation\"\nAre you worried about the effort it would take you to design a Bot that speaks multiple languages? With the integration of Machine Translation middleware into the Bot Framework, you don't need to worry any more. Using the Bot created in one language across 60+ languages in a few lines of code makes it much simple to build and improve models. With personalization and customization included in the middleware, enabling a global Bot is a simple task. Combining translation middleware with LUIS and QnA maker is simple and includes passing the utterance after translation. The middleware also specifies if the response should be translated back to the user language or returned in the bot native language.\n\nThe translation middleware also includes the ability to identify patterns that shouldn’t be translated to the target language (such as names of locations or entities that are meaningful in their own terms). An example if the user says “My name is …” in a non-native language for the bot you want to avoid translating this name using a pattern in every language. The code snippet reflects this in pattern for French.\n\nThe middleware also includes a localization converter for currencies and dates to distinct cultures by adding LocaleConverterMiddleware. The Machine Translation middleware is currently released as preview with Bot Framework SDK V4.  \nGeneralizing the Model using Regex and Pattern features\nIn this release LUIS is introducing two features that enable an improved language understanding of entities and intents. Regex entities allows the identification of an entity in the utterance based on a regular expression. For example, a flight number regular expression includes two or three characters and then 4 digits. The Delta Airlines flight regular expression could be expressed as DL[0-9]{4}. Defining this regular expression entity will allow LUIS to extract matching entities from an utterance.\nPatterns, on the other hand, allow developers to define intents they could represent effectively without the need to provide extensive utterances. This is especially effective for forms that could capture a wide variety of common ways of expressing an utterance. Consider for example a shopping application, the pattern “add the {Shopping.Item} to my {Shopping.Cart}” is a common way of expressing the intent Shopping.AddToCart. \nPatterns are especially useful when there are similarities between utterances that reflect different intents. Take for example the utterances in a human resource domain “Who does Mike Jones report to?” and “Who reports to Mike Jones?”. The two statements contain the same tokens yet reflect different intents. This could be captured by either introducing many utterances, or by simply expressing these common utterances by their respective patterns “Who does {Employee} report to?” and “Who reports to “Employee}?”. Introducing these patterns, alongside the utterances allow LUIS to identify the most suitable intent to fit to an utterance. Moreover, patterns could also capture the distinct roles of entities. As an example, the pattern “Book a ticket from {Location:origin} to {Location:Destination}” in a flight booking application. This pattern captures the distinct roles of the locations included in the utterance.\nAdditionally, patterns could encompass entities with variable length represented as Patterns.any entities. The entities are detected first, prior to the matching of the pattern. In turn the pattern “Where is the {FormName} and who needs to sign it after I read it?” will match form names that could extent multiple tokens.\n\nInvolve your personal data scientist to help improve your model\nThe tools provided in Cognitive Services enable developers without a machine learning (ML) background to develop conversational systems. Once the bot is built, developers are faced with multiple options to further improve their models. Some of these options are rooted in ML, and in turn developers with limited ML experience might not fully explore these options. \n\nIn this release of LUIS and Bot Framework, we have taken our goal of democratizing ML further. We are incorporating tools to provide personalized data science guidance to developers on the existing applications. This includes identifying the areas where the current model falls short and providing suggestion to help improve the trained model. It also allows for automatically generating different architectures of multiple conversational models, which include LUIS and QnA maker, through the dispatcher tool. This might be creating a hierarchical architecture with a dispatching model or consolidating multiple models into one LUIS model. Collectively, these tools use LUIS to realize the different architectures and dissect the models to help provide the most suitable guidance to developers. The dispatch tool is currently released in preview on Bot Framework SDK V4.\nThese are some of the highlights of the features that have been introduced to LUIS and Bot Framework. Through integrating the different tools, and compliance with GDPR, HIPPA, and ISO, LUIS and Bot Framework are distinguishing themselves as the most enterprise-ready platform. These new additions make understanding customers and reaching new markets and users a few lines of code away. It also makes bot interactions more natural to your users.\nFor more information please visit:\n\nLanguage Understanding Service (LUIS)\nBot Framework\nBot Framework SDK\nBing Speech API\nTranslator Text API\nText Analytics APIs\nQnA Maker APIs\n\nWe look forward for you feedback and the amazing Bot you create.\n", "link": "https://azure.microsoft.com/en-us/blog/full-integrated-experience-simplifying-language-understanding-in-conversational-ai-systems/", "Role": "Principle RSDE Manager, Microsoft Research"},
{"Title": "Speech services now in preview", "Date": "Posted on June 4, 2018", "Contributor": "Grace Sturman", "Content": "\nThis blog post was authored by the Microsoft Speech Services team​. \nAt Microsoft Build 2018, the Microsoft Speech Services team announced the following new and improved products and services.\n\nSpeech service as a preview, including Speech to Text with custom speech, Text to Speech with custom voice, and Speech Translation.\nSpeech SDK as a preview, which will replace the old Bing Speech APIs when generally available in fall 2018. It will be the single SDK for most of our speech services, and will require only one Azure subscription key for speech recognition and LUIS (language understanding service). With simplified APIs, Speech SDK makes it easy for new and experienced speech developers.\nSpeech Devices SDK, as a restricted preview, has advanced multi-microphone array audio processing algorithm that's fine-tuned to the backend Speech Services, and works great on the Roobo's dev kits for exceptional speech experiences, and the ability to customize the wake word to strengthen your brand.\n\nTo learn more, please read the ZDNet article highlighting these products and services.\nWe also demonstrated our Speech Recognition capabilities in the Satya Nadella’s vision keynote at Microsoft Build 2018. You can skip to the 1:22:40 mark if you want to jump to this demonstration. You can see Speaker Identification, multiple simultaneous recognition, transcription, translation, besides other awesome AI features.\n\nIf you want to build a device like the prototype device used in that demo, check out the Speech Devices SDK. It also uses multi-microphone array hardware and advanced algorithm for audio processing.\n\nYou can find video recordings of our Speech related Build 2018 presentations below:\n\nSpeech Services in details (75 minutes)\nSpeech services overview (20 minutes)\nSpeech Devices SDK (20 minutes)\nCognitive Services Speech SDK (20 minutes)\nIntroduction of the Custom Voice Service (20 minutes)\n\nWant to try out Microsoft Speech services? You can try it out for free. To learn more and review sample code, please reference our documentation page.\nLet us know if you have questions or feedback via Stack Overflow by using the tag microsoft-cognitive. You can also append specific area topic tags to the URL by adding “+[tagName]” like:\n\nspeech-recognition\nspeech-to-text\ntext-to-speech\nmicrosoft-speech-api\n\nKeep an eye on the the Azure blog as we will continue to announce speech services news, updates to our SDKs, as well as new features.\n", "link": "https://azure.microsoft.com/en-us/blog/speech-services-now-in-preview/", "Role": "Senior Program Manager, Speech Services"},
{"Title": "Bing Visual Search and Entity Search APIs for video apps", "Date": "Posted on June 13, 2018", "Contributor": "Milan Gada", "Content": "\nIn this blog, I will go over how you can use the Bing Visual Search API, in combination with Bing Entity Search API to build an enhanced viewing experience in your video app.\nGeneral availability of Bing Visual Search API was announced at Build 2018, in this blog. Bing Visual Search API enables you to use an image as a query to get information about what entities are in the image, along with a list of visually similar images from the image index built by Bing. GA of Bing Entity Search was announced in this blog, published on March 1st, 2018. Bing Entity Search API enables you to brings rich contextual information about people, places, things, and local businesses to any application, blog, or website for a more engaging user experience.\nBy combining the power of these two APIs, you can build a more engaging experience in your video app, by following the steps listed below\n\nWrite a JavaScript function that triggers when the user clicks the pause button in your video app. In this JavaScript function, grab the paused video frame as an image. Take a look at this discussion to learn more about how to do this.\nPass the captured video frame via AJAX to a server-side function.\nIn your server side function, you can now call the Bing Visual Search API, with that image as input. Bing Visual Search documentation page provides code samples in multiple languages for this.\nBing Visual Search API will return a JSON string with insights for the image. The response format is well documented on this documentation page.\nThe entities can be found in “actions” tag with “_type” = “ImageEntityAction”. As an example, for this image, which has Tom Hanks and his wife, you will see two “actions” tag with “_type” = “ImageEntityAction”.\nYou can now use the “displayName” for these entities to call the Entity Search API. Code samples for Bing Entity Search API can be found on this documentation page, which you can then pass back to the video app, to render it in the UI.\n\nEntities in Bing are not limited to people. As an example, the Visual Search API identifies “Yosemite National Park” as an entity in this image. Depending on the types of videos you intend on showcasing in your video app, you can decide if you want to focus on certain or all types of entities.\n", "link": "https://azure.microsoft.com/en-us/blog/bing-visual-search-and-entity-search-apis-for-video-apps/", "Role": "Principal Program Manager, Azure Media Services"},
{"Title": "Get video insights in (even) more languages!", "Date": "Posted on June 28, 2018", "Contributor": "Ella Ben-Tov", "Content": "\nFor those of you who might not have tried it yet, Video Indexer is a cloud application and platform built upon media AI technologies to make it easier to extract insights from video and audio files. As a starting point for extracting the textual part of the insights, the solution creates a transcript based on the speech appearing in the file; this process is referred to as Speech-to-text. Today, Video Indexer’s Speech-to-text supports ten different languages. Supported languages include English, Spanish, French, German, Italian, Chinese (Simplified), Portuguese (Brazilian), Japanese, Arabic, and Russian.\nHowever, if the content you need is not in one of the above languages, fear not! Video Indexer partners with other transcription service providers to extend its speech-to-text capabilities to many more languages. One of those partnerships is with Zoom Media, which extended the Speech-to-text to Dutch, Danish, Norwegian and Swedish.\nA great example for using Video Indexer and Zoom Media is the Dutch public broadcaster AVROTROS; who uses Video Indexer to analyze videos and allow editors to search through them. Finus Tromp, Head of Interactive Media in AVROTROS shared, “We use Microsoft Video Indexer on a daily basis to supply our videos with relevant metadata. The gathered metadata gives us the opportunity to build new applications and enhance the user experience for our products.”\nBelow is an example for how a Dutch video transcript looks like in the Video Indexer Service:\n\nSpeech-to-text extensions in Video Indexer\nTo index a file in one of the extended language you will need to first use the provider’s solution to generate a VTT file (aka the result transcription file) in the required language and then send the VTT file along with the original video or audio file to Video Indexer to complete the indexing flow.\nOnce the file is indexed, the full set of insights, including the full transcript will be available for consumption both via the Video Indexer API and via the Video Indexer service. Video Indexer also include the ability to translate the that transcript to dozens of other languages.\nWe are excited to introduce an open-sourced release of an integration between Video Indexer and Zoom Media now available on GitHub.\n(Special thanks to Victor Pikula, Cloud Solution Architect in Microsoft, for designing and building this integration)\nThe solution is built using Azure Blob storage, Azure Logic Apps and the new v2 Video Indexer REST API. Let’s take a deeper look into how the integration is built.\nBelow is a high-level diagram of the flow:\n \r\nGiven a video or audio file, the file is first dropped into a Blob Storage. The Logic App “watches” any additions to the blob and as a result sends the file to both Video Indexer and Zoom Media. Zoom Media then generates a VTT file based on the required language and passes it back to Video Indexer to complete the indexing flow. There is more information about the integration, how to get Video Indexer key and the Zoom Media key, and how to control the VTT language used available.\nHaving the integration built using Logic Apps makes it easy to customize and maintain it as it allows you to debug and configure it quickly and without any need of coding skills.\n\nSo, as you can see, with Video Indexer’s powerful media AI technologies, coupled with the ability integrate speech-to-text capabilities in any language and the ease of use of Azure services such as Logic Apps, everyone can get more out of their videos around the world!\nHave questions or feedback? We would love to hear from you!\nUse our UserVoice to help us prioritize features, or email VISupport@Microsoft.com with any questions.\n", "link": "https://azure.microsoft.com/en-us/blog/get-video-insights-in-even-more-languages/", "Role": "Principal Program Manager, Azure Media Services, Video Indexer"},
{"Title": "Azure Search – Announcing the general availability of synonyms", "Date": "Posted on July 2, 2018", "Contributor": "Liam Cavanagh", "Content": "\nToday we are announcing the general availability of synonyms. Synonyms allow Azure Search to associate equivalent terms that implicitly expand the scope of a query, without the user having to provide the alternate terms.\nA good example of this capability was demonstrated at the recent Microsoft Build conference, where we showed how NBA.com searches their vast photo library of players, owners, and celebrities. In this application Azure Search synonyms are used to enable nicknames of Lebron James such as “The King” or “King James” to be returned regardless of which of the three terms are used in the query.\nIn Azure Search, synonym support is based on synonym maps that you define and upload to your search service. These maps constitute an independent resource, such as indexes or data sources, and can be used by any searchable field in any index in your search service. Synonym maps use the Apache Solr format as outlined in the example synonym map below:\n\r\nPOST https://[servicename].search.windows.net/synonymmaps?api-version=2017-11-11\t\r\napi-key: [admin key]\t\r\n{\t\r\n   \"name\":\"mysynonymmap\",\t\r\n   \"format\":\"solr\",\t\r\n   \"synonyms\": \"\t\r\n\tUSA, United States, United States of America\\n\t\r\n\tWashington, Wash., WA => WA\\n\"\t\r\n}\r\n\nIn the above example, you can see there are two types of synonyms that are used. The first example is a set of synonyms which indicate that a user could search using any of the three terms (\"USA\", \"United States\", or \"United States of America\") in their query and any document that contains either of these terms will become viable search results.\nThe second example is an explicit mapping and is denoted by an arrow \"=>\" that states that a search query that matches the left hand side of \"=>\" will be replaced with the alternatives on the right hand side. Given the rule below, search queries \"Washington\", \"Wash.\", or \"WA\" will all be rewritten to \"WA\". Explicit mapping only applies in the direction specified and does not rewrite the query \"WA\" to \"Washington\" in this case.\nThe first option is useful in situations where you have multiple ways of referring to the same thing in the index. The second is good when the data is normalized to single value in the index, but people would refer to the same thing by multiple terms in a query. \nTIP: Azure Search Traffic Analytics is a great way to identify queries with low recall that can be candidates to be used as synonyms.\nOnce the synonym map has been created, you can then associate it with any fields within your indexes. Afterwards, the synonym map can be updated with new synonyms that will instantly be incorporated into subsequent searches. For example, if you were using the REST API to create your Azure Search index, you could apply it to a field called “description” as follows:\n\r\nPOST https://[servicename].search.windows.net/indexes?api-version=2017-11-11\t\r\napi-key: [admin key]\t\r\n{\t\r\n    \"name\":\"myindex\",\t\r\n    \"fields\":[\t\r\n    {\t\r\n        \"name\":\"id\",\t\r\n        \"type\":\"Edm.String\",\t\r\n        \"key\":true\t\r\n    },\t\r\n    {\t\r\n        \"name\":\"description\",\t\r\n        \"type\":\"Edm.String\",\t\r\n        \"searchable\":true,\t\r\n        \"synonymMaps\":[\t\r\n            \"mysynonymmap\"\t\r\n        ]\t\r\n    }\t\r\n    ]\t\r\n}\t\r\n\nLearn more about synonyms in Azure Search.\n", "link": "https://azure.microsoft.com/en-us/blog/azure-search-announcing-the-general-availability-of-synonyms/", "Role": "Principal Program Manager, Azure Search"},
{"Title": "Get 'stache-stats with Face API & #MyMoustacheRobot", "Date": "Posted on November 9, 2015", "Contributor": "Julia Nikitina", "Content": "\nUnder the name of Project Oxford, Microsoft has made available a set of RESTful APIs that aim to make it possible for developers to build apps that feature face recognition, speech processing, and other machine learning algorithms. Zhen Liu, director from Microsoft Cloud and Enterprise in China, delivered a session on how you can use these APIs easily in the 7th occurence of the China Cloud Computing Conference held in Beijing.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-11-09/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Project Oxford announced at //Build 2015", "Date": "Posted on April 29, 2015", "Contributor": "Julia Nikitina", "Content": "\nUse Face, Speech, and Computer Vision API or Language Understanding Intelligent Service to make your apps smarter. For more information, check out the //Build sessions below.\n\nVision APIs: Understanding Images in your app by Meenaz Merchant on 5/1\nDay 2 Keynote presentation by Joseph Sirosh on 4/30\nMicrosoft Project Oxford: Adding smarts to your applications by Harry Shum and Ryan Galgon on 4/2\n\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-04-29/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Apps World North America Hackathon winner used Project Oxford", "Date": "Posted on May 12, 2015", "Contributor": "Julia Nikitina", "Content": "\nThe winning team of the Apps World North America Hackathon used Project Oxford to win a Surface Pro 3 by using face recognition in SONY glasses.\n\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-05-12/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Project Oxford used in final roundof Beauty of Programming 2015", "Date": "Posted on May 27, 2015", "Contributor": "Julia Nikitina", "Content": "\nThe contest, aiming at strengthening Microsoft brand in Asia Pacific region, strives to encourage young talent to discover more about programming, so as to use it to solve practical problems. This year, one of the finalists use Project Oxford to create a brand new personal voice assisstant.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-05-27/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Project Oxford featured in China Cloud Computing Conference", "Date": "Posted on June 3, 2015", "Contributor": "Julia Nikitina", "Content": "\nUnder the name of Project Oxford, Microsoft has made available a set of RESTful APIs that aim to make it possible for developers to build apps that feature face recognition, speech processing, and other machine learning algorithms. Zhen Liu, director from Microsoft Cloud and Enterprise in China, delivered a session on how you can use these APIs easily in the 7th occurence of the China Cloud Computing Conference held in Beijing.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-06-03/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "#TwinsOrNotRobot recognizes your doppelganger", "Date": "Posted on June 24, 2015", "Contributor": "Julia Nikitina", "Content": "\nTwinsOrNot.net is the latest machine learning-powered game built using the Project Oxford platform – it incorporates facial recognition technology to determine how much you look like a sibling, your friend, or your favorite celebrity! Check it out here: https://twinsornot.net/ or on Windows Phone store.\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-06-24/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "Unlock your door using Face API & Windows 10 IoT Core", "Date": "Posted on August 11, 2015", "Contributor": "Julia Nikitina", "Content": "\nMicrosoft developers have also used Windows 10 IoT Core to build stuff like facial-recognition door locks that only open if it recognizes you, a robot programming kit, and an app to control a handheld fan. The facial recognition technology is powered by Project Oxford.Read more: Hello, Windows 10 IoT Core\nWhy Microsoft made a Windows-powered robotic air hockey table\nUnlock Your Door With Your Face With Windows 10 For IoT Core\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-service-2015-08-11/", "Role": "Senior Program Manager, Cognitive Services"},
{"Title": "New machine-assisted text classification on Content Moderator now in public preview", "Date": "Posted on March 12, 2018", "Contributor": "Sanjeev Jagtap", "Content": "\nThis blog post is co-authored by Ashish Jhanwar, Data Scientist, Microsoft\nContent Moderator is part of Microsoft Cognitive Services allowing businesses to use machine assisted moderation of text, images, and videos that augment human review.\nThe text moderation capability now includes a new machine-learning based text classification feature which uses a trained model to identify possible abusive, derogatory or discriminatory language such as slang, abbreviated words, offensive, and intentionally misspelled words for review.\nIn contrast to the existing text moderation service that flags profanity terms, the text classification feature helps detect potentially undesired content that may be deemed as inappropriate depending on context. In addition, to convey the likelihood of each category it may recommend a human review of the content.\nThe text classification feature is in preview and supports the English language.\nHow to use\nContent Moderator consists of a set of REST APIs. The text moderation API adds an additional request parameter in the form of classify=True. If you specify the parameter as true, and the auto-detected language of your input text is English, the API will output the additional classification insights as shown in the following sections.\nIf you specify the language as English for non-English text, the API assumes the language as English, and outputs the additional insights, but they may not be relevant or useful.\nThe following code sample shows how to invoke the new feature by using the text moderation API.\n\r\nusing System;\r\nusing System.Collections.Generic;\r\nusing System.IO;\r\nusing System.Linq;\r\nusing System.Net.Http;\r\nusing System.Net.Http.Headers;\r\nusing System.Text;\r\nusing System.Threading.Tasks;\r\n\r\nnamespace TextClassifier\r\n{\r\n    class Program\r\n    {\r\n        //Content Moderator Key, API endpoint, and the new parameter\r\n        public const string CONTENTMODERATOR_APIKEY = \"YOUR API KEY\";\r\n        public const string APIURI = \"https://[REGIONNAME].api.cognitive.microsoft.com/contentmoderator/moderate/v1.0/ProcessText/Screen\";\r\n        public const string CLASSIFYPARAMETER = \"classify=True\";\r\n\r\n        static void Main(string[] args)\r\n        {\r\n            string ResponseJSON;\r\n            string Message = \"This is crap!\";\r\n\r\n            HttpClient client = new HttpClient();\r\n            client.BaseAddress = new Uri(APIURI);\r\n\r\n            string FullUri = APIURI + \"?\" + CLASSIFYPARAMETER;\r\n\r\n            // Add an Accept header for JSON format.\r\n            client.DefaultRequestHeaders.Accept.Add(\r\n            new MediaTypeWithQualityHeaderValue(\"text/plain\"));\r\n\r\n            client.DefaultRequestHeaders.Add(\"Ocp-Apim-Subscription-Key\", CONTENTMODERATOR_APIKEY);\r\n\r\n            HttpResponseMessage response = null;\r\n            response = client.PostAsync(FullUri, new StringContent(\r\n                                   Message, System.Text.Encoding.UTF8, \"text/plain\")).Result;\r\n\r\n            if (response.StatusCode == System.Net.HttpStatusCode.OK)\r\n            {\r\n                Console.WriteLine(\"Message insights:\");\r\n                Console.WriteLine();\r\n                ResponseJSON = response.Content.ReadAsStringAsync().Result;\r\n                Console.Write(ResponseJSON);\r\n                Console.ReadKey();\r\n            }\r\n        }\r\n    }\r\n}\nSample response\nIf you run the preceding sample console application, the resulting output shows the following classification insights. The ReviewRecommended value is set to true because the score for a classification was greater than the internal thresholds. Customers use either the ReviewRecommended flag to determine when content is flagged for human review or custom thresholds based on their content policies. The scores are in the range from 0 to 1.\n\r\n \"Classification\": {\r\n    \"ReviewRecommended\": true,\r\n    \"Category1\": { \"Score\": 0.0746903046965599 },\r\n    \"Category2\": { \"Score\": 0.23644307255744934 },\r\n    \"Category3\": { \"Score\": 0.98799997568130493 }\r\n  }\r\n\nExplanation of the response\n\nCategory1: Represents the potential presence of language that may be considered sexually explicit or adult in certain situations.\nCategory2: Represents the potential presence of language that may be considered sexually suggestive or mature in certain situations.\nCategory3: Represents the potential presence of language that may be considered offensive in certain situations.\nScore: The score range is between 0 and 1. The higher the score, the higher the model is predicting that the category may be applicable. This preview relies on a statistical model rather than manually coded outcomes. We recommend testing with your own content to determine how each category aligns to your requirements.\nReviewRecommended: ReviewRecommended is either true or false depending on the internal score thresholds. Customers should assess whether to use this value or decide on custom thresholds based on their content policies.\n\nBenefits of machine-assisted text moderation\nThe text classification feature is powered by a blend of advanced machine learning and Natural Language Processing (NLP) techniques. It is designed to work in different text domains like chats, comments, paragraphs etc.\nBusinesses use the text moderation service to either block, approve or review the content based on their policies and thresholds. The text moderation service can be used to augment human moderation of environments that require partners, employees and consumers to generate text content. These include chat rooms, discussion boards, chatbots, eCommerce catalogs, documents, and more.\nNext steps\nSign up for Content Moderator by using either the Azure portal or the Content Moderator human review tool. Get the API key and your region as explained in the Credentials article.\nUse the text moderation API console to test drive the capability online. Get started on your integration by either using the REST API samples or the .NET SDK samples.\n", "link": "https://azure.microsoft.com/en-us/blog/machine-assisted-text-classification-on-content-moderator-public-preview/", "Role": "Senior Program Manager - Content Moderator, Microsoft Cognitive Services Team"},
{"Title": "Bring your own vocabulary to Microsoft Video Indexer", "Date": "Posted on December 18, 2017", "Contributor": "Dr. Royi Ronen", "Content": "\nSelf-service customization for speech recognition\nVideo Indexer (VI) now supports industry and business specific customization for automatic speech recognition (ASR) through integration with the Microsoft Custom Speech Service! Supported languages are: Modern Standard Arabic, German, Spanish, English, French, Hindi, Italian, Japanese, Portuguese, Russian and Chinese.\nASR is an important audio analysis feature in Video Indexer. Speech recognition is artificial intelligence at its best, mimicking the human cognitive ability to extract words from audio. In this blog post, we will learn how to customize ASR in VI, to better fit specialized needs.\nBefore we get in to technical details, let’s take inspiration from a situation we have all experienced. Try to recall your first days on a job. You can probably remember feeling flooded with new words, product names, cryptic acronyms, and ways to use them. After some time, however, you can understand all these new words. You adapted yourself to the vocabulary.\nASR systems are great, but when it comes to recognizing a specialized vocabulary, ASR systems are just like humans. They need to adapt. Video Indexer now supports a customization layer for speech recognition, which allows you to teach the ASR engine new words, acronyms, and how they are used in your business context.\nHow does Automatic Speech Recognition work? Why is customization needed?\nRoughly speaking, ASR works with two basic models - an acoustic model and a language model. The acoustic model is responsible for translating the audio signal to phonemes, parts of words. Based on these phonemes, guesses regarding how these phonemes can be sequenced into words know to the system’s lexicon are generated. The language model is then used to choose the most reasonable sequence of words out of these guesses, based on the probabilities of words to occur one after the other, as learned from large samples of text.\nWhen input speech contains new words, the system cannot propose them as guesses, and they won’t be recognized correctly. For instance, Kubernetes, a new Azure product, is a word that we will teach VI to recognize in the example below. In other cases, the words exist, but the language model is not expecting them to appear in a certain context. For example, container service is not a 2-word sequence that a non-specialized language model would be scoring highly probable.\nHow does customization work?\nVideo Indexer lets you customize speech recognition by uploading adaptation text, namely text from the domain whose vocabulary you’d like the engine to adapt to. New words appearing in the adaptation text will now be recognized, assuming default pronunciation, and the language model will learn new probable sequences of words.\nAn example\nLet’s take a video on Azure Containers as an example. First, we upload the video to video indexer, without adaptation. Go to the VI portal,  click 'upload’, and choose the file from your machine.\nAfter a few minutes, the video on Kubernetes will be indexed (view result). Let us see where adaptation can help. Go 9 minute and 46 seconds into the video. The word ‘Kubernetes’ is a new, highly specific, word that the system does not know, and is therefore recognized as “communities”.\n\nHere are two other examples. At 00:49, “a VM” was recognized as “IBM”. Again, specific domain vocabulary, this time an acronym. The same happens for “PM” at 00:17, where it is not recognized.\n\nTo solve these, and other, issues, we need to apply language adaptation. We will start with a partial solution, which will help us understand the full solution.\nExample 1: Partial adaptation – words without context\nVI allows you to provide adaptation text that introduces your vocabulary to the speech recognition system. At this point, we will introduce just three lines, each with a word including Kubernetes, VM, and PM. The file is available for your review.\nGo to the customization settings by clicking on the highlighted icon on the upper-right hand corner of the VI portal, as shown below:\n\nOn the next screen, click “add file”, and upload the adaptation file.\n\nMake sure you activate the file as adaptation data.\n\nAfter the model has been adapted, re-index the file. And… Kubernetes is now recognized!\n\nVM is also recognized, as well as PM at 00:17.\n\nHowever, there is still room for more adaptation. Manually adding words can only help so much, since we cannot cover all the words, and we would also like the language model to learn from real instances of the vocabulary. This will make use of context, parts of speech, and other cues which can be learned from a larger corpus. In the next example, we will take a more complete approach by adding a decent corpus of real sentences from the domain. \nExample 2: Adapting the language model\nSimilar to what we have done above, let us now use as adaptation text a few pages of documentation about Azure containers. We have collected this adaptation text for your review. Below is an example for this style of adaptation data:\nTo mount an Azure file share as a volume in Azure Container Instances, you need three values: the storage account name, the share name, and the storage access key… The task of automating and managing a large number of containers and how they interact is known as orchestration. Popular container orchestrators include Kubernetes, DC/OS, and Docker Swarm, all of which are available in the Azure Container Service.\nWe recommend taking a look at the whole file. Let’s see a few examples of the effect. Let’s go back to 09:46. “Orchestrated” became orchestrator because of the adaptation text context.\n\nHere is another nice example in which highly specific terms become recognizable.\nBefore adaptation:\n\nAfter adaptation:\n\nDo’s and don’ts for language model adaptation\nThe system learns based on probabilities of word combinations, so to learn best:\n\nGive enough real examples of sentences as they would be spoken, hundreds to thousands is a good base.\nPut only one sentence per line, not more. Otherwise the system will learn probabilities across sentences.\nIt is okay to put one word as a sentence to boost the word against others, but the system learns best from full sentences.\nWhen introducing new words or acronyms, if possible, give as many examples of usage in a full sentence to give as much context as possible to the system.\nTry to put several adaptation options, and see how they work for you.\n\nSome patterns to avoid in adaptation data:\n\nRepetition of the exact same sentence multiple times. It does not boost further the probability and may create bias against the rest of the input.\nIncluding uncommon symbols (~, # @ % &) as they will get discarded, as well as the sentence they appear into.\nPutting too large inputs, including hundreds of thousands of sentences. These will dilute the effect of boosting.\n\nUsing the VI language adaptation API\nTo support adaptation, we have added a new customization tab to the site, and a new web API to manage the adaptation texts, training of the adaptation text, and transcription using adapted models.\nIn the Api/Partner/LinguisticTrainingData web API you will be able to create, read, update, and delete the adaptation text files. The files are plain *.txt  files which contain your adaptation data. For an improved user experience, mainly in the UI, we have groups that each file belongs to. This is especially useful when wanting to disable or enable multiple files at once in the UI.\nAfter adaptation data files are uploaded, we need to use them to customize the system using the Api/Partner/LinguisticModel  API's, which creates a linguistic model based on one or more files. In cases where there is more than a single file provided we concatenate the files into a single one. Preparing a customized model can take several minutes, and you are required to make sure that your model status is \"Complete\" before using it in indexing.\nThe last and most important step is the transcription itself. We added to the upload a new field named “linguisticModel” that accept a valid, customized linguistic model ID to be used for transcription. When re-indexing, we use the same model ID provided in the original indexing.\nImportant note: There is a slight difference in the user experience when using our site and API. When using our site, we allow enabling/disabling training data files and groups, and we will choose the active model during file upload/re-index. When using the API, we disregard the active state and index the videos based on model ID provided at run time. This difference is intentional to allow both simplicity in our website and more a robust experience for developers.\nThe full API can be found in our developer portal.\nConclusion\nAdaptation for speech recognition is necessary in order to teach the ASR system new words and how they are being used in a domain’s vocabulary. In Video Indexer, we provide adaptation technology which takes nothing but adaptation text and modifies the language model in the ASR system to make more intelligent guesses, given that the transcribed speech comes from the same domain as the adaptation text. This is useful to teach your system acronyms (e.g. VM), new words (e.g. Kubernetes), and new uses of known words (e.g. container).\n", "link": "https://azure.microsoft.com/en-us/blog/bring-your-own-vocabulary-to-microsoft-video-indexer/", "Role": "Principal Data Science Manager, Azure"},
{"Title": "Cognitive Services - Availability of SDKs for the latest Bing Search APIs", "Date": "Posted on January 25, 2018", "Contributor": "Ronak Shah", "Content": "\nWe are pleased to announce preview availability of SDKs for the Cognitive Serivces - Bing Search APIs. Currently available as REST APIs, the Bing APIs v7 now have SDKs in four languages: C#, Java, Node.js, and Python. These SDKs include offerings such as Bing Web Search, Bing Image Search, Bing Custom Search, Bing News Search, Bing Video Search, Bing Entity Search, and Bing Spell Check. \nHere are some of the salient features of these SDKs:\n\nEasy to use and highly flexible in adjusting your basis application scenario.\nEncompass all the API v7 functionalities, languages, and countries.\nReduce assembly footprint through individual SDK for each Bing offering.\nEnable development in C#, Java, Node.js, and Python.\nProvide ability to use the new/existing Bing APIs access keys, both free and paid.\nWell documented through samples and parameter references.\nSupported through Azure and other developer forums.\nOpensource under MIT license and available on GitHub for collaboration.\n\n\nGetting Started with Bing SDKs\nFor C#, both NuGet packages and SDKs are available for individual Bing offerings. The best place to start is with C# samples. These samples provide an easy to follow step-by-step guide on running various application specific scenarios through corresponding NuGet packages.\nEach Bing offering has corresponding samples along with a NuGet package that you can use to build your application on top of.\nFor development in other languages, check out Java samples, Node samples, and Python samples. They are all covered under MIT license and include step-by-step guide for you to get started.\nIf you are already utilizing Bing REST APIs in your app, give these SDKs a spin with your existing access keys and let us know how they work in your scenario. If you are just getting started or want to utilize Bing APIs, you can start with free access keys or buy your subscription. As always, we are excited to see what applications you build and how Bing APIs help you do more with your business. For any ideas or suggestions, please reach out to us at User Voice or share your feedback with Azure Support. \nUseful Links\n\nBing APIs – information and demos – on Cognitive Services\nC# SDK samples \nC# SDKs\nJava SDK samples\nJava SDKs\nNode.js SDK samples\nNode.js SDKs\nPython SDK samples\nPython SDKs\nFree trial keys\nBuy subscription keys\n\n", "link": "https://azure.microsoft.com/en-us/blog/cognitive-services-and-availability-of-sdks-for-the-latest-bing-search-apis/", "Role": "Senior PM"},
{"Title": "Bing Speech API extends its text to speech support to 34 languages", "Date": "Posted on February 5, 2018", "Contributor": "Qinying Liao", "Content": "\nVoice is becoming more and more prevalent as a mode of interaction with all kinds of devices and services. The ability to provide not only voice input but also voice output or Text-to-speech (TTS), is also becoming a critical technology that supports AI. Whether you need to interact on a device, over the phone, in a vehicle, through a building PA system, or even with a translated input, TTS is a crucial part of your end-to-end solution. It is also a necessity for all applications that enable accessibility.\nWe are excited to announce that the Speech API, a Microsoft Cognitive Service, now offers six new TTS languages to all developers, bringing the total number of available languages to 34:\n\nBulgarian (language code: bg-BG)\nCroatian (hr-HR)\nMalaysia (ms-MY)\nSlovenian (sl-SI)\nTamil (ta-IN)\nVietnamese (vi-VN)\n\nPowered by the latest AI technology, these 34 languages are available across 48 locales and 78 voice fonts. Through a single API, developers can access the latest-generation of speech recognition and TTS models.\nThis Text-to-Speech API can be integrated by developers for a broad set of use cases. It can be used on its own for accessibility, hand-free communication or media consumption, or any other machine-to-human interactions. It can also be combined with other Cognitive Services APIs such as the Speech to Text and Language Understanding ones to create comprehensive voice driven solutions, online or on device.\nIn addition, these new TTS languages will become available through the Microsoft Translator Speech API and the Microsoft Translator apps by the end of February 2018, making these new languages text-to-speech output available for developers integrating the Translator speech API as well as end-users of the Microsoft Translator apps and Translator live feature.\nTry these new TTS languages today \nFor the complete list of our 34 languages, 48 locales and 78 voice fonts please refer to this documentation \n", "link": "https://azure.microsoft.com/en-us/blog/bing-speech-api-extends-its-text-to-speech-support-to-34-languages/", "Role": "Senior Program Manager"},
{"Title": "Brand Detection in Microsoft Video Indexer", "Date": "Posted on February 6, 2018", "Contributor": "Abed Asi", "Content": "\nWe are delighted to announce a new capability in Microsoft Video Indexer: Brand Detection from speech and from visual text! If you are not yet familiar with Video Indexer, you may want to take a look at a few examples on our portal.\nHaving brands in the video index gives you insights on names of products and organizations, which appear in a video or audio asset without having to watch it. Particularly, it enables you to search over large amounts of video and audio. Customers find Brand Detection useful in a wide variety of business scenarios such as contents archive and discovery, contextual advertising, social media analysis, retail compete analysis and many more.\nOut of the box brand detection\nLet us take a look at an example. In this Microsoft Build 2017 Day 2 presentation, the brand \"Microsoft Windows\" appears multiple times. Sometimes in the transcript, sometimes as visual text and never as verbatim. Video Indexer detects with high precision that a term is indeed brand based on the context, covering over 90k brands out of the box, and constantly updating. At 02:25, Video Indexer detects the brand from speech and then again at 02:40 from visual text, which is part of the windows logo.\n\n\nTalking about windows in the context of construction will not detect the word \"Windows\" as a brand, and same for Box, Apple, Fox, etc., based on advanced Machine Learning algorithms that know how to disambiguate from context. Brand Detection works for all our supported languages. Click here for full Microsoft Build 2017 Day 2 keynote video and index.\nBring your own brands\nWe have already got mining algorithms in place, which mine brands and will be updating the brand base regularly. However, we also allow customization of brands which includes adding and excluding brands from the index. The following screenshot shows the brand customization screen. You can reach it from the customization button on the upper-right corner of the VI portal main page. Let us add 'Mod Pizza' as a brand to be included.\n\nThe result of indexing after the addition of the new custom brand follows:\n\n\nClick here for full Microsoft Build 2017 Day 2 Keynote video and customized index.\nUsing the Brand Customization API\nIn the Api Partner Customization Brands Web API you will be able to customize brand detection. You can enable and disable the detection of out of the box brands, as well as add your own custom brands for VI to detect or ignore.\nTo add your own brand for detection or filtering, all you need is the brand name. It is recommended to add its Wikipedia page as well if possible for improved results. You can also specify to which categories the brand belongs. From that moment forward, every video that is indexed, will account for your customization.\nConclusion\nBrand Detection is a new capability in Video Indexer, which enables you to index brand mentions in speech and visual text, based on a large built-in brands catalog as well as with customization. Brands are disambiguated from other terms using context.\nPlease visit our API documentation on the VI developer portal, for more details on how to use brand detection in VI.\n", "link": "https://azure.microsoft.com/en-us/blog/brand-detection-in-microsoft-video-indexer/", "Role": "Senior Applied Researcher, Azure Media Services"},
{"Title": "Microsoft Azure IP Advantage: Our first year", "Date": "Posted on February 8, 2018", "Contributor": "Erich Andersen", "Content": "\nOne year ago, we announced Azure IP Advantage, the industry’s leading program to help cloud service customers stay focused on their digital transformation journey and avoid IP issues. The program has been a tremendous success so far with many customers telling us that it is a key differentiator for Azure and that they choose Azure in part because of the value they get from these benefits.\nHere are some of the highlights from our first year:\n\nCustomers around the world find that Azure IP Advantage has been a valuable deterrent against IP lawsuits, which is especially important as cloud-related patent litigation has increased over the past 4 years. Customers of our partner 21 Vianet like Mobike, the world’s largest bicycle sharing company headquartered in China, explain the benefits of offering IP protection programs to Azure clients. “Azure IP Advantage helps us by reducing potential IP risks as we march into new markets. From technologies to patent offerings, Microsoft is providing a comprehensive protection for us to thrive on cloud without worry.”\nMicrosoft expanded Azure IP Advantage to China in partnership with 21Vianet, ensuring that Azure customers in China enjoy the same great IP protection benefits as customers in the rest of the world.\nMicrosoft invests about $10 million a year to maintain the 10,000 patents that are available to customers under the program. This is money that our customers do not need to pay themselves!  When they select Azure to deploy their workloads and run their apps, customers now benefit from the ability they have to access our portfolio.\nThose 10,000 patents include more than 500 patents related to artificial intelligence. Customers are increasingly taking advantage of Microsoft Cognitive Services to add AI capabilities to their apps. These innovations have been harvested from Microsoft’s world-class development community over many years. We will continue to prioritize these assets in the Azure IP Advantage portfolio for the benefit of customers, as the importance of artificial intelligence as a service grows. \n\nAzure’s adoption rate has grown at 90% year-over-year as customers continue to embrace the benefits of running workloads in the cloud. We’ve seen how the additional benefits of Azure IP Advantage has become vital to them as well.\nThe core benefits of the program are straight forward. Azure IP Advantage provides customers with uncapped indemnification for Microsoft cloud services, including for the open source components that power these services. Eligible customers have also access to 10,000 Microsoft patents to deter and defend their own applications running on Azure against patent lawsuits by operating companies.  We’ve seen competitors try to match some aspects of this offering since we launched it, but none to date have come close. \nAs we move into the second year of Azure IP Advantage, we look forward to working with our customers to continue improving the benefits available to them through our expertise in using IP to minimize risk. \nIf you are an Azure customer and have thoughts on the program and how it could be improved, please contact us at ipadvant@microsoft.com. It’s been an exciting year for the Azure IP Advantage program, and we’re looking forward to another great one in 2018.\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-azure-ip-advantage-our-first-year/", "Role": "Corporate Vice President, Deputy General Counsel"},
{"Title": "Custom Speech: Code-free automated machine learning for speech recognition", "Date": "Posted on February 8, 2018", "Contributor": "Xuedong Huang", "Content": "\nVoice is the new interface driving ambient computing. This statement has never been more true than it is today. Speech recognition is transforming our daily lives from digital assistants, dictation of emails and documents, to transcriptions of lectures and meetings. These scenarios are possible today thanks to years of research in speech recognition and technological jumps enabled by neural networks. Microsoft is at the forefront of Speech Recognition with its research results, reaching human parity on the Switchboard research benchmark.\nOur goal is to empower developers with our AI advances, so they can build new and transformative experiences for their customers. We offer a spectrum of APIs to address the various scenarios and situations developers encounter. Cognitive Services Speech API gives developers access to state of the art speech models. Premium scenarios, using domain specific vocabulary or complex acoustic conditions, offer Custom Speech Service that enables developers to automatically tune speech recognition models to their specific needs. Our services have been previewed on a wide range of scenarios with customers.\n\nSpeech recognition systems are composed of several components. The most important components are the acoustic and language models. If your application contains vocabulary items that occur rarely in everyday language, customizing the language model will significantly improve recognition accuracy. Customers can upload textual data like words and sentences of the target domain to build language models which can then be deployed and accessed by any devices through our Speech API.\nUniversity lectures are a typical example as they contain domain specific terminology. In a biology lecture for example we could encounter terms like “Nerodia erythrogaster\" which are extremely specific but important to correctly transcribe. Presentation Translator, an add-in to PowerPoint which customizes the language model based on slide content, addresses the lecture or any presentation scenario offering highly accurate transcription results for domain specific audio. This video on getting started with Presentation Translator provides a deeper explanation, and this video demonstrates the Presentation Translator add-in.\n\nSimilarly, customizing the acoustic model enables the speech recognition system to be accurate in particular environments. For example, if a voice-enabled app is aimed for use in a warehouse or factory, a custom acoustic model can accurately recognize speech in the presence of loud or persistent background noise. Customers can upload audio data with accompanying transcripts to build acoustic models.\n\nCustom Speech Service enables acoustic and language model adaptation with zero coding. Our user interface guides you through data import, model adaptation, evaluation and optimization through measuring word error rate and tracking improvements. It also guides you through model deployment at scale, so models can be accessed by your application on any devices.\nCreating, updating, and deploying models takes only minutes, making it easy to build and iteratively improve your application. To learn and start building your own speech recognition models, visit Custom Speech Service and our Custom Speech Service documentation pages.\n", "link": "https://azure.microsoft.com/en-us/blog/custom-speech-code-free-automated-machine-learning-for-speech-recognition/", "Role": "Technical Fellow, Cloud and AI"},
{"Title": "Microsoft updates Cognitive Services terms", "Date": "Posted on February 15, 2018", "Contributor": null, "Content": "\nThis post was authored by the Microsoft Cognitive Services Team.\nWe understand that the commitments we make about data are essential for any organization. To give customers more control, we’ve updated our Cognitive Services terms for customer data. Here’s what this means for our customers.\nOn February 1, we started moving Cognitive Services under the same terms as other Azure services. Under the new terms, Cognitive Services customers own, and can manage and delete their customer data. With this change, many Cognitive Services are now aligned with the same terms that apply to other Azure services.\nTerms for Computer Vision, Face, Content Moderator, Text Analytics, and Bing Speech services have already changed, with updates coming to Language Understanding on March 1 and Microsoft Translator on May 1. As new products are added to Cognitive Services, they will align with the same standards as other Azure services, with the exception of Bing Search Services.\nBing Search Services data will continue to be treated differently than other customer data. For example, we use search queries that you provide to Bing Search Services to improve our search algorithms over time.\nWe are making these updates because we strive to be transparent in our privacy practices and responsibly manage the data we store and process on behalf of our customers.\nFor more information, please take a look at the Cognitive Services section of our Trust Center, a resource for customers to learn how we implement and support security, privacy, compliance, transparency in all our cloud products and services.\n-The Microsoft Cognitive Services Team\n", "link": "https://azure.microsoft.com/en-us/blog/microsoft-updates-cognitive-services-terms/", "Role": null},
{"Title": "Announcing new milestones for Microsoft Cognitive Services vision and search services in Azure", "Date": "Posted on March 1, 2018", "Contributor": "Joseph Sirosh", "Content": "\nArtificial Intelligence (AI) has emerged as one of the most disruptive forces behind the digital transformation of business. At Microsoft, we believe everyone—developers, data scientists and enterprises—should have access to the benefits of AI to augment human ingenuity in unique and differentiated ways. We’ve been conducting research in AI for more than two decades and infusing it into our products and services. Now we’re bringing it to everyone through simple, yet powerful tools. One of those tools is Microsoft Cognitive Services, a collection of cloud-hosted APIs that let developers easily add AI capabilities for vision, speech, language, knowledge and search into applications, across devices and platforms such as iOS, Android and Windows.\nTo date more than a million developers have already discovered and tried our Cognitive Services, and many new customers are harnessing the power of AI, such as major auto insurance provider Progressive best known for Flo, its iconic spokesperson. The company wanted to take advantage of customers’ increasing use of mobile channels to interact with its brand. Progressive used Microsoft Azure Bot Service and Cognitive Services to quickly and easily build the Flo Chatbot—currently available on Facebook Messenger—which answers customer questions, provides quotes and even offers a bit of witty banter in Flo’s well-known style.\nToday, we’re announcing new milestones for Cognitive Services vision and search services in Azure.\nBringing vision capabilities to every developer\nFor years, Microsoft researchers have been pushing the boundaries of computer vision, making systems able to more accurately identify images. The following milestones represent one of the many examples of how we’re integrating our research advances into our enterprise services.\nToday, I’m pleased to announce the public preview of Custom Vision service on the Azure Portal (Figure 1). Microsoft Custom Vision service makes it possible for developers to easily train a classifier with their own data, export the models and embed these custom classifiers directly in their applications, and run it offline in real time on iOS, Android and many other edge devices. We built Custom Vision with state-of-the-art machine learning that offers developers the ability to train their own classifier to recognize what matters in their scenarios.\nWith a couple of clicks, Custom Vision service can be used for a multiplicity of scenarios: retailers can easily create models that can auto-classify images from their catalogs (dresses vs shoes, etc.), social sites can more effectively filter and classify images of specific products, or national parks can detect whether images from cameras include wild animals or not. Last month, we also announced Custom Vision Service is able to export models to the CoreML format for iOS 11 and to the TensorFlow format for Android. The exported models are optimized for the constraints of a mobile device, so classification on device happens in real time.\n\nFigure 1: Custom Vision service, now available in Azure preview \nThe Face API is a generally available cloud-based service that provides face and emotion recognition. It detects the location and attributes of human faces and emotions in an image, which can be used to personalize user experiences. With the Face API, developers can help determine if two faces belong to the same person, identify previously tagged people, find similar-looking faces in a collection, and find or group photos of the same person from a collection.\nStarting today, the Face API now integrates several improvements, including million-scale recognition to better help customers for their vision scenarios (Figure 2). The million-scale recognition capabilities represent a new type of person group now with up to a million people, and a new type of face list with up to a million faces. With this update, developers can now teach the Face API to recognize up to 1 million people and still get lightning-fast responses.\n\nFigure 2: The Face API now integrates several improvements, including million-scale recognition \nHarnessing search capabilities for every developer\nAnother key area of AI investment has been around search: everyone around the globe can gather rich information from Bing Search to query the web, but we’re also empowering developers to leverage it through multiple search APIs. Embedding it into any app with a few lines of code, to help users find the right information among the knowledge across the planet.\nPart of the search capabilities of Cognitive services, Bing Entity Search brings rich context about people, places, things and local businesses to any app, blog or website for a more engaging user experience. I’m also pleased to announce Bing Entity Search is now generally available today on the Azure Portal.\nWith Bing Entity Search, developers can now identify the most relevant entity based on searched terms and provide primary details about those entities (Figure 3). Entities spans across multiple international markets and market types including information about famous people, places, movies, TV shows, video games and books.\nMany scenarios can be covered with Bing Entity Search: for instance, a messaging app could provide an entity snapshot of a restaurant, making it easier for a group to plan an evening. A social media app could augment users’ photos with information about the locations of each photo. A news app could provide entity snapshots for entities in the article.\n\nFigure 3: Augmenting content with entity search results\nToday’s milestones illustrate our commitment to make our AI Platform suitable for every business scenario, with enterprise-grade tools to make application development easier and respecting customers’ data.\nTo learn more and start building vision and search intelligent apps, please visit the Cognitive Services site in Azure and our documentation pages.\nI invite you to visit www.azure.com/ai to learn more about how AI can augment and empower your digital transformation efforts. We’ve also launched the AI School to help developers get up to speed with these AI technologies.\n\nJoseph\n@josephsirosh\n", "link": "https://azure.microsoft.com/en-us/blog/announcing-new-milestones-for-microsoft-cognitive-services-vision-and-search-services-in-azure/", "Role": "Corporate Vice President, Artificial Intelligence & Research"},
{"Title": "Azure cloud data and AI services training roundup", "Date": "Posted on March 12, 2018", "Contributor": "Eric  Hudson", "Content": "\nLooking to transform your business by improving your on-premises environments? Accelerating your move to the cloud, and gaining transformative insights from your data? Here’s your opportunity to learn from the experts and ask the questions that help your organization move forward.\nJoin us for one or all of these training sessions to take a deep dive into a variety of topics. Including products like Azure Cosmos DB, along with Microsoft innovations in artificial intelligence, advanced analytics, and big data. \nAzure Cosmos DB\nEngineering experts are leading a seven-part training series on Azure Cosmos DB, complete with interactive Q&As. In addition to a high-level technical deep dive, this series covers a wide array of topics, including:\n\nGraph API\nTable API\nBuilding Mongo DB apps\n\nBy the end of this series, you’ll be able to build serverless applications and conduct real-time analytics using Azure Cosmos DB, Azure Functions, and Spark. Register to attend the whole Azure Cosmos DB series, or register for the sessions that interest you.\nArtificial Intelligence (AI)\nLearn to create the next generation of applications spanning an intelligent cloud as well as an intelligent edge powered by AI. Microsoft offers a comprehensive set of flexible AI services for any scenario and enterprise grade AI infrastructure that runs AI workloads anywhere at scale. Modern AI tools designed for developers and data scientists help you create AI solutions easily, with maximum productivity.\nUnlock deeper learning with the new Microsoft Cognitive Toolkit\nData is powerful, but navigating it can be slow, unreliable, and overly complex. Join us to learn about the Microsoft Cognitive Toolkit which offers deep learning capabilities that allow you to enable intelligence within massive datasets. In this session, you’ll learn:\n\nWhat’s new with the Microsoft Cognitive Toolkit.\nHow to maximize the programming languages and algorithms you already use.\nCognitive Toolkit features, including support for ONNX, C#/.NET API, and model simplification/compression.\n\nFilter content at scale with Cognitive Services' Content Moderator\nLearn how Azure Cognitive Services' Content Moderator filters out offensive and unwanted content from text, images, and videos at scale. By combining intelligent machine assisted technology with an intuitive human review system, Content Moderator enables quick and reliable content scanning. In this session, you’ll learn:\n\nContent Moderator platform basics.\nHow to use the machine learning-based APIs.\nHow to easily integrate human review tools with just a few lines of code.\n\nAdvanced analytics and big data\nData volumes are exploding. Deliver better experiences and make better decisions by analyzing massive amounts of data in real time. By including diverse datasets from the start, you’ll make more informed decisions that are predictive and holistic rather than reactive and disconnected. \nAccelerate innovation with Microsoft Azure Databricks\nLearn how your organization can accelerate data-driven innovation with Azure Databricks, a fast, easy-to-use, and collaborative Apache Spark based analytics platform. Designed in collaboration with the creators of Apache Spark, it combines the best of Databricks and Azure to help you accelerate innovation with one-click set up, streamlined workflows, and an interactive workspace that enables collaboration among data scientists, data engineers, and business analysts. In this session, you’ll learn how to:\n\nUse Databricks Notebooks to unify your processes and instantly deploy to production.\nLaunch your new Spark environment with a single click.\nIntegrate effortlessly with a wide variety of data stores.\nImprove and scale your analytics with a high-performance processing engine optimized for the comprehensive, trusted Azure platform.\n\nInterested in training on other Azure related topics? Take a look at the wide variety of live and on-demand training sessions now available on Azure.com.\n", "link": "https://azure.microsoft.com/en-us/blog/azure-cloud-data-and-ai-services-february-training-roundup/", "Role": "Senior Product Marketing Manager, CADD & AI"}
]